{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "1. be sure to use exponential RELU\n",
    "2. dataset includes 4 classes, 9 test sujects\n",
    "3. expect no spacial interdependency between channels\n",
    "4. expect interdependacny in time-domain. Use Conv nets to get frequencies\n",
    "5. input is 2D-array, where height is the umber of channels, width is time\n",
    "6. aim is 76% accuracy\n",
    "7. We do not even try hybrid/residual nets because it was said that it performs worse than Deep net\n",
    "\n",
    "Simle NN:\n",
    "\n",
    "We applied batch normalization,\n",
    "as recommended in the original paper (Ioffe and Szegedy, 2015), to the output of convolutional\n",
    "layers before the nonlinearity. Dropout randomly sets some inputs for a layer to zero in each\n",
    "training update. It is meant to prevent co-adaption of different units and can be seen as analogous\n",
    "to training an ensemble of networks. We drop out the inputs to all convolutional layers after the\n",
    "first with a probability of 0.5. Finally, our new tied loss function is designed to further regularize\n",
    "our cropped training (see Section 2.5.4 for an explanation).\n",
    "\n",
    "\n",
    "Our deep ConvNet had four convolution-max-pooling blocks, with a special first block designed\n",
    "to handle EEG input (see below), followed by three standard convolution-max-pooling blocks and\n",
    "a dense softmax classification layer\n",
    "\n",
    "Dropout for my conv layer is p=0.8 and for my fc layer is p=0.5. Interestingly, I saw the opposite trend where a low dropout number (I tried 0.2 out of curiosity) made overfitting worse and dropped my validation accuracy by 10-15%. Beside dropout, I didn't add any other regularization since that mainly helps with overfitting I believe. \n",
    "\n",
    "\n",
    "\n",
    "LET\"S TRY TO HAVE TIMESTAMP AS JUST A REGULAR PARAMETER OF THE NETWORK\n",
    "Idea 1 - let's try to have timestamp as just a regular parameter of the network - I can't perform convolutions in this case - after the first convolution number of channels and numer of samplesmesses up\n",
    "\n",
    "Idea 2 - let's try to pass the data without additional windowing. Seem Ok at a first glance but not sure whether state of the LSTM cell is really perserved - it seems that addition of LSTM does not help to imporve accuracy\n",
    "\n",
    "Idea 3 - let's try tp window the data first. Then let's reshape first to dimensions so batch size will be number_of_trials * number_of_windows_in_the_trial. Let's specify statefullness of the cell and make batch_size=number_of_windows_in_the_trial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# import tf\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# import os functions\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"./EEG_data/X_test.npy\")\n",
    "y_test = np.load(\"./EEG_data/y_test.npy\") - 769\n",
    "person_train_valid = np.load(\"./EEG_data/person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"./EEG_data/X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"./EEG_data/y_train_valid.npy\") - 769\n",
    "person_test = np.load(\"./EEG_data/person_test.npy\")\n",
    "\n",
    "print(\"training/Valid data shape: {}\".format(X_train_valid.shape))       # training data of many persons\n",
    "print(\"Test data shape: {}\".format(X_test.shape))                        # test data of many persons\n",
    "print(\"Training/Valid target shape: {}\".format(y_train_valid.shape))     # training labels of many persons\n",
    "print(\"Test target shape: {}\".format(y_test.shape))                      # test labels of many persons\n",
    "print(\"Person train/valid  shape: {}\".format(person_train_valid.shape))  # which person correspond to the trail in test set\n",
    "print(\"Person test shape: {}\".format(person_test.shape))                 # which person correspond to the trail in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### divide dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1692, 22, 1000)\n",
      "Training label shape: (1692,)\n",
      "Validation data shape: (423, 22, 1000)\n",
      "Validation label shape: (423,)\n",
      "Test data shape: (443, 22, 1000)\n",
      "Test label shape: (443,)\n"
     ]
    }
   ],
   "source": [
    "perm = np.random.permutation(X_train_valid.shape[0])\n",
    "num_train = int(0.8 * X_train_valid.shape[0])\n",
    "num_valid = X_train_valid.shape[0] - num_train\n",
    "X_train =  X_train_valid[perm[0:num_train]]\n",
    "y_train =  y_train_valid[perm[0:num_train]]\n",
    "X_valid = X_train_valid[perm[num_train: ]]\n",
    "y_valid = y_train_valid[perm[num_train: ]]\n",
    "\n",
    "\n",
    "print(\"Training data shape: {}\".format(X_train.shape))\n",
    "print(\"Training label shape: {}\".format(y_train.shape))\n",
    "print(\"Validation data shape: {}\".format(X_valid.shape))\n",
    "print(\"Validation label shape: {}\".format(y_valid.shape))\n",
    "print(\"Test data shape: {}\".format(X_test.shape))\n",
    "print(\"Test label shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 person dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape for 1 person: (189, 22, 1000)\n",
      "Training label shape for 1 person: (189,)\n",
      "Validation data shape for 1 person: (48, 22, 1000)\n",
      "Validation label shape for 1 person: (48,)\n",
      "Test data shape for 1 person: (50, 22, 1000)\n",
      "Test label shape for 1 person: (50,)\n"
     ]
    }
   ],
   "source": [
    "person_num = 0\n",
    "indices_train_valid = np.where(person_train_valid == person_num)[0]\n",
    "indices_test = np.where(person_test == person_num)[0]\n",
    "\n",
    "single_person_X_train_valid = X_train_valid[indices_train_valid]\n",
    "single_person_y_train_valid = y_train_valid[indices_train_valid]\n",
    "\n",
    "perm = np.random.permutation(single_person_X_train_valid.shape[0])\n",
    "num_train = int(0.8 * single_person_X_train_valid.shape[0])\n",
    "num_valid = single_person_X_train_valid.shape[0] - num_train\n",
    "single_person_X_train =  single_person_X_train_valid[perm[0:num_train]]\n",
    "single_person_y_train =  single_person_y_train_valid[perm[0:num_train]]\n",
    "single_person_X_valid = single_person_X_train_valid[perm[num_train: ]]\n",
    "single_person_y_valid = single_person_y_train_valid[perm[num_train: ]]\n",
    "\n",
    "single_person_X_test = X_test[indices_test]\n",
    "single_person_y_test = y_test[indices_test]\n",
    "\n",
    "\n",
    "print(\"Training data shape for 1 person: {}\".format(single_person_X_train.shape))\n",
    "print(\"Training label shape for 1 person: {}\".format(single_person_y_train.shape))\n",
    "print(\"Validation data shape for 1 person: {}\".format(single_person_X_valid.shape))\n",
    "print(\"Validation label shape for 1 person: {}\".format(single_person_y_valid.shape))\n",
    "print(\"Test data shape for 1 person: {}\".format(single_person_X_test.shape))\n",
    "print(\"Test label shape for 1 person: {}\".format(single_person_y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(X_arr, y_arr, time_window=100, time_step=1, time_stride=1):\n",
    "    temp_x = np.moveaxis(X_arr, 2, 0)\n",
    "    temp_x = temp_x.astype(np.float32)\n",
    "    buff = []\n",
    "    \n",
    "    num_slices = (len(temp_x)-time_window*time_step) // time_stride + 1\n",
    "    \n",
    "    # get time slices for data\n",
    "    for i in range(num_slices):\n",
    "        buff.append(temp_x[i*time_stride:i*time_stride + time_window*time_step:time_step])\n",
    "        buff[i] = np.moveaxis(buff[i], 0, 2)\n",
    "        # uncomment this if additional dimension is needed\n",
    "        # buff[i] = buff[i].reshape(1, buff[i].shape[0], buff[i].shape[1], buff[i].shape[2])\n",
    "        \n",
    "    temp_x = np.concatenate(buff)\n",
    "        \n",
    "    # get time slice for labels\n",
    "    temp_y = np.ones((X_arr.shape[0],num_slices))\n",
    "    \n",
    "    for i in range(len(y_arr)):\n",
    "        temp_y[i] = temp_y[i] * y_arr[i]\n",
    "        \n",
    "    temp_y = temp_y.reshape((-1))\n",
    "    \n",
    "    return temp_x, temp_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape with slices: (1692, 22, 1000)\n",
      "Training label shape with slice: (1692,)\n",
      "Validation data shape with slices: (423, 22, 1000)\n",
      "Validation label shape with slice: (423,)\n",
      "Test data shape with slices: (443, 22, 1000)\n",
      "Test label shape with slice: (443,)\n"
     ]
    }
   ],
   "source": [
    "TIME_WINDOW = 1000               # number of time stamps in the windpw\n",
    "TIME_STEP = 1                   # reverese to frequency\n",
    "TIME_STRIDE = 300                # stride as in convolution\n",
    "NUM_OF_WINDOWS = (1000-TIME_WINDOW*TIME_STEP)//TIME_STRIDE + 1\n",
    "\n",
    "# normilize the data USING ONLY TRAIN DATA MEAN AND STANDARD DEVIATION\n",
    "X_train_norm = (X_train - np.mean(X_train))/np.std(X_train)\n",
    "X_valid_norm = (X_valid - np.mean(X_train))/np.std(X_train)\n",
    "X_test_norm = (X_test - np.mean(X_train))/np.std(X_train)\n",
    "\n",
    "# cut the slices\n",
    "X_train_slices, y_train_slices = sliding_window(X_train_norm, \n",
    "                                                y_train, \n",
    "                                                time_window=TIME_WINDOW, \n",
    "                                                time_step=TIME_STEP, \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "X_valid_slices, y_valid_slices = sliding_window(X_valid_norm, \n",
    "                                                y_valid, \n",
    "                                                time_window=TIME_WINDOW, \n",
    "                                                time_step=TIME_STEP, \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "X_test_slices, y_test_slices = sliding_window(X_test_norm, \n",
    "                                              y_test, \n",
    "                                              time_window=TIME_WINDOW, \n",
    "                                              time_step=TIME_STEP, \n",
    "                                              time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "print(\"Training data shape with slices: {}\".format(X_train_slices.shape))\n",
    "print(\"Training label shape with slice: {}\".format(y_train_slices.shape))\n",
    "print(\"Validation data shape with slices: {}\".format(X_valid_slices.shape))\n",
    "print(\"Validation label shape with slice: {}\".format(y_valid_slices.shape))\n",
    "print(\"Test data shape with slices: {}\".format(X_test_slices.shape))\n",
    "print(\"Test label shape with slice: {}\".format(y_test_slices.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "# input\n",
    "naive_rnn_input = layers.Input(shape=(22, 1000))\n",
    "# conv across channels?\n",
    "p1 = layers.Permute((2, 1))(naive_rnn_input)\n",
    "lstm1 = layers.LSTM(128, return_sequences=True)(p1)\n",
    "lstm2 = layers.LSTM(128, return_sequences=True)(lstm1)\n",
    "d2 = layers.Dense(128, activation=\"elu\")(lstm2)\n",
    "f2 = layers.Flatten()(d2)\n",
    "\n",
    "# output\n",
    "naive_rnn_output = layers.Dense(4, activation=\"softmax\")(f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement learning rate decay\n",
    "initial_learning_rate = 1e-3\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                                                          decay_steps=1692,\n",
    "                                                          decay_rate=0.5,\n",
    "                                                          staircase=True)\n",
    "\n",
    "# save model with the best accuracy \n",
    "naive_rnn_callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/naive_rnn',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "naive_rnn_model = keras.Model(inputs = naive_rnn_input, outputs = naive_rnn_output, name=\"naive_rnn_model\")\n",
    "naive_rnn_model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule), \n",
    "                        loss=\"sparse_categorical_crossentropy\", \n",
    "                        metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"naive_rnn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute (Permute)            (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1000, 128)         77312     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000, 128)         131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000, 128)         16512     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 512004    \n",
      "=================================================================\n",
      "Total params: 737,412\n",
      "Trainable params: 737,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "naive_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'naive_rnn_callbacks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3b033185aaaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                                                 \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                                 \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                                                 callbacks=naive_rnn_callbacks)\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'naive_rnn_callbacks' is not defined"
     ]
    }
   ],
   "source": [
    "naive_rnn_model_loss_hist = naive_rnn_model.fit(X_train, y_train,\n",
    "                                                validation_data = (X_valid, y_valid),\n",
    "                                                epochs = 3,\n",
    "                                                callbacks=naive_rnn_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rnn_model = keras.models.load_model('./model_checkpoints/naive_rnn')\n",
    "\n",
    "best_rnn_model_results = best_rnn_model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Naive RNN model test loss:', best_rnn_model_results[0])\n",
    "print('Naive RNN model test acc:', best_rnn_model_results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that naive RNN model suffers from overfit - training loss of the model continues to drop while validation loss saturates and accuracy does not go beyond 50%. This can be fixed in 3 ways - dataset augmentation, architecture improvements and regularization.\n",
    "\n",
    "1. Architecture improvements: add dropouts to our layers; try GRU instead of LSTM;\n",
    "2. Dataset augmentation: normilize inputs, decrease the length of the example and upsample using sliding window approach - that produces correlation between examples, but if traning/validation/test sets are separated beforehand, that should be fine.\n",
    "3. Regularizations: \n",
    "\n",
    "Results:\n",
    "\n",
    "1. Dropout layers do not help to improve accuracy. It stays around 45% anyway\n",
    "2. Augmentation allowed to achieve validation accuracy of 55% and test accuracy of 50% if std not squared. Trained for 2 epoch. If std is squared, 2 epochs give 52% validation and 50% test. After this accuracy satruates. From this moment on all data is presumed to be normilized without square.\n",
    "3. Usage of sliding window lead to dramatic decrease of accuracy. From this point on it is assumed that no sliding wndow is applied.\n",
    "4. a) With length of 1000 we can achieve 56% validation 54% test over 5 epochs. After this learning saturates.\n",
    "4. b) Decrease of the length to 800 allowed to achieve 58% validation 55% test over 5 epochs. After this learning saturates.\n",
    "4. c) Decrease of the length to 750 allowed to achieve 59% validation 53% test over 3 epochs. After this learning saturates. \n",
    "4. d) Decrease of the length to 600 allowed to achieve 65% validation 57% test over 5 epochs. After this learning saturates. \n",
    "4. e) Overall, the gain from smaller window is also small +/- 3% at best. This is why from now we will rn experiments with 1000 but in the end also try 750%.\n",
    "\n",
    "5. When network starts to overfit you can decrease learning rate to 1e-4 to get a couple of additional persents.\n",
    "6. Form this point on we use size of 1000 samples per example if reverse is not specified.\n",
    "7. a) Added regularization to hidden layer of 0.001. Validation 56% test 53%\n",
    "7. b) Added regularization to hidden layer of 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "# input\n",
    "aug_rnn_input = layers.Input(shape=(22, TIME_WINDOW))\n",
    "# conv across channels?\n",
    "p1 = layers.Permute((2, 1))(aug_rnn_input)\n",
    "lstm1 = layers.LSTM(64, \n",
    "                    return_sequences=True, \n",
    "                    dropout=0.3, \n",
    "                    kernel_regularizer=keras.regularizers.l2(0.005),\n",
    "                    activity_regularizer = keras.regularizers.l2(0.005),\n",
    "                    recurrent_regularizer=keras.regularizers.l2(0.005))(p1)\n",
    "lstm2 = layers.LSTM(64, \n",
    "                    return_sequences=True, \n",
    "                    dropout=0.3,\n",
    "                    kernel_regularizer=keras.regularizers.l2(0.005),\n",
    "                    activity_regularizer = keras.regularizers.l2(0.005),\n",
    "                    recurrent_regularizer=keras.regularizers.l2(0.005))(lstm1)\n",
    "\n",
    "\n",
    "f2 = layers.Flatten()(lstm2)\n",
    "do2 = layers.Dropout(0.3)(f2)\n",
    "elu2 = layers.Dense(128, activation=\"elu\",  kernel_regularizer=keras.regularizers.l2(0.005))(do2)\n",
    "\n",
    "# output\n",
    "aug_rnn_output = layers.Dense(4, activation=\"softmax\")(elu2)### Construct model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with the best accuracy \n",
    "aug_rnn_callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/augmented_rnn',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)### Compile model\n",
    "]\n",
    "\n",
    "\n",
    "aug_rnn_model = keras.Model(inputs = aug_rnn_input, outputs = aug_rnn_output, name=\"augmented_rnn_model\")\n",
    "aug_rnn_model.compile(optimizer=keras.optimizers.Adam(), \n",
    "                      loss=\"sparse_categorical_crossentropy\", \n",
    "                      metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"augmented_rnn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute (Permute)            (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1000, 64)          22272     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000, 64)          33024     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1000, 64)          33024     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8192128   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,280,964\n",
      "Trainable params: 8,280,964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "aug_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/2\n",
      "1692/1692 [==============================] - 241s 142ms/sample - loss: 6.7103 - acc: 0.3836 - val_loss: 2.7707 - val_acc: 0.3570\n",
      "Epoch 2/2\n",
      "1692/1692 [==============================] - 230s 136ms/sample - loss: 2.8565 - acc: 0.4456 - val_loss: 2.0907 - val_acc: 0.5366\n"
     ]
    }
   ],
   "source": [
    "num_epochs += 1\n",
    "train_shuffle = np.arange(len(X_train_slices))\n",
    "np.random.shuffle(train_shuffle)\n",
    "aug_rnn_model_loss_hist = aug_rnn_model.fit(X_train_slices[train_shuffle], y_train_slices[train_shuffle],\n",
    "                                            validation_data = (X_valid_slices, y_valid_slices),\n",
    "                                            epochs = 2,\n",
    "                                            batch_size=10\n",
    "                                            # due to yet unresolved tf bug, we can not save augmented model in a callback\n",
    "                                            # https://github.com/tensorflow/tensorflow/issues/32505\n",
    "                                            #callbacks=aug_rnn_callbacks\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443/443 [==============================] - 6s 13ms/sample - loss: 1.8969 - acc: 0.5350\n",
      "Naive RNN model test loss: 1.8969259714165336\n",
      "Naive RNN model test acc: 0.5349887\n"
     ]
    }
   ],
   "source": [
    "aug_rnn_model_results = aug_rnn_model.evaluate(X_test_slices, y_test_slices)\n",
    "\n",
    "print('Naive RNN model test loss:', aug_rnn_model_results[0])\n",
    "print('Naive RNN model test acc:', aug_rnn_model_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_prj_C247",
   "language": "python",
   "name": "venv_prj_c247"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 654.545454,
   "position": {
    "height": "40px",
    "left": "266.375px",
    "right": "20px",
    "top": "2px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
