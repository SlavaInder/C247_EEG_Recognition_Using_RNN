{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "In this notebook we find optimal architecture for RNN network and compare its performance with shallow and deep CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# import tf\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# import os functions\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/Valid data shape: (2115, 22, 1000)\n",
      "Test data shape: (443, 22, 1000)\n",
      "Training/Valid target shape: (2115,)\n",
      "Test target shape: (443,)\n",
      "Person train/valid  shape: (2115, 1)\n",
      "Person test shape: (443, 1)\n"
     ]
    }
   ],
   "source": [
    "X_test = np.load(\"./EEG_data/X_test.npy\")\n",
    "y_test = np.load(\"./EEG_data/y_test.npy\") - 769\n",
    "person_train_valid = np.load(\"./EEG_data/person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"./EEG_data/X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"./EEG_data/y_train_valid.npy\") - 769\n",
    "person_test = np.load(\"./EEG_data/person_test.npy\")\n",
    "\n",
    "print(\"training/Valid data shape: {}\".format(X_train_valid.shape))       # training data of many persons\n",
    "print(\"Test data shape: {}\".format(X_test.shape))                        # test data of many persons\n",
    "print(\"Training/Valid target shape: {}\".format(y_train_valid.shape))     # training labels of many persons\n",
    "print(\"Test target shape: {}\".format(y_test.shape))                      # test labels of many persons\n",
    "print(\"Person train/valid  shape: {}\".format(person_train_valid.shape))  # which person correspond to the trail in test set\n",
    "print(\"Person test shape: {}\".format(person_test.shape))                 # which person correspond to the trail in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### divide dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1692, 22, 1000)\n",
      "Training label shape: (1692,)\n",
      "Validation data shape: (423, 22, 1000)\n",
      "Validation label shape: (423,)\n",
      "Test data shape: (443, 22, 1000)\n",
      "Test label shape: (443,)\n"
     ]
    }
   ],
   "source": [
    "perm = np.random.permutation(X_train_valid.shape[0])\n",
    "num_train = int(0.8 * X_train_valid.shape[0])\n",
    "num_valid = X_train_valid.shape[0] - num_train\n",
    "X_train =  X_train_valid[perm[0:num_train]]\n",
    "y_train =  y_train_valid[perm[0:num_train]]\n",
    "X_valid = X_train_valid[perm[num_train: ]]\n",
    "y_valid = y_train_valid[perm[num_train: ]]\n",
    "\n",
    "\n",
    "print(\"Training data shape: {}\".format(X_train.shape))\n",
    "print(\"Training label shape: {}\".format(y_train.shape))\n",
    "print(\"Validation data shape: {}\".format(X_valid.shape))\n",
    "print(\"Validation label shape: {}\".format(y_valid.shape))\n",
    "print(\"Test data shape: {}\".format(X_test.shape))\n",
    "print(\"Test label shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(X_arr, y_arr, time_window=100, time_step=1, time_stride=1):\n",
    "    temp_x = np.moveaxis(X_arr, 2, 0)\n",
    "    temp_x = temp_x.astype(np.float32)\n",
    "    buff = []\n",
    "    \n",
    "    num_slices = (len(temp_x)-time_window*time_step) // time_stride + 1\n",
    "    \n",
    "    # get time slices for data\n",
    "    for i in range(num_slices):\n",
    "        buff.append(temp_x[i*time_stride:i*time_stride + time_window*time_step:time_step])\n",
    "        buff[i] = np.moveaxis(buff[i], 0, 2)\n",
    "        # uncomment this if additional dimension is needed\n",
    "        # buff[i] = buff[i].reshape(1, buff[i].shape[0], buff[i].shape[1], buff[i].shape[2])\n",
    "        \n",
    "    temp_x = np.concatenate(buff)\n",
    "        \n",
    "    # get time slice for labels\n",
    "    temp_y = np.ones((X_arr.shape[0],num_slices))\n",
    "    \n",
    "    for i in range(len(y_arr)):\n",
    "        temp_y[i] = temp_y[i] * y_arr[i]\n",
    "        \n",
    "    temp_y = temp_y.reshape((-1))\n",
    "    \n",
    "    return temp_x, temp_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Naive RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we calculate baseline accuracy for RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "# input\n",
    "naive_rnn_input = layers.Input(shape=(22, 1000))\n",
    "# conv across channels?\n",
    "p1 = layers.Permute((2, 1))(naive_rnn_input)\n",
    "lstm1 = layers.LSTM(64, return_sequences=True)(p1)\n",
    "lstm2 = layers.LSTM(64, return_sequences=True)(lstm1)\n",
    "f2 = layers.Flatten()(lstm2)\n",
    "d2 = layers.Dense(64, activation=\"elu\")(f2)\n",
    "\n",
    "# output\n",
    "naive_rnn_output = layers.Dense(4, activation=\"softmax\")(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/naive_rnn_1000',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "naive_rnn_model = keras.Model(inputs = naive_rnn_input, outputs = naive_rnn_output, name=\"naive_rnn_model\")\n",
    "naive_rnn_model.compile(optimizer=\"Adam\", \n",
    "                        loss=\"sparse_categorical_crossentropy\", \n",
    "                        metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"naive_rnn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute (Permute)            (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1000, 64)          22272     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000, 64)          33024     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4096064   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 4,151,620\n",
      "Trainable params: 4,151,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "naive_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/5\n",
      "1664/1692 [============================>.] - ETA: 2s - loss: 1.8725 - acc: 0.3035\n",
      "Epoch 00001: val_loss improved from inf to 1.33201, saving model to ./model_checkpoints/naive_rnn_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/naive_rnn_1000\\assets\n",
      "1692/1692 [==============================] - 150s 89ms/sample - loss: 1.8645 - acc: 0.3026 - val_loss: 1.3320 - val_acc: 0.3452\n",
      "Epoch 2/5\n",
      "1664/1692 [============================>.] - ETA: 2s - loss: 1.0971 - acc: 0.5180\n",
      "Epoch 00002: val_loss improved from 1.33201 to 1.26551, saving model to ./model_checkpoints/naive_rnn_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/naive_rnn_1000\\assets\n",
      "1692/1692 [==============================] - 184s 109ms/sample - loss: 1.0990 - acc: 0.5177 - val_loss: 1.2655 - val_acc: 0.4279\n",
      "Epoch 3/5\n",
      "1664/1692 [============================>.] - ETA: 3s - loss: 0.7006 - acc: 0.7560\n",
      "Epoch 00003: val_loss did not improve from 1.26551\n",
      "1692/1692 [==============================] - 199s 118ms/sample - loss: 0.6980 - acc: 0.7565 - val_loss: 1.2851 - val_acc: 0.4610\n",
      "Epoch 4/5\n",
      "1664/1692 [============================>.] - ETA: 3s - loss: 0.3209 - acc: 0.9111\n",
      "Epoch 00004: val_loss did not improve from 1.26551\n",
      "1692/1692 [==============================] - 196s 116ms/sample - loss: 0.3213 - acc: 0.9102 - val_loss: 1.6244 - val_acc: 0.4752\n",
      "Epoch 5/5\n",
      "1664/1692 [============================>.] - ETA: 3s - loss: 0.1343 - acc: 0.9748\n",
      "Epoch 00005: val_loss did not improve from 1.26551\n",
      "1692/1692 [==============================] - 202s 120ms/sample - loss: 0.1332 - acc: 0.9752 - val_loss: 1.6895 - val_acc: 0.5012\n"
     ]
    }
   ],
   "source": [
    "naive_rnn_model_loss_hist = naive_rnn_model.fit(X_train, y_train,\n",
    "                                                validation_data = (X_valid, y_valid),\n",
    "                                                epochs = 5,\n",
    "                                                callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2520c80e2e8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = naive_rnn_model_loss_hist.history\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist['loss'])\n",
    "plt.plot(hist['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hist['acc'])\n",
    "plt.plot(hist['val_acc'])\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: RNN with more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we compare performance of baseline RNN and RNNs with more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute_2 (Permute)          (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 1000, 64)          22272     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 1000, 64)          33024     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 1000, 64)          33024     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4096064   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 4,184,644\n",
      "Trainable params: 4,184,644\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "rnn_3_lstm_1_dense_input = layers.Input(shape=(22, 1000))\n",
    "\n",
    "# conv across channels?\n",
    "p1 = layers.Permute((2, 1))(rnn_3_lstm_1_dense_input)\n",
    "lstm1 = layers.LSTM(64, return_sequences=True)(p1)\n",
    "lstm2 = layers.LSTM(64, return_sequences=True)(lstm1)\n",
    "lstm3 = layers.LSTM(64, return_sequences=True)(lstm2)\n",
    "f2 = layers.Flatten()(lstm3)\n",
    "d2 = layers.Dense(64, activation=\"elu\")(f2)\n",
    "\n",
    "# output\n",
    "rnn_3_lstm_1_dense_output = layers.Dense(4, activation=\"softmax\")(d2)\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/rnn_3_lstm_1_dense_1000',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "rnn_3_lstm_1_dense_model = keras.Model(inputs = rnn_3_lstm_1_dense_input, outputs = rnn_3_lstm_1_dense_output)\n",
    "rnn_3_lstm_1_dense_model.compile(optimizer=\"Adam\", \n",
    "                                 loss=\"sparse_categorical_crossentropy\", \n",
    "                                 metrics=[\"acc\"])\n",
    "\n",
    "rnn_3_lstm_1_dense_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/5\n",
      "1664/1692 [============================>.] - ETA: 3s - loss: 1.6527 - acc: 0.3311\n",
      "Epoch 00001: val_loss improved from inf to 1.32762, saving model to ./model_checkpoints/rnn_3_lstm_1_dense_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_3_lstm_1_dense_1000\\assets\n",
      "1692/1692 [==============================] - 211s 125ms/sample - loss: 1.6480 - acc: 0.3304 - val_loss: 1.3276 - val_acc: 0.3924\n",
      "Epoch 2/5\n",
      "1664/1692 [============================>.] - ETA: 4s - loss: 1.0913 - acc: 0.5541 \n",
      "Epoch 00002: val_loss improved from 1.32762 to 1.28518, saving model to ./model_checkpoints/rnn_3_lstm_1_dense_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_3_lstm_1_dense_1000\\assets\n",
      "1692/1692 [==============================] - 316s 187ms/sample - loss: 1.0891 - acc: 0.5556 - val_loss: 1.2852 - val_acc: 0.4515\n",
      "Epoch 3/5\n",
      "1664/1692 [============================>.] - ETA: 5s - loss: 0.7301 - acc: 0.7115 \n",
      "Epoch 00003: val_loss did not improve from 1.28518\n",
      "1692/1692 [==============================] - 324s 191ms/sample - loss: 0.7357 - acc: 0.7074 - val_loss: 1.4283 - val_acc: 0.4610\n",
      "Epoch 4/5\n",
      "1664/1692 [============================>.] - ETA: 5s - loss: 0.3995 - acc: 0.8618 \n",
      "Epoch 00004: val_loss did not improve from 1.28518\n",
      "1692/1692 [==============================] - 328s 194ms/sample - loss: 0.4049 - acc: 0.8593 - val_loss: 1.6498 - val_acc: 0.4681\n",
      "Epoch 5/5\n",
      "1664/1692 [============================>.] - ETA: 5s - loss: 0.1849 - acc: 0.9453 \n",
      "Epoch 00005: val_loss did not improve from 1.28518\n",
      "1692/1692 [==============================] - 327s 193ms/sample - loss: 0.1855 - acc: 0.9444 - val_loss: 1.9590 - val_acc: 0.4586\n"
     ]
    }
   ],
   "source": [
    "rnn_3_lstm_1_dense_model_loss_hist = rnn_3_lstm_1_dense_model.fit(X_train, y_train,\n",
    "                                                                  validation_data = (X_valid, y_valid),\n",
    "                                                                  epochs = 5,\n",
    "                                                                  callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute_3 (Permute)          (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 1000, 64)          22272     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 1000, 64)          33024     \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 1000, 64)          33024     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4096064   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 4,188,804\n",
      "Trainable params: 4,188,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "rnn_3_lstm_2_dense_input = layers.Input(shape=(22, 1000))\n",
    "\n",
    "# conv across channels?\n",
    "p1 = layers.Permute((2, 1))(rnn_3_lstm_2_dense_input)\n",
    "lstm1 = layers.LSTM(64, return_sequences=True)(p1)\n",
    "lstm2 = layers.LSTM(64, return_sequences=True)(lstm1)\n",
    "lstm3 = layers.LSTM(64, return_sequences=True)(lstm2)\n",
    "f2 = layers.Flatten()(lstm3)\n",
    "d2 = layers.Dense(64, activation=\"elu\")(f2)\n",
    "d3 = layers.Dense(64, activation=\"elu\")(d2)\n",
    "\n",
    "\n",
    "# output\n",
    "rnn_3_lstm_2_dense_output = layers.Dense(4, activation=\"softmax\")(d3)\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/rnn_3_lstm_2_dense_1000',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "rnn_3_lstm_2_dense_model = keras.Model(inputs = rnn_3_lstm_2_dense_input, outputs = rnn_3_lstm_2_dense_output)\n",
    "rnn_3_lstm_2_dense_model.compile(optimizer=\"Adam\", \n",
    "                                 loss=\"sparse_categorical_crossentropy\", \n",
    "                                 metrics=[\"acc\"])\n",
    "\n",
    "rnn_3_lstm_2_dense_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/5\n",
      "1664/1692 [============================>.] - ETA: 5s - loss: 0.8260 - acc: 0.6845 \n",
      "Epoch 00001: val_loss did not improve from 1.21840\n",
      "1692/1692 [==============================] - 331s 196ms/sample - loss: 0.8264 - acc: 0.6850 - val_loss: 1.4114 - val_acc: 0.4374\n",
      "Epoch 2/5\n",
      "1664/1692 [============================>.] - ETA: 5s - loss: 0.4094 - acc: 0.8618 \n",
      "Epoch 00002: val_loss did not improve from 1.21840\n",
      "1692/1692 [==============================] - 331s 196ms/sample - loss: 0.4096 - acc: 0.8617 - val_loss: 1.6600 - val_acc: 0.4586\n",
      "Epoch 3/5\n",
      "1664/1692 [============================>.] - ETA: 5s - loss: 0.1963 - acc: 0.9339 \n",
      "Epoch 00003: val_loss did not improve from 1.21840\n",
      "1692/1692 [==============================] - 334s 198ms/sample - loss: 0.1977 - acc: 0.9332 - val_loss: 1.8766 - val_acc: 0.4823\n",
      "Epoch 4/5\n",
      "1664/1692 [============================>.] - ETA: 5s - loss: 0.0641 - acc: 0.9832 \n",
      "Epoch 00004: val_loss did not improve from 1.21840\n",
      "1692/1692 [==============================] - 332s 196ms/sample - loss: 0.0645 - acc: 0.9829 - val_loss: 2.1180 - val_acc: 0.4752\n",
      "Epoch 5/5\n",
      "1664/1692 [============================>.] - ETA: 5s - loss: 0.0458 - acc: 0.9874 \n",
      "Epoch 00005: val_loss did not improve from 1.21840\n",
      "1692/1692 [==============================] - 339s 200ms/sample - loss: 0.0455 - acc: 0.9876 - val_loss: 2.3679 - val_acc: 0.4775\n"
     ]
    }
   ],
   "source": [
    "rnn_3_lstm_2_dense_model_loss_hist = rnn_3_lstm_2_dense_model.fit(X_train, y_train,\n",
    "                                                                  validation_data = (X_valid, y_valid),\n",
    "                                                                  epochs = 5,\n",
    "                                                                  callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be argued, but based on the validation loss dynamics we can say that RNN does not significantly benefit from more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: augmentation of RNN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we show how RNN architecture is affected by architecture modifications, such as regularization,  number of hidden units, dropout. We start with dropout ~ 0.3 which is a common solution. Then we try different regularization, adter which variate number of hidden units and then dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_rnn_lstm_model(TIME_WINDOW=1000, hidden_units=64, dropout=0, regularizer=0):\n",
    "    # input\n",
    "    aug_rnn_input = layers.Input(shape=(22, TIME_WINDOW))\n",
    "\n",
    "    p1 = layers.Permute((2, 1))(aug_rnn_input)\n",
    "    lstm1 = layers.LSTM(hidden_units, \n",
    "                        return_sequences=True, \n",
    "                        dropout=dropout, \n",
    "                        kernel_regularizer=keras.regularizers.l2(regularizer))(p1)\n",
    "    lstm2 = layers.LSTM(hidden_units, \n",
    "                        return_sequences=True, \n",
    "                        dropout=dropout,\n",
    "                        kernel_regularizer=keras.regularizers.l2(regularizer))(lstm1)\n",
    "\n",
    "\n",
    "    f2 = layers.Flatten()(lstm2)\n",
    "    do2 = layers.Dropout(dropout)(f2)\n",
    "    elu2 = layers.Dense(hidden_units, activation=\"elu\",  kernel_regularizer=keras.regularizers.l2(regularizer))(do2)\n",
    "\n",
    "    # output\n",
    "    aug_rnn_output = layers.Dense(4, activation=\"softmax\")(elu2)\n",
    "    \n",
    "    return keras.Model(inputs = aug_rnn_input, outputs = aug_rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/5\n",
      "1664/1692 [============================>.] - ETA: 4s - loss: 1.6933 - acc: 0.3438\n",
      "Epoch 00001: val_loss improved from inf to 1.32933, saving model to ./model_checkpoints/rnn_2_lstm_1_dense_1_conig_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_2_lstm_1_dense_1_conig_1000\\assets\n",
      "1692/1692 [==============================] - 271s 160ms/sample - loss: 1.6847 - acc: 0.3452 - val_loss: 1.3293 - val_acc: 0.3972\n",
      "Epoch 2/5\n",
      "1664/1692 [============================>.] - ETA: 4s - loss: 0.9941 - acc: 0.6238\n",
      "Epoch 00002: val_loss did not improve from 1.32933\n",
      "1692/1692 [==============================] - 273s 161ms/sample - loss: 0.9964 - acc: 0.6235 - val_loss: 1.4541 - val_acc: 0.4255\n",
      "Epoch 3/5\n",
      "1408/1692 [=======================>......] - ETA: 42s - loss: 0.7389 - acc: 0.7267WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-419ddc8975ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m                                                            \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                                                            \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                                                            callbacks=checkpoint_callback)\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TIME_WINDOW = 1000\n",
    "HIDDEN = 64\n",
    "DROPOUT = 0.3\n",
    "REGULARIZER = 0.0001\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/rnn_2_lstm_1_dense_1_conig_1000' ,\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_00001_reg_1000 = construct_rnn_lstm_model(TIME_WINDOW, \n",
    "                                                                                  HIDDEN, \n",
    "                                                                                  DROPOUT, \n",
    "                                                                                  REGULARIZER)\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_00001_reg_1000.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_00001_reg_1000.fit(X_train, y_train,\n",
    "                                                           validation_data = (X_valid, y_valid),\n",
    "                                                           epochs = 5,\n",
    "                                                           callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOW = 1000\n",
    "HIDDEN = 64\n",
    "DROPOUT = 0.3\n",
    "REGULARIZER = 0.001\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/rnn_2_lstm_1_dense_2_conig_1000' ,\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_0001_reg_1000 = construct_rnn_lstm_model(TIME_WINDOW, \n",
    "                                                                                 HIDDEN, \n",
    "                                                                                 DROPOUT, \n",
    "                                                                                 REGULARIZER)\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_0001_reg_1000.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_0001_reg_1000.fit(X_train, y_train,\n",
    "                                                           validation_data = (X_valid, y_valid),\n",
    "                                                           epochs = 5,\n",
    "                                                           callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOW = 1000\n",
    "HIDDEN = 64\n",
    "DROPOUT = 0.3\n",
    "REGULARIZER = 0.005\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000' ,\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_0005_reg_1000 = construct_rnn_lstm_model(TIME_WINDOW, \n",
    "                                                                                 HIDDEN, \n",
    "                                                                                 DROPOUT, \n",
    "                                                                                 REGULARIZER)\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_0005_reg_1000.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_0005_reg_1000.fit(X_train, y_train,\n",
    "                                                           validation_data = (X_valid, y_valid),\n",
    "                                                           epochs = 5,\n",
    "                                                           callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOW = 1000\n",
    "HIDDEN = 64\n",
    "DROPOUT = 0.3\n",
    "REGULARIZER = 0.01\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000' ,\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_001_reg_1000 = construct_rnn_lstm_model(TIME_WINDOW, \n",
    "                                                                                HIDDEN, \n",
    "                                                                                DROPOUT, \n",
    "                                                                                REGULARIZER)\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_001_reg_1000.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_001_reg_1000.fit(X_train, y_train,\n",
    "                                                         validation_data = (X_valid, y_valid),\n",
    "                                                         epochs = 5,\n",
    "                                                         callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW EXPERIMENTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to iterate through regularization through range approximately [0.1 - 0.0001], dropout - 0.2-0.5, hidden units - 32 to 128, sampl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_shallow_model_300 = keras.models.load_model('./model_checkpoints/shallow_model_300')\n",
    "best_shallow_model_500 = keras.models.load_model('./model_checkpoints/shallow_model_500')\n",
    "best_shallow_model_600 = keras.models.load_model('./model_checkpoints/shallow_model_600')\n",
    "best_shallow_model_700 = keras.models.load_model('./model_checkpoints/shallow_model_700')\n",
    "best_shallow_model_800 = keras.models.load_model('./model_checkpoints/shallow_model_800')\n",
    "best_shallow_model_900 = keras.models.load_model('./model_checkpoints/shallow_model_900')\n",
    "best_shallow_model_1000 = keras.models.load_model('./model_checkpoints/shallow_model_1000')\n",
    "\n",
    "accuracies.append(best_shallow_model_1000.evaluate(X_valid_slices, y_valid_slices)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that naive RNN model suffers from overfit - training loss of the model continues to drop while validation loss saturates and accuracy does not go beyond 50%. This can be fixed in 3 ways - dataset augmentation, architecture improvements and regularization.\n",
    "\n",
    "1. Architecture improvements: add dropouts to our layers; try GRU instead of LSTM;\n",
    "2. Dataset augmentation: normilize inputs, decrease the length of the example and upsample using sliding window approach - that produces correlation between examples, but if traning/validation/test sets are separated beforehand, that should be fine.\n",
    "3. Regularizations: \n",
    "\n",
    "Results:\n",
    "\n",
    "1. Dropout layers do not help to improve accuracy. It stays around 45% anyway\n",
    "2. Augmentation allowed to achieve validation accuracy of 55% and test accuracy of 50% if std not squared. Trained for 2 epoch. If std is squared, 2 epochs give 52% validation and 50% test. After this accuracy satruates. From this moment on all data is presumed to be normilized without square.\n",
    "3. Usage of sliding window lead to dramatic decrease of accuracy. From this point on it is assumed that no sliding wndow is applied.\n",
    "4. a) With length of 1000 we can achieve 56% validation 54% test over 5 epochs. After this learning saturates.\n",
    "4. b) Decrease of the length to 800 allowed to achieve 58% validation 55% test over 5 epochs. After this learning saturates.\n",
    "4. c) Decrease of the length to 750 allowed to achieve 59% validation 53% test over 3 epochs. After this learning saturates. \n",
    "4. d) Decrease of the length to 600 allowed to achieve 65% validation 57% test over 5 epochs. After this learning saturates. \n",
    "4. e) Overall, the gain from smaller window is also small +/- 3% at best. This is why from now we will rn experiments with 1000 but in the end also try 750%.\n",
    "\n",
    "5. When network starts to overfit you can decrease learning rate to 1e-4 to get a couple of additional persents.\n",
    "6. Form this point on we use size of 1000 samples per example if reverse is not specified.\n",
    "7. a) Added regularization to hidden layer of 0.001. Validation 56% test 53%\n",
    "7. b) Added regularization to hidden layer of 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443/443 [==============================] - 6s 13ms/sample - loss: 1.8969 - acc: 0.5350\n",
      "Naive RNN model test loss: 1.8969259714165336\n",
      "Naive RNN model test acc: 0.5349887\n"
     ]
    }
   ],
   "source": [
    "aug_rnn_model_results = aug_rnn_model.evaluate(X_test_slices, y_test_slices)\n",
    "\n",
    "print('RNN model test loss:', aug_rnn_model_results[0])\n",
    "print('RNN model test acc:', aug_rnn_model_results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_prj_C247",
   "language": "python",
   "name": "venv_prj_c247"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 654.545454,
   "position": {
    "height": "40px",
    "left": "266.375px",
    "right": "20px",
    "top": "2px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
