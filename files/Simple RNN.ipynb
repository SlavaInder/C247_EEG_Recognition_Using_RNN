{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "In this notebook we find optimal architecture for RNN network and compare its performance with shallow and deep CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# import tf\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# import os functions\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/Valid data shape: (2115, 22, 1000)\n",
      "Test data shape: (443, 22, 1000)\n",
      "Training/Valid target shape: (2115,)\n",
      "Test target shape: (443,)\n",
      "Person train/valid  shape: (2115, 1)\n",
      "Person test shape: (443, 1)\n"
     ]
    }
   ],
   "source": [
    "X_test = np.load(\"./EEG_data/X_test.npy\")\n",
    "y_test = np.load(\"./EEG_data/y_test.npy\") - 769\n",
    "person_train_valid = np.load(\"./EEG_data/person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"./EEG_data/X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"./EEG_data/y_train_valid.npy\") - 769\n",
    "person_test = np.load(\"./EEG_data/person_test.npy\")\n",
    "\n",
    "print(\"training/Valid data shape: {}\".format(X_train_valid.shape))       # training data of many persons\n",
    "print(\"Test data shape: {}\".format(X_test.shape))                        # test data of many persons\n",
    "print(\"Training/Valid target shape: {}\".format(y_train_valid.shape))     # training labels of many persons\n",
    "print(\"Test target shape: {}\".format(y_test.shape))                      # test labels of many persons\n",
    "print(\"Person train/valid  shape: {}\".format(person_train_valid.shape))  # which person correspond to the trail in test set\n",
    "print(\"Person test shape: {}\".format(person_test.shape))                 # which person correspond to the trail in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### divide dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1692, 22, 1000)\n",
      "Training label shape: (1692,)\n",
      "Validation data shape: (423, 22, 1000)\n",
      "Validation label shape: (423,)\n",
      "Test data shape: (443, 22, 1000)\n",
      "Test label shape: (443,)\n"
     ]
    }
   ],
   "source": [
    "perm = np.random.permutation(X_train_valid.shape[0])\n",
    "num_train = int(0.8 * X_train_valid.shape[0])\n",
    "num_valid = X_train_valid.shape[0] - num_train\n",
    "X_train =  X_train_valid[perm[0:num_train]]\n",
    "y_train =  y_train_valid[perm[0:num_train]]\n",
    "X_valid = X_train_valid[perm[num_train: ]]\n",
    "y_valid = y_train_valid[perm[num_train: ]]\n",
    "\n",
    "\n",
    "print(\"Training data shape: {}\".format(X_train.shape))\n",
    "print(\"Training label shape: {}\".format(y_train.shape))\n",
    "print(\"Validation data shape: {}\".format(X_valid.shape))\n",
    "print(\"Validation label shape: {}\".format(y_valid.shape))\n",
    "print(\"Test data shape: {}\".format(X_test.shape))\n",
    "print(\"Test label shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(X_arr, y_arr, time_window=100, time_step=1, time_stride=1):\n",
    "    temp_x = np.moveaxis(X_arr, 2, 0)\n",
    "    temp_x = temp_x.astype(np.float32)\n",
    "    buff = []\n",
    "    \n",
    "    num_slices = (len(temp_x)-time_window*time_step) // time_stride + 1\n",
    "#     print('temp_x', len(temp_x))\n",
    "#     print('time_window', time_window)\n",
    "#     print('num_slices', num_slices)\n",
    "    \n",
    "    # get time slices for data\n",
    "    for i in range(num_slices):\n",
    "        buff.append(temp_x[i*time_stride:i*time_stride + time_window*time_step:time_step])\n",
    "        buff[i] = np.moveaxis(buff[i], 0, 2)\n",
    "        # uncomment this if additional dimension is needed\n",
    "        # buff[i] = buff[i].reshape(1, buff[i].shape[0], buff[i].shape[1], buff[i].shape[2])\n",
    "        \n",
    "    temp_x = np.concatenate(buff)\n",
    "        \n",
    "    # get time slice for labels\n",
    "    temp_y = np.ones((X_arr.shape[0],num_slices))\n",
    "    \n",
    "    for i in range(len(y_arr)):\n",
    "        temp_y[i] = temp_y[i] * y_arr[i]\n",
    "        \n",
    "    temp_y = temp_y.reshape((-1))\n",
    "    \n",
    "    return temp_x, temp_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Naive RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we calculate baseline accuracy for RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "# input\n",
    "naive_rnn_input = layers.Input(shape=(22, 1000))\n",
    "# conv across channels?\n",
    "p1 = layers.Permute((2, 1))(naive_rnn_input)\n",
    "lstm1 = layers.LSTM(64, return_sequences=True)(p1)\n",
    "lstm2 = layers.LSTM(64, return_sequences=True)(lstm1)\n",
    "f2 = layers.Flatten()(lstm2)\n",
    "d2 = layers.Dense(64, activation=\"elu\")(f2)\n",
    "\n",
    "# output\n",
    "naive_rnn_output = layers.Dense(4, activation=\"softmax\")(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/naive_rnn_1000',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "naive_rnn_model = keras.Model(inputs = naive_rnn_input, outputs = naive_rnn_output, name=\"naive_rnn_model\")\n",
    "naive_rnn_model.compile(optimizer=\"Adam\", \n",
    "                        loss=\"sparse_categorical_crossentropy\", \n",
    "                        metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"naive_rnn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute (Permute)            (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1000, 64)          22272     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000, 64)          33024     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4096064   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 4,151,620\n",
      "Trainable params: 4,151,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "naive_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.8192 - acc: 0.3395\n",
      "Epoch 00001: val_loss improved from inf to 1.28449, saving model to ./model_checkpoints/naive_rnn_1000\n",
      "WARNING:tensorflow:From /home/ddepe/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/naive_rnn_1000/assets\n",
      "1692/1692 [==============================] - 9s 6ms/sample - loss: 1.8093 - acc: 0.3404 - val_loss: 1.2845 - val_acc: 0.3901\n",
      "Epoch 2/5\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8735 - acc: 0.6636\n",
      "Epoch 00002: val_loss did not improve from 1.28449\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 0.8721 - acc: 0.6655 - val_loss: 1.3463 - val_acc: 0.4563\n",
      "Epoch 3/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4573 - acc: 0.8618\n",
      "Epoch 00003: val_loss did not improve from 1.28449\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 0.4576 - acc: 0.8623 - val_loss: 1.3562 - val_acc: 0.4965\n",
      "Epoch 4/5\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.2042 - acc: 0.9467\n",
      "Epoch 00004: val_loss did not improve from 1.28449\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 0.2039 - acc: 0.9468 - val_loss: 1.5273 - val_acc: 0.4681\n",
      "Epoch 5/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9892\n",
      "Epoch 00005: val_loss did not improve from 1.28449\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 0.0710 - acc: 0.9894 - val_loss: 1.5320 - val_acc: 0.5154\n"
     ]
    }
   ],
   "source": [
    "naive_rnn_model_loss_hist = naive_rnn_model.fit(X_train, y_train,\n",
    "                                                validation_data = (X_valid, y_valid),\n",
    "                                                epochs = 5,\n",
    "                                                callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f766425ded0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = naive_rnn_model_loss_hist.history\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist['loss'])\n",
    "plt.plot(hist['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hist['acc'])\n",
    "plt.plot(hist['val_acc'])\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: RNN with more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we compare performance of baseline RNN and RNNs with more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1000, 64)          22272     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 1000, 64)          33024     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 1000, 64)          33024     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4096064   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 4,184,644\n",
      "Trainable params: 4,184,644\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "rnn_3_lstm_1_dense_input = layers.Input(shape=(22, 1000))\n",
    "\n",
    "# conv across channels?\n",
    "p1 = layers.Permute((2, 1))(rnn_3_lstm_1_dense_input)\n",
    "lstm1 = layers.LSTM(64, return_sequences=True)(p1)\n",
    "lstm2 = layers.LSTM(64, return_sequences=True)(lstm1)\n",
    "lstm3 = layers.LSTM(64, return_sequences=True)(lstm2)\n",
    "f2 = layers.Flatten()(lstm3)\n",
    "d2 = layers.Dense(64, activation=\"elu\")(f2)\n",
    "\n",
    "# output\n",
    "rnn_3_lstm_1_dense_output = layers.Dense(4, activation=\"softmax\")(d2)\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/rnn_3_lstm_1_dense_1000',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "rnn_3_lstm_1_dense_model = keras.Model(inputs = rnn_3_lstm_1_dense_input, outputs = rnn_3_lstm_1_dense_output)\n",
    "rnn_3_lstm_1_dense_model.compile(optimizer=\"Adam\", \n",
    "                                 loss=\"sparse_categorical_crossentropy\", \n",
    "                                 metrics=[\"acc\"])\n",
    "\n",
    "rnn_3_lstm_1_dense_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7414 - acc: 0.3329\n",
      "Epoch 00001: val_loss improved from inf to 1.31247, saving model to ./model_checkpoints/rnn_3_lstm_1_dense_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_3_lstm_1_dense_1000/assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 1.7362 - acc: 0.3327 - val_loss: 1.3125 - val_acc: 0.3783\n",
      "Epoch 2/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9508 - acc: 0.6286\n",
      "Epoch 00002: val_loss improved from 1.31247 to 1.29895, saving model to ./model_checkpoints/rnn_3_lstm_1_dense_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_3_lstm_1_dense_1000/assets\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.9541 - acc: 0.6271 - val_loss: 1.2990 - val_acc: 0.4775\n",
      "Epoch 3/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6257 - acc: 0.7873\n",
      "Epoch 00003: val_loss did not improve from 1.29895\n",
      "1692/1692 [==============================] - 5s 3ms/sample - loss: 0.6260 - acc: 0.7872 - val_loss: 1.3403 - val_acc: 0.5035\n",
      "Epoch 4/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.8798\n",
      "Epoch 00004: val_loss did not improve from 1.29895\n",
      "1692/1692 [==============================] - 4s 3ms/sample - loss: 0.3619 - acc: 0.8777 - val_loss: 1.4166 - val_acc: 0.5130\n",
      "Epoch 5/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9387\n",
      "Epoch 00005: val_loss did not improve from 1.29895\n",
      "1692/1692 [==============================] - 4s 3ms/sample - loss: 0.1937 - acc: 0.9368 - val_loss: 1.7577 - val_acc: 0.5012\n"
     ]
    }
   ],
   "source": [
    "rnn_3_lstm_1_dense_model_loss_hist = rnn_3_lstm_1_dense_model.fit(X_train, y_train,\n",
    "                                                                  validation_data = (X_valid, y_valid),\n",
    "                                                                  epochs = 5,\n",
    "                                                                  callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute_2 (Permute)          (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 1000, 64)          22272     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 1000, 64)          33024     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 1000, 64)          33024     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4096064   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 4,188,804\n",
      "Trainable params: 4,188,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "rnn_3_lstm_2_dense_input = layers.Input(shape=(22, 1000))\n",
    "\n",
    "# conv across channels?\n",
    "p1 = layers.Permute((2, 1))(rnn_3_lstm_2_dense_input)\n",
    "lstm1 = layers.LSTM(64, return_sequences=True)(p1)\n",
    "lstm2 = layers.LSTM(64, return_sequences=True)(lstm1)\n",
    "lstm3 = layers.LSTM(64, return_sequences=True)(lstm2)\n",
    "f2 = layers.Flatten()(lstm3)\n",
    "d2 = layers.Dense(64, activation=\"elu\")(f2)\n",
    "d3 = layers.Dense(64, activation=\"elu\")(d2)\n",
    "\n",
    "\n",
    "# output\n",
    "rnn_3_lstm_2_dense_output = layers.Dense(4, activation=\"softmax\")(d3)\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/rnn_3_lstm_2_dense_1000',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "rnn_3_lstm_2_dense_model = keras.Model(inputs = rnn_3_lstm_2_dense_input, outputs = rnn_3_lstm_2_dense_output)\n",
    "rnn_3_lstm_2_dense_model.compile(optimizer=\"Adam\", \n",
    "                                 loss=\"sparse_categorical_crossentropy\", \n",
    "                                 metrics=[\"acc\"])\n",
    "\n",
    "rnn_3_lstm_2_dense_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5436 - acc: 0.4014\n",
      "Epoch 00001: val_loss improved from inf to 1.19305, saving model to ./model_checkpoints/rnn_3_lstm_2_dense_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_3_lstm_2_dense_1000/assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 1.5364 - acc: 0.4019 - val_loss: 1.1931 - val_acc: 0.4775\n",
      "Epoch 2/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8281 - acc: 0.6743\n",
      "Epoch 00002: val_loss did not improve from 1.19305\n",
      "1692/1692 [==============================] - 5s 3ms/sample - loss: 0.8233 - acc: 0.6767 - val_loss: 1.2249 - val_acc: 0.4917\n",
      "Epoch 3/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4365 - acc: 0.8431\n",
      "Epoch 00003: val_loss did not improve from 1.19305\n",
      "1692/1692 [==============================] - 4s 3ms/sample - loss: 0.4389 - acc: 0.8428 - val_loss: 1.4281 - val_acc: 0.5508\n",
      "Epoch 4/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9447\n",
      "Epoch 00004: val_loss did not improve from 1.19305\n",
      "1692/1692 [==============================] - 4s 3ms/sample - loss: 0.2012 - acc: 0.9444 - val_loss: 1.6815 - val_acc: 0.5296\n",
      "Epoch 5/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9645\n",
      "Epoch 00005: val_loss did not improve from 1.19305\n",
      "1692/1692 [==============================] - 4s 3ms/sample - loss: 0.1081 - acc: 0.9645 - val_loss: 2.0727 - val_acc: 0.5225\n"
     ]
    }
   ],
   "source": [
    "rnn_3_lstm_2_dense_model_loss_hist = rnn_3_lstm_2_dense_model.fit(X_train, y_train,\n",
    "                                                                  validation_data = (X_valid, y_valid),\n",
    "                                                                  epochs = 5,\n",
    "                                                                  callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be argued, but based on the validation loss dynamics we can say that RNN does not significantly benefit from more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: augmentation of RNN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we show how RNN architecture is affected by architecture modifications, such as regularization,  number of hidden units, dropout. We start with dropout ~ 0.3 which is a common solution. Then we try different regularization, adter which variate number of hidden units and then dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_rnn_lstm_model(TIME_WINDOW=1000, hidden_units=64, dropout=0, regularizer=0):\n",
    "    # input\n",
    "    aug_rnn_input = layers.Input(shape=(22, TIME_WINDOW))\n",
    "\n",
    "    p1 = layers.Permute((2, 1))(aug_rnn_input)\n",
    "    lstm1 = layers.LSTM(hidden_units, \n",
    "                        return_sequences=True, \n",
    "                        dropout=dropout, \n",
    "                        kernel_regularizer=keras.regularizers.l2(regularizer))(p1)\n",
    "    lstm2 = layers.LSTM(hidden_units, \n",
    "                        return_sequences=True, \n",
    "                        dropout=dropout,\n",
    "                        kernel_regularizer=keras.regularizers.l2(regularizer))(lstm1)\n",
    "\n",
    "\n",
    "    f2 = layers.Flatten()(lstm2)\n",
    "    do2 = layers.Dropout(dropout)(f2)\n",
    "    elu2 = layers.Dense(hidden_units, activation=\"elu\",  kernel_regularizer=keras.regularizers.l2(regularizer))(do2)\n",
    "\n",
    "    # output\n",
    "    aug_rnn_output = layers.Dense(4, activation=\"softmax\")(elu2)\n",
    "    \n",
    "    return keras.Model(inputs = aug_rnn_input, outputs = aug_rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7934 - acc: 0.3305\n",
      "Epoch 00001: val_loss improved from inf to 1.40891, saving model to ./model_checkpoints/rnn_2_lstm_1_dense_1_conig_1000\n",
      "WARNING:tensorflow:From /home/ddepe/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_2_lstm_1_dense_1_conig_1000/assets\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.7870 - acc: 0.3316 - val_loss: 1.4089 - val_acc: 0.3286\n",
      "Epoch 2/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1302 - acc: 0.5216\n",
      "Epoch 00002: val_loss improved from 1.40891 to 1.37855, saving model to ./model_checkpoints/rnn_2_lstm_1_dense_1_conig_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_2_lstm_1_dense_1_conig_1000/assets\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 1.1333 - acc: 0.5219 - val_loss: 1.3785 - val_acc: 0.4586\n",
      "Epoch 3/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8786 - acc: 0.6623\n",
      "Epoch 00003: val_loss improved from 1.37855 to 1.28593, saving model to ./model_checkpoints/rnn_2_lstm_1_dense_1_conig_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_2_lstm_1_dense_1_conig_1000/assets\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.8773 - acc: 0.6619 - val_loss: 1.2859 - val_acc: 0.4894\n",
      "Epoch 4/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6733 - acc: 0.7644\n",
      "Epoch 00004: val_loss did not improve from 1.28593\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 0.6732 - acc: 0.7636 - val_loss: 1.3955 - val_acc: 0.5012\n",
      "Epoch 5/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4686 - acc: 0.8468\n",
      "Epoch 00005: val_loss did not improve from 1.28593\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 0.4689 - acc: 0.8463 - val_loss: 1.4913 - val_acc: 0.5201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3d8838ca90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIME_WINDOW = 1000\n",
    "HIDDEN = 64\n",
    "DROPOUT = 0.3\n",
    "REGULARIZER = 0.0001\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/rnn_2_lstm_1_dense_1_conig_1000' ,\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_00001_reg_1000 = construct_rnn_lstm_model(TIME_WINDOW, \n",
    "                                                                                  HIDDEN, \n",
    "                                                                                  DROPOUT, \n",
    "                                                                                  REGULARIZER)\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_00001_reg_1000.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_00001_reg_1000.fit(X_train, y_train,\n",
    "                                                           validation_data = (X_valid, y_valid),\n",
    "                                                           epochs = 5,\n",
    "                                                           callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.9654 - acc: 0.3570\n",
      "Epoch 00001: val_loss improved from inf to 1.62642, saving model to ./model_checkpoints/rnn_2_lstm_1_dense_2_conig_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_2_lstm_1_dense_2_conig_1000/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.9613 - acc: 0.3576 - val_loss: 1.6264 - val_acc: 0.4397\n",
      "Epoch 2/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3425 - acc: 0.6154\n",
      "Epoch 00002: val_loss did not improve from 1.62642\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.3441 - acc: 0.6141 - val_loss: 1.6350 - val_acc: 0.4586\n",
      "Epoch 3/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1136 - acc: 0.7097\n",
      "Epoch 00003: val_loss did not improve from 1.62642\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.1134 - acc: 0.7098 - val_loss: 1.6568 - val_acc: 0.4681\n",
      "Epoch 4/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9302 - acc: 0.8047\n",
      "Epoch 00004: val_loss did not improve from 1.62642\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 0.9344 - acc: 0.8032 - val_loss: 1.8838 - val_acc: 0.4917\n",
      "Epoch 5/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8626 - acc: 0.8413\n",
      "Epoch 00005: val_loss did not improve from 1.62642\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 0.8653 - acc: 0.8398 - val_loss: 2.0423 - val_acc: 0.5059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f75e85b2fd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIME_WINDOW = 1000\n",
    "HIDDEN = 64\n",
    "DROPOUT = 0.3\n",
    "REGULARIZER = 0.001\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/rnn_2_lstm_1_dense_2_conig_1000' ,\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_0001_reg_1000 = construct_rnn_lstm_model(TIME_WINDOW, \n",
    "                                                                                 HIDDEN, \n",
    "                                                                                 DROPOUT, \n",
    "                                                                                 REGULARIZER)\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_0001_reg_1000.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_0001_reg_1000.fit(X_train, y_train,\n",
    "                                                           validation_data = (X_valid, y_valid),\n",
    "                                                           epochs = 5,\n",
    "                                                           callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 3.2333 - acc: 0.3492\n",
      "Epoch 00001: val_loss improved from inf to 2.74692, saving model to ./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 3.2223 - acc: 0.3522 - val_loss: 2.7469 - val_acc: 0.4350\n",
      "Epoch 2/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.2790 - acc: 0.5883\n",
      "Epoch 00002: val_loss improved from 2.74692 to 2.44742, saving model to ./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000/assets\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 2.2834 - acc: 0.5839 - val_loss: 2.4474 - val_acc: 0.4515\n",
      "Epoch 3/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.8971 - acc: 0.6929\n",
      "Epoch 00003: val_loss improved from 2.44742 to 2.38443, saving model to ./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000/assets\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 1.9040 - acc: 0.6903 - val_loss: 2.3844 - val_acc: 0.4799\n",
      "Epoch 4/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7405 - acc: 0.7410\n",
      "Epoch 00004: val_loss did not improve from 2.38443\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.7457 - acc: 0.7382 - val_loss: 2.4700 - val_acc: 0.4681\n",
      "Epoch 5/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.6310 - acc: 0.7831\n",
      "Epoch 00005: val_loss did not improve from 2.38443\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.6331 - acc: 0.7819 - val_loss: 2.5638 - val_acc: 0.5012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f75786ebf10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIME_WINDOW = 1000\n",
    "HIDDEN = 64\n",
    "DROPOUT = 0.3\n",
    "REGULARIZER = 0.005\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000' ,\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_0005_reg_1000 = construct_rnn_lstm_model(TIME_WINDOW, \n",
    "                                                                                 HIDDEN, \n",
    "                                                                                 DROPOUT, \n",
    "                                                                                 REGULARIZER)\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_0005_reg_1000.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_0005_reg_1000.fit(X_train, y_train,\n",
    "                                                           validation_data = (X_valid, y_valid),\n",
    "                                                           epochs = 5,\n",
    "                                                           callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 4.3282 - acc: 0.3768\n",
      "Epoch 00001: val_loss improved from inf to 3.50676, saving model to ./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 4.3132 - acc: 0.3800 - val_loss: 3.5068 - val_acc: 0.4350\n",
      "Epoch 2/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.7595 - acc: 0.6076\n",
      "Epoch 00002: val_loss improved from 3.50676 to 2.88623, saving model to ./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000/assets\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 2.7542 - acc: 0.6082 - val_loss: 2.8862 - val_acc: 0.4870\n",
      "Epoch 3/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.3572 - acc: 0.6821\n",
      "Epoch 00003: val_loss improved from 2.88623 to 2.71505, saving model to ./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000/assets\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 2.3576 - acc: 0.6826 - val_loss: 2.7151 - val_acc: 0.5296\n",
      "Epoch 4/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.1543 - acc: 0.7115\n",
      "Epoch 00004: val_loss improved from 2.71505 to 2.56288, saving model to ./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000/assets\n",
      "1692/1692 [==============================] - 8s 4ms/sample - loss: 2.1578 - acc: 0.7086 - val_loss: 2.5629 - val_acc: 0.5390\n",
      "Epoch 5/5\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.0147 - acc: 0.7482\n",
      "Epoch 00005: val_loss did not improve from 2.56288\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.0186 - acc: 0.7482 - val_loss: 2.7159 - val_acc: 0.5390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f755dbde690>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIME_WINDOW = 1000\n",
    "HIDDEN = 64\n",
    "DROPOUT = 0.3\n",
    "REGULARIZER = 0.01\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/rnn_2_lstm_1_dense_3_conig_1000' ,\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_001_reg_1000 = construct_rnn_lstm_model(TIME_WINDOW, \n",
    "                                                                                HIDDEN, \n",
    "                                                                                DROPOUT, \n",
    "                                                                                REGULARIZER)\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_001_reg_1000.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "rnn_2_lstm_1_dense_64_hidden_03_dropout_001_reg_1000.fit(X_train, y_train,\n",
    "                                                         validation_data = (X_valid, y_valid),\n",
    "                                                         epochs = 5,\n",
    "                                                         callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW EXPERIMENTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to iterate through regularization through range approximately [0.1 - 0.0001], dropout - 0.2-0.5, hidden units - 32 to 128, sampl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_TIME_WINDOW = hp.HParam('TIME_WINDOW', hp.Discrete([500, 600])) \n",
    "HP_BATCH_SIZE = hp.HParam('BATCH_SIZE', hp.Discrete([32])) \n",
    "HP_HIDDEN = hp.HParam('HIDDEN', hp.Discrete([64, 128]))\n",
    "HP_DROPOUT = hp.HParam('DROPOUT', hp.Discrete([0.2, 0.3]))\n",
    "HP_REGULARIZER = hp.HParam('REGULARIZER', hp.Discrete([.001, .005]))\n",
    "HP_LEARNING = hp.HParam('LEARNING', hp.Discrete([.00001]))\n",
    "HP_BETA = hp.HParam('BETA', hp.Discrete([.9]))\n",
    "VAL_ACCURACY = 'val_accuracy'\n",
    "TEST_ACCURACY = 'test accuracy'\n",
    "\n",
    "\n",
    "X_train_norm = X_train - np.mean(X_train)/np.std(X_train)\n",
    "X_valid_norm = X_valid - np.mean(X_train)/np.std(X_train)\n",
    "X_test_norm = X_test - np.mean(X_train)/np.std(X_train)\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_TIME_WINDOW, \n",
    "#              HP_BATCH_SIZE, \n",
    "             HP_HIDDEN, \n",
    "             HP_DROPOUT, \n",
    "             HP_REGULARIZER, \n",
    "#              HP_LEARNING, \n",
    "#              HP_BETA\n",
    "            ],\n",
    "    metrics=[hp.Metric(VAL_ACCURACY, display_name='Val Accuracy'), \n",
    "             hp.Metric(TEST_ACCURACY, display_name='Test Accuracy')],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_rnn_lstm_model(hparams):\n",
    "    # input\n",
    "    aug_rnn_input = layers.Input(shape=(22, hparams[HP_TIME_WINDOW]))\n",
    "\n",
    "    p1 = layers.Permute((2, 1))(aug_rnn_input)\n",
    "    lstm1 = layers.LSTM(hparams[HP_HIDDEN], \n",
    "                        return_sequences=True, \n",
    "                        dropout=hparams[HP_DROPOUT], \n",
    "                        kernel_regularizer=keras.regularizers.l2(hparams[HP_REGULARIZER]),\n",
    "                        activity_regularizer=keras.regularizers.l2(hparams[HP_REGULARIZER]),\n",
    "                        recurrent_regularizer=keras.regularizers.l2(hparams[HP_REGULARIZER]))(p1)\n",
    "    lstm2 = layers.LSTM(hparams[HP_HIDDEN], \n",
    "                        return_sequences=True, \n",
    "                        dropout=hparams[HP_DROPOUT],\n",
    "                        kernel_regularizer=keras.regularizers.l2(hparams[HP_REGULARIZER]),\n",
    "                        activity_regularizer=keras.regularizers.l2(hparams[HP_REGULARIZER]),\n",
    "                        recurrent_regularizer=keras.regularizers.l2(hparams[HP_REGULARIZER]))(lstm1)\n",
    "\n",
    "\n",
    "    f2 = layers.Flatten()(lstm2)\n",
    "    do2 = layers.Dropout(hparams[HP_DROPOUT])(f2)\n",
    "    elu2 = layers.Dense(hparams[HP_HIDDEN], activation=\"elu\",  kernel_regularizer=keras.regularizers.l2(hparams[HP_REGULARIZER]))(do2)\n",
    "\n",
    "    # output\n",
    "    aug_rnn_output = layers.Dense(4, activation=\"softmax\")(elu2)\n",
    "    \n",
    "    return keras.Model(inputs = aug_rnn_input, outputs = aug_rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with the best accuracy \n",
    "# checkpoint_callback = [\n",
    "#     keras.callbacks.ModelCheckpoint(\n",
    "#         filepath='./model_checkpoints/rnn_lstm_model' ,\n",
    "#         # Path where to save the model\n",
    "#         # The two parameters below mean that we will overwrite\n",
    "#         # the current checkpoint if and only if\n",
    "#         # the `val_loss` score has improved.\n",
    "#         save_best_only=True,\n",
    "#         monitor='val_loss',\n",
    "#         verbose=1)\n",
    "# ]\n",
    "\n",
    "def train_test_rnn_lstm_model(hparams):\n",
    "    rnn_lstm_model = construct_rnn_lstm_model(hparams)\n",
    "    \n",
    "    adam = keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING], beta_1=hparams[HP_BETA])\n",
    "\n",
    "    rnn_lstm_model.compile(optimizer=adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    rnn_lstm_model.summary()\n",
    "    hist = rnn_lstm_model.fit(X_train_slices, y_train_slices, validation_data = (X_valid_slices, y_valid_slices), \n",
    "                              epochs = 100, \n",
    "#                               callbacks=checkpoint_callback, \n",
    "                              batch_size=hparams[HP_BATCH_SIZE]).history\n",
    "    _, accuracy = rnn_lstm_model.evaluate(X_test_slices, y_test_slices)\n",
    "    return accuracy, hist, rnn_lstm_model\n",
    "\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    test_accuracy, hist, model = train_test_rnn_lstm_model(hparams)\n",
    "    tf.summary.scalar(VAL_ACCURACY, max(hist['val_acc']), step=1)\n",
    "    tf.summary.scalar(TEST_ACCURACY, test_accuracy, step=1)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape with slices: (1692, 22, 500)\n",
      "Training label shape with slice: (1692,)\n",
      "Validation data shape with slices: (423, 22, 500)\n",
      "Validation label shape with slice: (423,)\n",
      "Testing data shape with slices: (443, 22, 500)\n",
      "Testing label shape with slice: (443,)\n",
      "--- Starting trial: run-0\n",
      "{'TIME_WINDOW': 500, 'BATCH_SIZE': 32, 'HIDDEN': 64, 'DROPOUT': 0.2, 'REGULARIZER': 0.001, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 22, 500)]         0         \n",
      "_________________________________________________________________\n",
      "permute (Permute)            (None, 500, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 500, 64)           22272     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 500, 64)           33024     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                2048064   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 2,103,620\n",
      "Trainable params: 2,103,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1692/1692 [==============================] - 5s 3ms/sample - loss: 8.0421 - acc: 0.3044 - val_loss: 8.4849 - val_acc: 0.3073\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.7583 - acc: 0.4090 - val_loss: 8.2148 - val_acc: 0.3830\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.5236 - acc: 0.4787 - val_loss: 7.9651 - val_acc: 0.4350\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.3012 - acc: 0.5248 - val_loss: 7.7489 - val_acc: 0.4113\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.1010 - acc: 0.5567 - val_loss: 7.5303 - val_acc: 0.4775\n",
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 2s 988us/sample - loss: 6.9093 - acc: 0.5987 - val_loss: 7.3340 - val_acc: 0.4941\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.7349 - acc: 0.6152 - val_loss: 7.1499 - val_acc: 0.5154\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.5780 - acc: 0.6217 - val_loss: 6.9777 - val_acc: 0.4941\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.4130 - acc: 0.6418 - val_loss: 6.8048 - val_acc: 0.5414\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.2741 - acc: 0.6495 - val_loss: 6.6432 - val_acc: 0.5461\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.1268 - acc: 0.6613 - val_loss: 6.4872 - val_acc: 0.5390\n",
      "Epoch 12/100\n",
      "1692/1692 [==============================] - 2s 977us/sample - loss: 5.9998 - acc: 0.6761 - val_loss: 6.3393 - val_acc: 0.5508\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 2s 982us/sample - loss: 5.8649 - acc: 0.6779 - val_loss: 6.1973 - val_acc: 0.5556\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.7430 - acc: 0.6879 - val_loss: 6.0609 - val_acc: 0.5697\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.6126 - acc: 0.6980 - val_loss: 5.9297 - val_acc: 0.5650\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.4960 - acc: 0.7175 - val_loss: 5.8015 - val_acc: 0.5697\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.3859 - acc: 0.7145 - val_loss: 5.6787 - val_acc: 0.5603\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.2725 - acc: 0.7311 - val_loss: 5.5619 - val_acc: 0.5579\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.1746 - acc: 0.7340 - val_loss: 5.4477 - val_acc: 0.5579\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 2s 983us/sample - loss: 5.0700 - acc: 0.7305 - val_loss: 5.3390 - val_acc: 0.5674\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.9891 - acc: 0.7199 - val_loss: 5.2333 - val_acc: 0.5650\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.8905 - acc: 0.7429 - val_loss: 5.1328 - val_acc: 0.5603\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.8113 - acc: 0.7358 - val_loss: 5.0329 - val_acc: 0.5721\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.7098 - acc: 0.7518 - val_loss: 4.9392 - val_acc: 0.5745\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.6416 - acc: 0.7500 - val_loss: 4.8493 - val_acc: 0.5674\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.5677 - acc: 0.7376 - val_loss: 4.7623 - val_acc: 0.5745\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.4823 - acc: 0.7541 - val_loss: 4.6822 - val_acc: 0.5792\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.4091 - acc: 0.7606 - val_loss: 4.5999 - val_acc: 0.5745\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.3420 - acc: 0.7707 - val_loss: 4.5259 - val_acc: 0.5887\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.2631 - acc: 0.7719 - val_loss: 4.4552 - val_acc: 0.5768\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.1941 - acc: 0.7772 - val_loss: 4.3841 - val_acc: 0.5768\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.1286 - acc: 0.7695 - val_loss: 4.3162 - val_acc: 0.5816\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.0834 - acc: 0.7677 - val_loss: 4.2520 - val_acc: 0.5792\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.0055 - acc: 0.7654 - val_loss: 4.1922 - val_acc: 0.5768\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.9516 - acc: 0.7736 - val_loss: 4.1309 - val_acc: 0.5839\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.8917 - acc: 0.7878 - val_loss: 4.0751 - val_acc: 0.5792\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.8411 - acc: 0.7801 - val_loss: 4.0190 - val_acc: 0.5816\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.7870 - acc: 0.7748 - val_loss: 3.9690 - val_acc: 0.5721\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.7315 - acc: 0.7943 - val_loss: 3.9136 - val_acc: 0.5768\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.6767 - acc: 0.7943 - val_loss: 3.8631 - val_acc: 0.5721\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.6336 - acc: 0.7902 - val_loss: 3.8162 - val_acc: 0.5674\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5901 - acc: 0.7772 - val_loss: 3.7687 - val_acc: 0.5697\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5514 - acc: 0.7926 - val_loss: 3.7249 - val_acc: 0.5650\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4980 - acc: 0.7896 - val_loss: 3.6824 - val_acc: 0.5603\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4468 - acc: 0.7943 - val_loss: 3.6415 - val_acc: 0.5603\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4116 - acc: 0.7896 - val_loss: 3.5993 - val_acc: 0.5603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3664 - acc: 0.8008 - val_loss: 3.5589 - val_acc: 0.5626\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3213 - acc: 0.8002 - val_loss: 3.5223 - val_acc: 0.5650\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2935 - acc: 0.7949 - val_loss: 3.4866 - val_acc: 0.5626\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2399 - acc: 0.8026 - val_loss: 3.4481 - val_acc: 0.5626\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2210 - acc: 0.8038 - val_loss: 3.4120 - val_acc: 0.5650\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1799 - acc: 0.7991 - val_loss: 3.3791 - val_acc: 0.5626\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1358 - acc: 0.8020 - val_loss: 3.3494 - val_acc: 0.5626\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1057 - acc: 0.7979 - val_loss: 3.3164 - val_acc: 0.5626\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0643 - acc: 0.8056 - val_loss: 3.2867 - val_acc: 0.5603\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0314 - acc: 0.8126 - val_loss: 3.2549 - val_acc: 0.5674\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9994 - acc: 0.8097 - val_loss: 3.2270 - val_acc: 0.5626\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9655 - acc: 0.8138 - val_loss: 3.2013 - val_acc: 0.5556\n",
      "Epoch 59/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9276 - acc: 0.8357 - val_loss: 3.1769 - val_acc: 0.5579\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9143 - acc: 0.8168 - val_loss: 3.1514 - val_acc: 0.5650\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8898 - acc: 0.8085 - val_loss: 3.1269 - val_acc: 0.5579\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8450 - acc: 0.8245 - val_loss: 3.1009 - val_acc: 0.5603\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8249 - acc: 0.8138 - val_loss: 3.0792 - val_acc: 0.5603\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7967 - acc: 0.8280 - val_loss: 3.0570 - val_acc: 0.5556\n",
      "Epoch 65/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7701 - acc: 0.8251 - val_loss: 3.0323 - val_acc: 0.5697\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7427 - acc: 0.8298 - val_loss: 3.0112 - val_acc: 0.5650\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7291 - acc: 0.8180 - val_loss: 2.9933 - val_acc: 0.5603\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6963 - acc: 0.8156 - val_loss: 2.9722 - val_acc: 0.5650\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6715 - acc: 0.8126 - val_loss: 2.9533 - val_acc: 0.5650\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6507 - acc: 0.8245 - val_loss: 2.9343 - val_acc: 0.5603\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6267 - acc: 0.8245 - val_loss: 2.9149 - val_acc: 0.5674\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5982 - acc: 0.8339 - val_loss: 2.8997 - val_acc: 0.5650\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5741 - acc: 0.8286 - val_loss: 2.8822 - val_acc: 0.5697\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5547 - acc: 0.8398 - val_loss: 2.8662 - val_acc: 0.5674\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5511 - acc: 0.8191 - val_loss: 2.8486 - val_acc: 0.5674\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5132 - acc: 0.8410 - val_loss: 2.8340 - val_acc: 0.5674\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5044 - acc: 0.8274 - val_loss: 2.8175 - val_acc: 0.5721\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4685 - acc: 0.8493 - val_loss: 2.8013 - val_acc: 0.5697\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4624 - acc: 0.8404 - val_loss: 2.7897 - val_acc: 0.5674\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4270 - acc: 0.8534 - val_loss: 2.7747 - val_acc: 0.5674\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4148 - acc: 0.8428 - val_loss: 2.7610 - val_acc: 0.5674\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4002 - acc: 0.8363 - val_loss: 2.7475 - val_acc: 0.5721\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 2s 997us/sample - loss: 2.3830 - acc: 0.8351 - val_loss: 2.7344 - val_acc: 0.5697\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.3747 - acc: 0.8416 - val_loss: 2.7210 - val_acc: 0.5721\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.3552 - acc: 0.8463 - val_loss: 2.7078 - val_acc: 0.5721\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.3256 - acc: 0.8469 - val_loss: 2.6962 - val_acc: 0.5674\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.3196 - acc: 0.8410 - val_loss: 2.6846 - val_acc: 0.5674\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.2937 - acc: 0.8505 - val_loss: 2.6714 - val_acc: 0.5697\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.2786 - acc: 0.8493 - val_loss: 2.6608 - val_acc: 0.5626\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.2549 - acc: 0.8528 - val_loss: 2.6506 - val_acc: 0.5697\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.2535 - acc: 0.8446 - val_loss: 2.6388 - val_acc: 0.5674\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.2386 - acc: 0.8582 - val_loss: 2.6272 - val_acc: 0.5650\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.2176 - acc: 0.8570 - val_loss: 2.6171 - val_acc: 0.5674\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.2148 - acc: 0.8351 - val_loss: 2.6072 - val_acc: 0.5674\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.1977 - acc: 0.8558 - val_loss: 2.5978 - val_acc: 0.5674\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.1727 - acc: 0.8605 - val_loss: 2.5860 - val_acc: 0.5721\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.1625 - acc: 0.8505 - val_loss: 2.5772 - val_acc: 0.5650\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.1344 - acc: 0.8623 - val_loss: 2.5665 - val_acc: 0.5721\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.1307 - acc: 0.8611 - val_loss: 2.5579 - val_acc: 0.5674\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.1212 - acc: 0.8522 - val_loss: 2.5480 - val_acc: 0.5721\n",
      "443/443 [==============================] - 0s 425us/sample - loss: 2.5995 - acc: 0.5282\n",
      "--- Starting trial: run-1\n",
      "{'TIME_WINDOW': 500, 'BATCH_SIZE': 32, 'HIDDEN': 64, 'DROPOUT': 0.2, 'REGULARIZER': 0.005, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 22, 500)]         0         \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 500, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 500, 64)           22272     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 500, 64)           33024     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2048064   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 2,103,620\n",
      "Trainable params: 2,103,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 33.7683 - acc: 0.2937 - val_loss: 35.6872 - val_acc: 0.3215\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 32.7042 - acc: 0.3676 - val_loss: 34.5190 - val_acc: 0.3853\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 31.7281 - acc: 0.4527 - val_loss: 33.4712 - val_acc: 0.4066\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 30.8328 - acc: 0.4929 - val_loss: 32.4865 - val_acc: 0.4184\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 30.0092 - acc: 0.5443 - val_loss: 31.5788 - val_acc: 0.4515\n",
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 29.2091 - acc: 0.5556 - val_loss: 30.7079 - val_acc: 0.4634\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 28.4583 - acc: 0.5792 - val_loss: 29.8665 - val_acc: 0.4563\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 27.7390 - acc: 0.5969 - val_loss: 29.0688 - val_acc: 0.4846\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 27.0584 - acc: 0.6176 - val_loss: 28.2856 - val_acc: 0.4917\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 26.3805 - acc: 0.6448 - val_loss: 27.5329 - val_acc: 0.4870\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 25.7534 - acc: 0.6472 - val_loss: 26.8056 - val_acc: 0.4988\n",
      "Epoch 12/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 25.1299 - acc: 0.6625 - val_loss: 26.0982 - val_acc: 0.4965\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 24.5246 - acc: 0.6655 - val_loss: 25.4128 - val_acc: 0.4894\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 23.9566 - acc: 0.6673 - val_loss: 24.7465 - val_acc: 0.4965\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 23.4053 - acc: 0.6779 - val_loss: 24.1038 - val_acc: 0.4846\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 22.8766 - acc: 0.6850 - val_loss: 23.4835 - val_acc: 0.4870\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 22.3470 - acc: 0.7039 - val_loss: 22.8741 - val_acc: 0.4941\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 21.8299 - acc: 0.7063 - val_loss: 22.2866 - val_acc: 0.4965\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 21.3456 - acc: 0.7033 - val_loss: 21.7191 - val_acc: 0.4870\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.8645 - acc: 0.7086 - val_loss: 21.1692 - val_acc: 0.4965\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.4081 - acc: 0.7069 - val_loss: 20.6418 - val_acc: 0.5012\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.9538 - acc: 0.7169 - val_loss: 20.1231 - val_acc: 0.5012\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.5313 - acc: 0.7275 - val_loss: 19.6268 - val_acc: 0.5012\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.1000 - acc: 0.7145 - val_loss: 19.1471 - val_acc: 0.4941\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.6936 - acc: 0.7246 - val_loss: 18.6850 - val_acc: 0.4965\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.3005 - acc: 0.7305 - val_loss: 18.2386 - val_acc: 0.4988\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.9182 - acc: 0.7340 - val_loss: 17.8046 - val_acc: 0.4965\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.5569 - acc: 0.7175 - val_loss: 17.3873 - val_acc: 0.4894\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.1906 - acc: 0.7429 - val_loss: 16.9856 - val_acc: 0.4988\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.8506 - acc: 0.7453 - val_loss: 16.5978 - val_acc: 0.5059\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.5117 - acc: 0.7494 - val_loss: 16.2225 - val_acc: 0.5130\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.1903 - acc: 0.7335 - val_loss: 15.8635 - val_acc: 0.5154\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.8604 - acc: 0.7583 - val_loss: 15.5168 - val_acc: 0.5272\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.5731 - acc: 0.7405 - val_loss: 15.1831 - val_acc: 0.5296\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.2771 - acc: 0.7476 - val_loss: 14.8613 - val_acc: 0.5296\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.9964 - acc: 0.7252 - val_loss: 14.5519 - val_acc: 0.5272\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.7111 - acc: 0.7642 - val_loss: 14.2515 - val_acc: 0.5272\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.4327 - acc: 0.7541 - val_loss: 13.9649 - val_acc: 0.5343\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.1730 - acc: 0.7606 - val_loss: 13.6872 - val_acc: 0.5366\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.9190 - acc: 0.7612 - val_loss: 13.4220 - val_acc: 0.5366\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.6774 - acc: 0.7648 - val_loss: 13.1649 - val_acc: 0.5414\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.4546 - acc: 0.7488 - val_loss: 12.9161 - val_acc: 0.5366\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.2151 - acc: 0.7595 - val_loss: 12.6772 - val_acc: 0.5390\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.0076 - acc: 0.7577 - val_loss: 12.4481 - val_acc: 0.5414\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.7635 - acc: 0.7689 - val_loss: 12.2260 - val_acc: 0.5414\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.5631 - acc: 0.7600 - val_loss: 12.0158 - val_acc: 0.5508\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.3646 - acc: 0.7589 - val_loss: 11.8091 - val_acc: 0.5414\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.1552 - acc: 0.7494 - val_loss: 11.6096 - val_acc: 0.5414\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.9659 - acc: 0.7636 - val_loss: 11.4165 - val_acc: 0.5366\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.7711 - acc: 0.7660 - val_loss: 11.2284 - val_acc: 0.5366\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.5826 - acc: 0.7725 - val_loss: 11.0474 - val_acc: 0.5390\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.4137 - acc: 0.7665 - val_loss: 10.8723 - val_acc: 0.5343\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.2230 - acc: 0.7713 - val_loss: 10.7014 - val_acc: 0.5319\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.0566 - acc: 0.7689 - val_loss: 10.5356 - val_acc: 0.5319\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.8975 - acc: 0.7707 - val_loss: 10.3767 - val_acc: 0.5319\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.7561 - acc: 0.7571 - val_loss: 10.2223 - val_acc: 0.5319\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.5914 - acc: 0.7707 - val_loss: 10.0730 - val_acc: 0.5296\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.4250 - acc: 0.7689 - val_loss: 9.9298 - val_acc: 0.5343\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.2897 - acc: 0.7701 - val_loss: 9.7910 - val_acc: 0.5343\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.1341 - acc: 0.7748 - val_loss: 9.6554 - val_acc: 0.5272\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.9973 - acc: 0.7748 - val_loss: 9.5245 - val_acc: 0.5272\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.8641 - acc: 0.7866 - val_loss: 9.3961 - val_acc: 0.5201\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.7395 - acc: 0.7583 - val_loss: 9.2741 - val_acc: 0.5201\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.5946 - acc: 0.7671 - val_loss: 9.1538 - val_acc: 0.5225\n",
      "Epoch 65/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.4879 - acc: 0.7665 - val_loss: 9.0380 - val_acc: 0.5343\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.3548 - acc: 0.7683 - val_loss: 8.9265 - val_acc: 0.5296\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.2326 - acc: 0.7937 - val_loss: 8.8169 - val_acc: 0.5319\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.1232 - acc: 0.7713 - val_loss: 8.7105 - val_acc: 0.5343\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.9992 - acc: 0.7813 - val_loss: 8.6062 - val_acc: 0.5319\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.8928 - acc: 0.7866 - val_loss: 8.5047 - val_acc: 0.5319\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.7790 - acc: 0.7837 - val_loss: 8.4062 - val_acc: 0.5319\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.6795 - acc: 0.7772 - val_loss: 8.3097 - val_acc: 0.5366\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.5676 - acc: 0.7772 - val_loss: 8.2168 - val_acc: 0.5508\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.4644 - acc: 0.7796 - val_loss: 8.1256 - val_acc: 0.5508\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.3604 - acc: 0.7813 - val_loss: 8.0346 - val_acc: 0.5556\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.2560 - acc: 0.7855 - val_loss: 7.9488 - val_acc: 0.5508\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.1701 - acc: 0.7985 - val_loss: 7.8621 - val_acc: 0.5532\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.0766 - acc: 0.7801 - val_loss: 7.7806 - val_acc: 0.5532\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.9751 - acc: 0.7766 - val_loss: 7.6981 - val_acc: 0.5556\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.8917 - acc: 0.7931 - val_loss: 7.6183 - val_acc: 0.5485\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.8007 - acc: 0.7855 - val_loss: 7.5388 - val_acc: 0.5485\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.7143 - acc: 0.7825 - val_loss: 7.4615 - val_acc: 0.5485\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.6343 - acc: 0.7843 - val_loss: 7.3866 - val_acc: 0.5461\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.5414 - acc: 0.7855 - val_loss: 7.3115 - val_acc: 0.5532\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.4759 - acc: 0.7914 - val_loss: 7.2392 - val_acc: 0.5532\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.3930 - acc: 0.7772 - val_loss: 7.1672 - val_acc: 0.5485\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.3119 - acc: 0.7884 - val_loss: 7.0960 - val_acc: 0.5556\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.2296 - acc: 0.8044 - val_loss: 7.0267 - val_acc: 0.5556\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.1633 - acc: 0.7778 - val_loss: 6.9598 - val_acc: 0.5579\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.0901 - acc: 0.8020 - val_loss: 6.8931 - val_acc: 0.5603\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.0066 - acc: 0.7991 - val_loss: 6.8274 - val_acc: 0.5532\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.9480 - acc: 0.7855 - val_loss: 6.7621 - val_acc: 0.5556\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.8682 - acc: 0.7884 - val_loss: 6.6989 - val_acc: 0.5508\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.8176 - acc: 0.7825 - val_loss: 6.6358 - val_acc: 0.5579\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.7388 - acc: 0.7991 - val_loss: 6.5761 - val_acc: 0.5603\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.6781 - acc: 0.7849 - val_loss: 6.5164 - val_acc: 0.5579\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.6262 - acc: 0.7772 - val_loss: 6.4571 - val_acc: 0.5603\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.5505 - acc: 0.7961 - val_loss: 6.3981 - val_acc: 0.5650\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.4885 - acc: 0.7908 - val_loss: 6.3416 - val_acc: 0.5674\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.4255 - acc: 0.7890 - val_loss: 6.2871 - val_acc: 0.5674\n",
      "443/443 [==============================] - 0s 500us/sample - loss: 6.2899 - acc: 0.5305\n",
      "--- Starting trial: run-2\n",
      "{'TIME_WINDOW': 500, 'BATCH_SIZE': 32, 'HIDDEN': 64, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 22, 500)]         0         \n",
      "_________________________________________________________________\n",
      "permute_2 (Permute)          (None, 500, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 500, 64)           22272     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 500, 64)           33024     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                2048064   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 2,103,620\n",
      "Trainable params: 2,103,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 7.8249 - acc: 0.3020 - val_loss: 8.4560 - val_acc: 0.3333\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.5790 - acc: 0.3658 - val_loss: 8.1745 - val_acc: 0.3853\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.3639 - acc: 0.4273 - val_loss: 7.9287 - val_acc: 0.4066\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.1796 - acc: 0.4456 - val_loss: 7.7070 - val_acc: 0.4161\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.0002 - acc: 0.4622 - val_loss: 7.5021 - val_acc: 0.4303\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.8336 - acc: 0.4882 - val_loss: 7.3063 - val_acc: 0.4184\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.6692 - acc: 0.5189 - val_loss: 7.1206 - val_acc: 0.4468\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.5232 - acc: 0.5355 - val_loss: 6.9426 - val_acc: 0.4515\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.3833 - acc: 0.5520 - val_loss: 6.7737 - val_acc: 0.4610\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.2555 - acc: 0.5691 - val_loss: 6.6092 - val_acc: 0.4752\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.1217 - acc: 0.5816 - val_loss: 6.4574 - val_acc: 0.4941\n",
      "Epoch 12/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.0044 - acc: 0.6011 - val_loss: 6.3174 - val_acc: 0.4870\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.8844 - acc: 0.6034 - val_loss: 6.1742 - val_acc: 0.5035\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.7788 - acc: 0.5934 - val_loss: 6.0421 - val_acc: 0.5035\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.6728 - acc: 0.6170 - val_loss: 5.9199 - val_acc: 0.5035\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.5586 - acc: 0.6318 - val_loss: 5.8005 - val_acc: 0.5130\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.4804 - acc: 0.6082 - val_loss: 5.6852 - val_acc: 0.5035\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.3725 - acc: 0.6235 - val_loss: 5.5744 - val_acc: 0.5012\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.2843 - acc: 0.6324 - val_loss: 5.4744 - val_acc: 0.4965\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.1849 - acc: 0.6584 - val_loss: 5.3769 - val_acc: 0.4846\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.1094 - acc: 0.6501 - val_loss: 5.2818 - val_acc: 0.4870\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.0231 - acc: 0.6513 - val_loss: 5.1936 - val_acc: 0.4917\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.9422 - acc: 0.6625 - val_loss: 5.1047 - val_acc: 0.4846\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.8736 - acc: 0.6619 - val_loss: 5.0240 - val_acc: 0.4846\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.8020 - acc: 0.6613 - val_loss: 4.9411 - val_acc: 0.4870\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.7165 - acc: 0.6767 - val_loss: 4.8609 - val_acc: 0.4894\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.6478 - acc: 0.6903 - val_loss: 4.7843 - val_acc: 0.4894\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.5634 - acc: 0.6950 - val_loss: 4.7129 - val_acc: 0.4894\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.5102 - acc: 0.6791 - val_loss: 4.6401 - val_acc: 0.4917\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.4414 - acc: 0.7027 - val_loss: 4.5716 - val_acc: 0.5059\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.3814 - acc: 0.6992 - val_loss: 4.5104 - val_acc: 0.5083\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.3295 - acc: 0.6921 - val_loss: 4.4375 - val_acc: 0.5130\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.2655 - acc: 0.7157 - val_loss: 4.3793 - val_acc: 0.5106\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.2152 - acc: 0.6927 - val_loss: 4.3173 - val_acc: 0.5154\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.1469 - acc: 0.7110 - val_loss: 4.2605 - val_acc: 0.5177\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.1006 - acc: 0.7051 - val_loss: 4.2072 - val_acc: 0.5130\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.0454 - acc: 0.6891 - val_loss: 4.1483 - val_acc: 0.5106\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.9953 - acc: 0.7015 - val_loss: 4.1015 - val_acc: 0.5059\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.9508 - acc: 0.7069 - val_loss: 4.0491 - val_acc: 0.5106\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.8998 - acc: 0.7074 - val_loss: 4.0021 - val_acc: 0.4988\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.8428 - acc: 0.7116 - val_loss: 3.9449 - val_acc: 0.5059\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.7874 - acc: 0.7364 - val_loss: 3.8974 - val_acc: 0.5083\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.7613 - acc: 0.7234 - val_loss: 3.8530 - val_acc: 0.5106\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.7133 - acc: 0.7104 - val_loss: 3.8088 - val_acc: 0.5154\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.6780 - acc: 0.7145 - val_loss: 3.7672 - val_acc: 0.5106\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.6249 - acc: 0.7169 - val_loss: 3.7277 - val_acc: 0.5154\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5860 - acc: 0.7204 - val_loss: 3.6860 - val_acc: 0.5154\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5371 - acc: 0.7293 - val_loss: 3.6495 - val_acc: 0.5225\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5259 - acc: 0.7163 - val_loss: 3.6088 - val_acc: 0.5296\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4653 - acc: 0.7329 - val_loss: 3.5774 - val_acc: 0.5272\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4266 - acc: 0.7305 - val_loss: 3.5402 - val_acc: 0.5272\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3909 - acc: 0.7299 - val_loss: 3.5097 - val_acc: 0.5225\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3569 - acc: 0.7311 - val_loss: 3.4714 - val_acc: 0.5248\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3348 - acc: 0.7234 - val_loss: 3.4400 - val_acc: 0.5201\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2910 - acc: 0.7258 - val_loss: 3.4134 - val_acc: 0.5272\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2677 - acc: 0.7340 - val_loss: 3.3820 - val_acc: 0.5272\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2214 - acc: 0.7405 - val_loss: 3.3526 - val_acc: 0.5272\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1997 - acc: 0.7287 - val_loss: 3.3243 - val_acc: 0.5272\n",
      "Epoch 59/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1666 - acc: 0.7281 - val_loss: 3.2905 - val_acc: 0.5366\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1255 - acc: 0.7547 - val_loss: 3.2668 - val_acc: 0.5390\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0953 - acc: 0.7270 - val_loss: 3.2425 - val_acc: 0.5414\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0729 - acc: 0.7346 - val_loss: 3.2153 - val_acc: 0.5366\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0579 - acc: 0.7258 - val_loss: 3.1932 - val_acc: 0.5390\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0182 - acc: 0.7329 - val_loss: 3.1695 - val_acc: 0.5343\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9878 - acc: 0.7470 - val_loss: 3.1503 - val_acc: 0.5296\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9592 - acc: 0.7459 - val_loss: 3.1268 - val_acc: 0.5366\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9419 - acc: 0.7411 - val_loss: 3.1042 - val_acc: 0.5319\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9142 - acc: 0.7530 - val_loss: 3.0826 - val_acc: 0.5414\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8983 - acc: 0.7376 - val_loss: 3.0610 - val_acc: 0.5343\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8553 - acc: 0.7530 - val_loss: 3.0399 - val_acc: 0.5390\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8359 - acc: 0.7553 - val_loss: 3.0190 - val_acc: 0.5414\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8277 - acc: 0.7547 - val_loss: 3.0020 - val_acc: 0.5390\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7833 - acc: 0.7618 - val_loss: 2.9868 - val_acc: 0.5414\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7693 - acc: 0.7459 - val_loss: 2.9677 - val_acc: 0.5437\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7532 - acc: 0.7524 - val_loss: 2.9484 - val_acc: 0.5366\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7240 - acc: 0.7618 - val_loss: 2.9285 - val_acc: 0.5366\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7144 - acc: 0.7453 - val_loss: 2.9119 - val_acc: 0.5437\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6888 - acc: 0.7518 - val_loss: 2.8948 - val_acc: 0.5414\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6709 - acc: 0.7547 - val_loss: 2.8775 - val_acc: 0.5390\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6531 - acc: 0.7524 - val_loss: 2.8648 - val_acc: 0.5343\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6352 - acc: 0.7482 - val_loss: 2.8479 - val_acc: 0.5366\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6158 - acc: 0.7683 - val_loss: 2.8352 - val_acc: 0.5366\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5900 - acc: 0.7695 - val_loss: 2.8200 - val_acc: 0.5390\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5748 - acc: 0.7624 - val_loss: 2.8053 - val_acc: 0.5437\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5537 - acc: 0.7636 - val_loss: 2.7910 - val_acc: 0.5485\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5437 - acc: 0.7589 - val_loss: 2.7763 - val_acc: 0.5485\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5292 - acc: 0.7571 - val_loss: 2.7622 - val_acc: 0.5508\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5077 - acc: 0.7624 - val_loss: 2.7514 - val_acc: 0.5508\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4969 - acc: 0.7600 - val_loss: 2.7393 - val_acc: 0.5508\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4729 - acc: 0.7630 - val_loss: 2.7274 - val_acc: 0.5485\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4582 - acc: 0.7713 - val_loss: 2.7120 - val_acc: 0.5485\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4453 - acc: 0.7642 - val_loss: 2.7022 - val_acc: 0.5485\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4402 - acc: 0.7689 - val_loss: 2.6882 - val_acc: 0.5485\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4100 - acc: 0.7719 - val_loss: 2.6763 - val_acc: 0.5485\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.3968 - acc: 0.7778 - val_loss: 2.6675 - val_acc: 0.5437\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.3799 - acc: 0.7736 - val_loss: 2.6603 - val_acc: 0.5508\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.3714 - acc: 0.7671 - val_loss: 2.6508 - val_acc: 0.5461\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.3497 - acc: 0.7760 - val_loss: 2.6367 - val_acc: 0.5485\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.3553 - acc: 0.7642 - val_loss: 2.6258 - val_acc: 0.5485\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.3251 - acc: 0.7866 - val_loss: 2.6170 - val_acc: 0.5485\n",
      "443/443 [==============================] - 0s 464us/sample - loss: 2.6347 - acc: 0.5440\n",
      "--- Starting trial: run-3\n",
      "{'TIME_WINDOW': 500, 'BATCH_SIZE': 32, 'HIDDEN': 64, 'DROPOUT': 0.3, 'REGULARIZER': 0.005, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 22, 500)]         0         \n",
      "_________________________________________________________________\n",
      "permute_3 (Permute)          (None, 500, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 500, 64)           22272     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 500, 64)           33024     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                2048064   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 2,103,620\n",
      "Trainable params: 2,103,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 34.4551 - acc: 0.2825 - val_loss: 37.5881 - val_acc: 0.3593\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 33.3975 - acc: 0.3930 - val_loss: 36.3032 - val_acc: 0.3972\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 32.4791 - acc: 0.4096 - val_loss: 35.0967 - val_acc: 0.4468\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 31.6204 - acc: 0.4586 - val_loss: 33.9938 - val_acc: 0.4515\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 30.8110 - acc: 0.4870 - val_loss: 32.9588 - val_acc: 0.4586\n",
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 30.0283 - acc: 0.5165 - val_loss: 31.9756 - val_acc: 0.4657\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 29.2852 - acc: 0.5473 - val_loss: 31.0522 - val_acc: 0.4586\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 28.5837 - acc: 0.5774 - val_loss: 30.1425 - val_acc: 0.4492\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 27.9087 - acc: 0.5810 - val_loss: 29.2826 - val_acc: 0.4492\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 27.2705 - acc: 0.5792 - val_loss: 28.4594 - val_acc: 0.4468\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 26.6316 - acc: 0.5863 - val_loss: 27.6637 - val_acc: 0.4563\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 26.0262 - acc: 0.6117 - val_loss: 26.8992 - val_acc: 0.4515\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 25.4273 - acc: 0.6188 - val_loss: 26.1630 - val_acc: 0.4610\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 24.8708 - acc: 0.6306 - val_loss: 25.4464 - val_acc: 0.4681\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 24.3182 - acc: 0.6200 - val_loss: 24.7623 - val_acc: 0.4704\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 23.7822 - acc: 0.6495 - val_loss: 24.0987 - val_acc: 0.4728\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 23.2610 - acc: 0.6241 - val_loss: 23.4546 - val_acc: 0.4775\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 22.7513 - acc: 0.6383 - val_loss: 22.8379 - val_acc: 0.4823\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 22.2599 - acc: 0.6643 - val_loss: 22.2311 - val_acc: 0.4965\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 21.7750 - acc: 0.6572 - val_loss: 21.6452 - val_acc: 0.5106\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 21.3147 - acc: 0.6814 - val_loss: 21.0805 - val_acc: 0.5035\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.8840 - acc: 0.6649 - val_loss: 20.5338 - val_acc: 0.5035\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.4594 - acc: 0.6637 - val_loss: 19.9984 - val_acc: 0.5106\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.0307 - acc: 0.6773 - val_loss: 19.4872 - val_acc: 0.5130\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.6244 - acc: 0.6761 - val_loss: 18.9891 - val_acc: 0.5154\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.2446 - acc: 0.6619 - val_loss: 18.5074 - val_acc: 0.5059\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.8581 - acc: 0.6738 - val_loss: 18.0476 - val_acc: 0.5059\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.4765 - acc: 0.6903 - val_loss: 17.6032 - val_acc: 0.5035\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.1327 - acc: 0.6903 - val_loss: 17.1777 - val_acc: 0.4988\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.7972 - acc: 0.6785 - val_loss: 16.7619 - val_acc: 0.4941\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.4594 - acc: 0.6921 - val_loss: 16.3687 - val_acc: 0.5012\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.1294 - acc: 0.6968 - val_loss: 15.9907 - val_acc: 0.5059\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.8303 - acc: 0.6761 - val_loss: 15.6320 - val_acc: 0.5106\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.5178 - acc: 0.6885 - val_loss: 15.2797 - val_acc: 0.5130\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.2127 - acc: 0.6814 - val_loss: 14.9459 - val_acc: 0.5154\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.9212 - acc: 0.6897 - val_loss: 14.6274 - val_acc: 0.5201\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.6428 - acc: 0.6915 - val_loss: 14.3181 - val_acc: 0.5225\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.3856 - acc: 0.6897 - val_loss: 14.0254 - val_acc: 0.5225\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.1025 - acc: 0.6998 - val_loss: 13.7439 - val_acc: 0.5201\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.8523 - acc: 0.7169 - val_loss: 13.4745 - val_acc: 0.5248\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.6012 - acc: 0.7021 - val_loss: 13.2143 - val_acc: 0.5272\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.3500 - acc: 0.7104 - val_loss: 12.9663 - val_acc: 0.5248\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.1226 - acc: 0.6962 - val_loss: 12.7277 - val_acc: 0.5225\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.8819 - acc: 0.7027 - val_loss: 12.5002 - val_acc: 0.5225\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.6534 - acc: 0.6844 - val_loss: 12.2810 - val_acc: 0.5201\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.4126 - acc: 0.7193 - val_loss: 12.0701 - val_acc: 0.5201\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.2186 - acc: 0.6980 - val_loss: 11.8668 - val_acc: 0.5225\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.9958 - acc: 0.7027 - val_loss: 11.6682 - val_acc: 0.5248\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.8013 - acc: 0.7187 - val_loss: 11.4803 - val_acc: 0.5248\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.6010 - acc: 0.7051 - val_loss: 11.3003 - val_acc: 0.5272\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.4048 - acc: 0.7069 - val_loss: 11.1218 - val_acc: 0.5225\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.2225 - acc: 0.7151 - val_loss: 10.9519 - val_acc: 0.5225\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.0389 - acc: 0.7074 - val_loss: 10.7870 - val_acc: 0.5201\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.8493 - acc: 0.7128 - val_loss: 10.6259 - val_acc: 0.5225\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.6701 - acc: 0.7151 - val_loss: 10.4670 - val_acc: 0.5177\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.5021 - acc: 0.7104 - val_loss: 10.3161 - val_acc: 0.5225\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.3437 - acc: 0.7074 - val_loss: 10.1687 - val_acc: 0.5201\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.1786 - acc: 0.7169 - val_loss: 10.0246 - val_acc: 0.5225\n",
      "Epoch 59/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.0326 - acc: 0.6962 - val_loss: 9.8829 - val_acc: 0.5177\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.8737 - acc: 0.6962 - val_loss: 9.7453 - val_acc: 0.5177\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.7140 - acc: 0.7228 - val_loss: 9.6114 - val_acc: 0.5296\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.5821 - acc: 0.6933 - val_loss: 9.4767 - val_acc: 0.5319\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.4416 - acc: 0.7098 - val_loss: 9.3486 - val_acc: 0.5343\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.2925 - acc: 0.7116 - val_loss: 9.2262 - val_acc: 0.5414\n",
      "Epoch 65/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.1689 - acc: 0.7051 - val_loss: 9.1052 - val_acc: 0.5461\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.0424 - acc: 0.6897 - val_loss: 8.9855 - val_acc: 0.5437\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.9132 - acc: 0.7051 - val_loss: 8.8679 - val_acc: 0.5437\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.7800 - acc: 0.7027 - val_loss: 8.7573 - val_acc: 0.5485\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.6593 - acc: 0.7009 - val_loss: 8.6476 - val_acc: 0.5461\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.5392 - acc: 0.7063 - val_loss: 8.5417 - val_acc: 0.5508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.4326 - acc: 0.7051 - val_loss: 8.4371 - val_acc: 0.5508\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.3079 - acc: 0.7110 - val_loss: 8.3360 - val_acc: 0.5485\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.2023 - acc: 0.6980 - val_loss: 8.2388 - val_acc: 0.5461\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.0984 - acc: 0.7145 - val_loss: 8.1435 - val_acc: 0.5437\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.9910 - acc: 0.6909 - val_loss: 8.0500 - val_acc: 0.5414\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.8726 - acc: 0.7092 - val_loss: 7.9588 - val_acc: 0.5414\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.7796 - acc: 0.7122 - val_loss: 7.8663 - val_acc: 0.5461\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.6938 - acc: 0.6980 - val_loss: 7.7810 - val_acc: 0.5437\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.5919 - acc: 0.7098 - val_loss: 7.6950 - val_acc: 0.5461\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.4992 - acc: 0.7122 - val_loss: 7.6129 - val_acc: 0.5461\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.4030 - acc: 0.7080 - val_loss: 7.5304 - val_acc: 0.5485\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.3156 - acc: 0.6980 - val_loss: 7.4502 - val_acc: 0.5485\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.2103 - acc: 0.7252 - val_loss: 7.3712 - val_acc: 0.5532\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.1373 - acc: 0.7204 - val_loss: 7.2979 - val_acc: 0.5508\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.0540 - acc: 0.7175 - val_loss: 7.2243 - val_acc: 0.5532\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.9730 - acc: 0.7009 - val_loss: 7.1514 - val_acc: 0.5532\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.8903 - acc: 0.6998 - val_loss: 7.0801 - val_acc: 0.5532\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.8011 - acc: 0.7128 - val_loss: 7.0102 - val_acc: 0.5508\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.7375 - acc: 0.7264 - val_loss: 6.9437 - val_acc: 0.5508\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.6570 - acc: 0.6962 - val_loss: 6.8779 - val_acc: 0.5508\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.5743 - acc: 0.7098 - val_loss: 6.8131 - val_acc: 0.5485\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.4964 - acc: 0.7193 - val_loss: 6.7482 - val_acc: 0.5485\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.4274 - acc: 0.7311 - val_loss: 6.6822 - val_acc: 0.5556\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.3626 - acc: 0.6986 - val_loss: 6.6213 - val_acc: 0.5556\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.2805 - acc: 0.7092 - val_loss: 6.5599 - val_acc: 0.5556\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.2177 - acc: 0.7092 - val_loss: 6.5019 - val_acc: 0.5532\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.1541 - acc: 0.7004 - val_loss: 6.4441 - val_acc: 0.5532\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.0838 - acc: 0.7157 - val_loss: 6.3851 - val_acc: 0.5556\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.0317 - acc: 0.6927 - val_loss: 6.3295 - val_acc: 0.5532\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.9533 - acc: 0.7222 - val_loss: 6.2738 - val_acc: 0.5556\n",
      "443/443 [==============================] - 0s 435us/sample - loss: 6.2982 - acc: 0.5079\n",
      "--- Starting trial: run-4\n",
      "{'TIME_WINDOW': 500, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.2, 'REGULARIZER': 0.001, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 22, 500)]         0         \n",
      "_________________________________________________________________\n",
      "permute_4 (Permute)          (None, 500, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 500, 128)          77312     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 500, 128)          131584    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               8192128   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,401,540\n",
      "Trainable params: 8,401,540\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 13.4152 - acc: 0.3369 - val_loss: 13.7364 - val_acc: 0.4681\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.6882 - acc: 0.5071 - val_loss: 13.0677 - val_acc: 0.5177\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.1125 - acc: 0.5898 - val_loss: 12.5159 - val_acc: 0.4965\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.6062 - acc: 0.6324 - val_loss: 12.0056 - val_acc: 0.5177\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.1522 - acc: 0.6897 - val_loss: 11.5808 - val_acc: 0.4941\n",
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.7391 - acc: 0.7104 - val_loss: 11.1477 - val_acc: 0.5272\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.3518 - acc: 0.7370 - val_loss: 10.7795 - val_acc: 0.5059\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.9918 - acc: 0.7630 - val_loss: 10.3773 - val_acc: 0.5508\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.6656 - acc: 0.7784 - val_loss: 10.0530 - val_acc: 0.5366\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.3505 - acc: 0.7926 - val_loss: 9.7244 - val_acc: 0.5366\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.0475 - acc: 0.8097 - val_loss: 9.4141 - val_acc: 0.5461\n",
      "Epoch 12/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.7659 - acc: 0.8245 - val_loss: 9.1295 - val_acc: 0.5390\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.5174 - acc: 0.8215 - val_loss: 8.8368 - val_acc: 0.5532\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.2658 - acc: 0.8292 - val_loss: 8.5799 - val_acc: 0.5626\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.0330 - acc: 0.8322 - val_loss: 8.3236 - val_acc: 0.5650\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.8022 - acc: 0.8487 - val_loss: 8.0810 - val_acc: 0.5603\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.5852 - acc: 0.8629 - val_loss: 7.8518 - val_acc: 0.5697\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.3890 - acc: 0.8682 - val_loss: 7.6299 - val_acc: 0.5697\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.1975 - acc: 0.8676 - val_loss: 7.4404 - val_acc: 0.5721\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.0278 - acc: 0.8753 - val_loss: 7.2442 - val_acc: 0.5650\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.8576 - acc: 0.8682 - val_loss: 7.0612 - val_acc: 0.5745\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.7006 - acc: 0.8694 - val_loss: 6.8934 - val_acc: 0.5556\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.5465 - acc: 0.8688 - val_loss: 6.7351 - val_acc: 0.5626\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.3871 - acc: 0.8853 - val_loss: 6.5874 - val_acc: 0.5603\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.2448 - acc: 0.8889 - val_loss: 6.4453 - val_acc: 0.5603\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.1172 - acc: 0.8913 - val_loss: 6.3129 - val_acc: 0.5556\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.9849 - acc: 0.8936 - val_loss: 6.1818 - val_acc: 0.5579\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.8618 - acc: 0.8907 - val_loss: 6.0606 - val_acc: 0.5556\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.7496 - acc: 0.8960 - val_loss: 5.9493 - val_acc: 0.5579\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.6254 - acc: 0.9060 - val_loss: 5.8370 - val_acc: 0.5697\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.5215 - acc: 0.9054 - val_loss: 5.7295 - val_acc: 0.5603\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.4272 - acc: 0.9025 - val_loss: 5.6335 - val_acc: 0.5697\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.3196 - acc: 0.8954 - val_loss: 5.5365 - val_acc: 0.5626\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.2318 - acc: 0.8995 - val_loss: 5.4438 - val_acc: 0.5674\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.1304 - acc: 0.9025 - val_loss: 5.3628 - val_acc: 0.5603\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.0401 - acc: 0.9007 - val_loss: 5.2706 - val_acc: 0.5697\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.9568 - acc: 0.9143 - val_loss: 5.1973 - val_acc: 0.5650\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.8749 - acc: 0.9054 - val_loss: 5.1194 - val_acc: 0.5603\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.7913 - acc: 0.9113 - val_loss: 5.0533 - val_acc: 0.5697\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.7157 - acc: 0.9149 - val_loss: 4.9783 - val_acc: 0.5697\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.6248 - acc: 0.9184 - val_loss: 4.9064 - val_acc: 0.5745\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.5639 - acc: 0.9090 - val_loss: 4.8491 - val_acc: 0.5745\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.4957 - acc: 0.9173 - val_loss: 4.7797 - val_acc: 0.5745\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.4369 - acc: 0.9060 - val_loss: 4.7228 - val_acc: 0.5768\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.3559 - acc: 0.9255 - val_loss: 4.6616 - val_acc: 0.5839\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.3006 - acc: 0.9261 - val_loss: 4.6067 - val_acc: 0.5816\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.2369 - acc: 0.9214 - val_loss: 4.5512 - val_acc: 0.5863\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.1703 - acc: 0.9320 - val_loss: 4.4960 - val_acc: 0.5816\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.1131 - acc: 0.9297 - val_loss: 4.4490 - val_acc: 0.5816\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.0498 - acc: 0.9379 - val_loss: 4.4001 - val_acc: 0.5816\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.0061 - acc: 0.9273 - val_loss: 4.3458 - val_acc: 0.5816\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.9482 - acc: 0.9379 - val_loss: 4.3027 - val_acc: 0.5768\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.9021 - acc: 0.9362 - val_loss: 4.2588 - val_acc: 0.5768\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.8517 - acc: 0.9255 - val_loss: 4.2154 - val_acc: 0.5792\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.8028 - acc: 0.9309 - val_loss: 4.1790 - val_acc: 0.5768\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.7553 - acc: 0.9338 - val_loss: 4.1388 - val_acc: 0.5792\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.7030 - acc: 0.9409 - val_loss: 4.1036 - val_acc: 0.5816\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.6583 - acc: 0.9350 - val_loss: 4.0656 - val_acc: 0.5792\n",
      "Epoch 59/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.6187 - acc: 0.9379 - val_loss: 4.0282 - val_acc: 0.5863\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5762 - acc: 0.9326 - val_loss: 3.9934 - val_acc: 0.5792\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5403 - acc: 0.9309 - val_loss: 3.9613 - val_acc: 0.5863\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5001 - acc: 0.9303 - val_loss: 3.9281 - val_acc: 0.5816\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4620 - acc: 0.9255 - val_loss: 3.8951 - val_acc: 0.5816\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4204 - acc: 0.9391 - val_loss: 3.8685 - val_acc: 0.5768\n",
      "Epoch 65/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3767 - acc: 0.9397 - val_loss: 3.8352 - val_acc: 0.5768\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3517 - acc: 0.9421 - val_loss: 3.8047 - val_acc: 0.5721\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3055 - acc: 0.9421 - val_loss: 3.7856 - val_acc: 0.5697\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2737 - acc: 0.9468 - val_loss: 3.7653 - val_acc: 0.5721\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2494 - acc: 0.9309 - val_loss: 3.7386 - val_acc: 0.5674\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2150 - acc: 0.9379 - val_loss: 3.7003 - val_acc: 0.5697\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1886 - acc: 0.9397 - val_loss: 3.6812 - val_acc: 0.5745\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1509 - acc: 0.9368 - val_loss: 3.6600 - val_acc: 0.5721\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1229 - acc: 0.9450 - val_loss: 3.6376 - val_acc: 0.5745\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0919 - acc: 0.9409 - val_loss: 3.6181 - val_acc: 0.5768\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0679 - acc: 0.9433 - val_loss: 3.5971 - val_acc: 0.5768\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0366 - acc: 0.9456 - val_loss: 3.5673 - val_acc: 0.5697\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0023 - acc: 0.9515 - val_loss: 3.5619 - val_acc: 0.5697\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9829 - acc: 0.9486 - val_loss: 3.5409 - val_acc: 0.5674\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9512 - acc: 0.9521 - val_loss: 3.5140 - val_acc: 0.5674\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9311 - acc: 0.9498 - val_loss: 3.5068 - val_acc: 0.5674\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9043 - acc: 0.9433 - val_loss: 3.4821 - val_acc: 0.5674\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8783 - acc: 0.9456 - val_loss: 3.4597 - val_acc: 0.5674\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8516 - acc: 0.9527 - val_loss: 3.4456 - val_acc: 0.5626\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8307 - acc: 0.9527 - val_loss: 3.4239 - val_acc: 0.5745\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8094 - acc: 0.9492 - val_loss: 3.4062 - val_acc: 0.5603\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7767 - acc: 0.9509 - val_loss: 3.3854 - val_acc: 0.5674\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7555 - acc: 0.9580 - val_loss: 3.3762 - val_acc: 0.5745\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7377 - acc: 0.9521 - val_loss: 3.3532 - val_acc: 0.5674\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7164 - acc: 0.9486 - val_loss: 3.3408 - val_acc: 0.5626\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6870 - acc: 0.9569 - val_loss: 3.3287 - val_acc: 0.5650\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6724 - acc: 0.9604 - val_loss: 3.3163 - val_acc: 0.5603\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6572 - acc: 0.9515 - val_loss: 3.2988 - val_acc: 0.5626\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6299 - acc: 0.9492 - val_loss: 3.2832 - val_acc: 0.5626\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5931 - acc: 0.9675 - val_loss: 3.2671 - val_acc: 0.5697\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5865 - acc: 0.9604 - val_loss: 3.2563 - val_acc: 0.5626\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5724 - acc: 0.9610 - val_loss: 3.2385 - val_acc: 0.5721\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5594 - acc: 0.9480 - val_loss: 3.2174 - val_acc: 0.5721\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5364 - acc: 0.9563 - val_loss: 3.2047 - val_acc: 0.5721\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5177 - acc: 0.9557 - val_loss: 3.1948 - val_acc: 0.5721\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4950 - acc: 0.9580 - val_loss: 3.1845 - val_acc: 0.5792\n",
      "443/443 [==============================] - 0s 520us/sample - loss: 3.2370 - acc: 0.5372\n",
      "--- Starting trial: run-5\n",
      "{'TIME_WINDOW': 500, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.2, 'REGULARIZER': 0.005, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 22, 500)]         0         \n",
      "_________________________________________________________________\n",
      "permute_5 (Permute)          (None, 500, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 500, 128)          77312     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 500, 128)          131584    \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               8192128   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,401,540\n",
      "Trainable params: 8,401,540\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 63.7419 - acc: 0.3026 - val_loss: 66.4206 - val_acc: 0.4634\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 60.2629 - acc: 0.4840 - val_loss: 62.7397 - val_acc: 0.4775\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 57.2860 - acc: 0.5786 - val_loss: 59.5163 - val_acc: 0.4988\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 54.7023 - acc: 0.6206 - val_loss: 56.6314 - val_acc: 0.4965\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 52.3552 - acc: 0.6773 - val_loss: 53.9879 - val_acc: 0.5083\n",
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 50.2192 - acc: 0.7004 - val_loss: 51.5627 - val_acc: 0.5035\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 48.2430 - acc: 0.7293 - val_loss: 49.2706 - val_acc: 0.5059\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 46.4179 - acc: 0.7725 - val_loss: 47.1410 - val_acc: 0.5414\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 44.7127 - acc: 0.7642 - val_loss: 45.1254 - val_acc: 0.5366\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 43.1139 - acc: 0.7807 - val_loss: 43.2413 - val_acc: 0.5508\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 41.6067 - acc: 0.8020 - val_loss: 41.4591 - val_acc: 0.5414\n",
      "Epoch 12/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 40.1544 - acc: 0.8150 - val_loss: 39.7893 - val_acc: 0.5556\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 38.8052 - acc: 0.8186 - val_loss: 38.2141 - val_acc: 0.5650\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 37.5241 - acc: 0.8227 - val_loss: 36.7197 - val_acc: 0.5650\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 36.3338 - acc: 0.8404 - val_loss: 35.3296 - val_acc: 0.5674\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 35.1964 - acc: 0.8333 - val_loss: 34.0208 - val_acc: 0.5745\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 34.1035 - acc: 0.8381 - val_loss: 32.7855 - val_acc: 0.5721\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 33.0878 - acc: 0.8369 - val_loss: 31.6302 - val_acc: 0.5697\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 32.1040 - acc: 0.8487 - val_loss: 30.5389 - val_acc: 0.5721\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 31.1715 - acc: 0.8552 - val_loss: 29.5173 - val_acc: 0.5745\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 30.2948 - acc: 0.8605 - val_loss: 28.5552 - val_acc: 0.5863\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 29.4411 - acc: 0.8694 - val_loss: 27.6602 - val_acc: 0.5745\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 28.6657 - acc: 0.8652 - val_loss: 26.8151 - val_acc: 0.5697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 27.8965 - acc: 0.8688 - val_loss: 26.0171 - val_acc: 0.5697\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 27.1697 - acc: 0.8706 - val_loss: 25.2650 - val_acc: 0.5768\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 26.4630 - acc: 0.8788 - val_loss: 24.5522 - val_acc: 0.5768\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 25.7864 - acc: 0.8812 - val_loss: 23.8807 - val_acc: 0.5721\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 25.1715 - acc: 0.8759 - val_loss: 23.2490 - val_acc: 0.5768\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 24.5697 - acc: 0.8729 - val_loss: 22.6463 - val_acc: 0.5745\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 23.9822 - acc: 0.8794 - val_loss: 22.0786 - val_acc: 0.5721\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 23.4111 - acc: 0.8848 - val_loss: 21.5379 - val_acc: 0.5792\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 22.8869 - acc: 0.8830 - val_loss: 21.0260 - val_acc: 0.5721\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 22.3720 - acc: 0.8871 - val_loss: 20.5356 - val_acc: 0.5650\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 21.8756 - acc: 0.8794 - val_loss: 20.0667 - val_acc: 0.5674\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 21.4011 - acc: 0.8788 - val_loss: 19.6230 - val_acc: 0.5745\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.9455 - acc: 0.8777 - val_loss: 19.1993 - val_acc: 0.5816\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.5204 - acc: 0.8853 - val_loss: 18.7926 - val_acc: 0.5839\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.0934 - acc: 0.8818 - val_loss: 18.4027 - val_acc: 0.5839\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.6771 - acc: 0.8877 - val_loss: 18.0311 - val_acc: 0.5768\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.2771 - acc: 0.8918 - val_loss: 17.6711 - val_acc: 0.5721\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.9084 - acc: 0.8883 - val_loss: 17.3223 - val_acc: 0.5839\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.5352 - acc: 0.8924 - val_loss: 16.9914 - val_acc: 0.5721\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.1848 - acc: 0.8972 - val_loss: 16.6745 - val_acc: 0.5626\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.8497 - acc: 0.9019 - val_loss: 16.3627 - val_acc: 0.5768\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.5269 - acc: 0.8930 - val_loss: 16.0675 - val_acc: 0.5816\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.2141 - acc: 0.8924 - val_loss: 15.7792 - val_acc: 0.5863\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.9143 - acc: 0.8877 - val_loss: 15.5028 - val_acc: 0.5839\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.6100 - acc: 0.9013 - val_loss: 15.2343 - val_acc: 0.5910\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.3336 - acc: 0.8918 - val_loss: 14.9763 - val_acc: 0.5887\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.0478 - acc: 0.8972 - val_loss: 14.7271 - val_acc: 0.5887\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.7954 - acc: 0.8895 - val_loss: 14.4867 - val_acc: 0.5863\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.5387 - acc: 0.8972 - val_loss: 14.2545 - val_acc: 0.5697\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.2839 - acc: 0.9019 - val_loss: 14.0284 - val_acc: 0.5768\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.0240 - acc: 0.8995 - val_loss: 13.8091 - val_acc: 0.5768\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.7870 - acc: 0.9054 - val_loss: 13.5948 - val_acc: 0.5816\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.5649 - acc: 0.8930 - val_loss: 13.3894 - val_acc: 0.5863\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.3372 - acc: 0.8948 - val_loss: 13.1908 - val_acc: 0.5887\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.1117 - acc: 0.9001 - val_loss: 12.9975 - val_acc: 0.5863\n",
      "Epoch 59/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.9043 - acc: 0.8960 - val_loss: 12.8068 - val_acc: 0.5887\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.7065 - acc: 0.8901 - val_loss: 12.6246 - val_acc: 0.5887\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.4986 - acc: 0.9043 - val_loss: 12.4495 - val_acc: 0.5957\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.2982 - acc: 0.8930 - val_loss: 12.2749 - val_acc: 0.5887\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.1144 - acc: 0.8983 - val_loss: 12.1080 - val_acc: 0.5887\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.9252 - acc: 0.9025 - val_loss: 11.9449 - val_acc: 0.5957\n",
      "Epoch 65/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.7403 - acc: 0.8978 - val_loss: 11.7855 - val_acc: 0.5934\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.5660 - acc: 0.9007 - val_loss: 11.6271 - val_acc: 0.5863\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.3853 - acc: 0.9043 - val_loss: 11.4813 - val_acc: 0.5816\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.2323 - acc: 0.9001 - val_loss: 11.3303 - val_acc: 0.5863\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.0592 - acc: 0.9001 - val_loss: 11.1878 - val_acc: 0.5768\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.9113 - acc: 0.8859 - val_loss: 11.0394 - val_acc: 0.5887\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.7432 - acc: 0.8948 - val_loss: 10.9042 - val_acc: 0.5816\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.5855 - acc: 0.8924 - val_loss: 10.7701 - val_acc: 0.5792\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.4386 - acc: 0.9048 - val_loss: 10.6397 - val_acc: 0.5768\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.3046 - acc: 0.8889 - val_loss: 10.5120 - val_acc: 0.5745\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.1412 - acc: 0.9096 - val_loss: 10.3826 - val_acc: 0.5697\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.0016 - acc: 0.9001 - val_loss: 10.2631 - val_acc: 0.5721\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.8696 - acc: 0.9037 - val_loss: 10.1374 - val_acc: 0.5887\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.7377 - acc: 0.9019 - val_loss: 10.0170 - val_acc: 0.5839\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.6004 - acc: 0.9037 - val_loss: 9.8998 - val_acc: 0.5839\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.4889 - acc: 0.8983 - val_loss: 9.7906 - val_acc: 0.5863\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.3604 - acc: 0.8995 - val_loss: 9.6837 - val_acc: 0.5839\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.2238 - acc: 0.9048 - val_loss: 9.5783 - val_acc: 0.5792\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.1085 - acc: 0.9096 - val_loss: 9.4658 - val_acc: 0.5816\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.0004 - acc: 0.9001 - val_loss: 9.3653 - val_acc: 0.5863\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.8785 - acc: 0.9060 - val_loss: 9.2657 - val_acc: 0.5863\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.7614 - acc: 0.9054 - val_loss: 9.1642 - val_acc: 0.5816\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.6559 - acc: 0.8983 - val_loss: 9.0636 - val_acc: 0.5792\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.5407 - acc: 0.8960 - val_loss: 8.9641 - val_acc: 0.5792\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.4376 - acc: 0.9043 - val_loss: 8.8756 - val_acc: 0.5792\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.3358 - acc: 0.9025 - val_loss: 8.7829 - val_acc: 0.5863\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.2342 - acc: 0.9037 - val_loss: 8.6936 - val_acc: 0.5863\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.1319 - acc: 0.8983 - val_loss: 8.6022 - val_acc: 0.5839\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.0285 - acc: 0.9048 - val_loss: 8.5120 - val_acc: 0.5768\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.9435 - acc: 0.8972 - val_loss: 8.4316 - val_acc: 0.5910\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.8479 - acc: 0.9060 - val_loss: 8.3432 - val_acc: 0.5839\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.7570 - acc: 0.9001 - val_loss: 8.2651 - val_acc: 0.5792\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.6644 - acc: 0.8978 - val_loss: 8.1842 - val_acc: 0.5816\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.5708 - acc: 0.9054 - val_loss: 8.1031 - val_acc: 0.5887\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.4838 - acc: 0.9119 - val_loss: 8.0261 - val_acc: 0.5887\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.4112 - acc: 0.8924 - val_loss: 7.9458 - val_acc: 0.5934\n",
      "443/443 [==============================] - 0s 491us/sample - loss: 7.9807 - acc: 0.5327\n",
      "--- Starting trial: run-6\n",
      "{'TIME_WINDOW': 500, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 22, 500)]         0         \n",
      "_________________________________________________________________\n",
      "permute_6 (Permute)          (None, 500, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 500, 128)          77312     \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 500, 128)          131584    \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               8192128   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,401,540\n",
      "Trainable params: 8,401,540\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1692/1692 [==============================] - 4s 3ms/sample - loss: 13.7091 - acc: 0.3150 - val_loss: 14.5284 - val_acc: 0.4255\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.0099 - acc: 0.4858 - val_loss: 13.8084 - val_acc: 0.4610\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.4633 - acc: 0.5455 - val_loss: 13.1862 - val_acc: 0.4775\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.9781 - acc: 0.5904 - val_loss: 12.6510 - val_acc: 0.4728\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.5551 - acc: 0.6141 - val_loss: 12.1346 - val_acc: 0.5035\n",
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.1613 - acc: 0.6673 - val_loss: 11.7049 - val_acc: 0.4965\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.7961 - acc: 0.6714 - val_loss: 11.2722 - val_acc: 0.5130\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.4653 - acc: 0.6962 - val_loss: 10.8895 - val_acc: 0.5177\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.1611 - acc: 0.7009 - val_loss: 10.5281 - val_acc: 0.5154\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.8563 - acc: 0.7234 - val_loss: 10.1717 - val_acc: 0.5319\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.5829 - acc: 0.7441 - val_loss: 9.8611 - val_acc: 0.5296\n",
      "Epoch 12/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.3257 - acc: 0.7435 - val_loss: 9.5553 - val_acc: 0.5319\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.0745 - acc: 0.7665 - val_loss: 9.2798 - val_acc: 0.5225\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.8301 - acc: 0.7660 - val_loss: 8.9983 - val_acc: 0.5414\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.6201 - acc: 0.7748 - val_loss: 8.7484 - val_acc: 0.5390\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.4073 - acc: 0.7784 - val_loss: 8.5076 - val_acc: 0.5414\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.1870 - acc: 0.7908 - val_loss: 8.2652 - val_acc: 0.5461\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.9968 - acc: 0.7778 - val_loss: 8.0555 - val_acc: 0.5366\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.8231 - acc: 0.7766 - val_loss: 7.8436 - val_acc: 0.5579\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.6318 - acc: 0.7943 - val_loss: 7.6462 - val_acc: 0.5579\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.4622 - acc: 0.8032 - val_loss: 7.4659 - val_acc: 0.5626\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.2922 - acc: 0.8197 - val_loss: 7.2684 - val_acc: 0.5674\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.1278 - acc: 0.8162 - val_loss: 7.1092 - val_acc: 0.5626\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.9918 - acc: 0.8109 - val_loss: 6.9335 - val_acc: 0.5674\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.8352 - acc: 0.8257 - val_loss: 6.7915 - val_acc: 0.5650\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.7033 - acc: 0.8191 - val_loss: 6.6330 - val_acc: 0.5650\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.5745 - acc: 0.8209 - val_loss: 6.4911 - val_acc: 0.5697\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.4436 - acc: 0.8280 - val_loss: 6.3517 - val_acc: 0.5721\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.3130 - acc: 0.8333 - val_loss: 6.2257 - val_acc: 0.5745\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.1954 - acc: 0.8351 - val_loss: 6.1167 - val_acc: 0.5816\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.0809 - acc: 0.8410 - val_loss: 5.9910 - val_acc: 0.5768\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.9738 - acc: 0.8310 - val_loss: 5.8930 - val_acc: 0.5721\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.8755 - acc: 0.8245 - val_loss: 5.7910 - val_acc: 0.5792\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.7649 - acc: 0.8381 - val_loss: 5.6818 - val_acc: 0.5721\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.6748 - acc: 0.8339 - val_loss: 5.5865 - val_acc: 0.5745\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.5588 - acc: 0.8587 - val_loss: 5.5011 - val_acc: 0.5768\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.4830 - acc: 0.8452 - val_loss: 5.4109 - val_acc: 0.5721\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.3930 - acc: 0.8398 - val_loss: 5.3322 - val_acc: 0.5721\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.3067 - acc: 0.8517 - val_loss: 5.2561 - val_acc: 0.5768\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.2216 - acc: 0.8505 - val_loss: 5.1680 - val_acc: 0.5768\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.1414 - acc: 0.8517 - val_loss: 5.1007 - val_acc: 0.5863\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.0760 - acc: 0.8440 - val_loss: 5.0325 - val_acc: 0.5863\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.9923 - acc: 0.8582 - val_loss: 4.9670 - val_acc: 0.5768\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.9199 - acc: 0.8582 - val_loss: 4.9026 - val_acc: 0.5745\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.8469 - acc: 0.8469 - val_loss: 4.8379 - val_acc: 0.5768\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.7725 - acc: 0.8658 - val_loss: 4.7676 - val_acc: 0.5768\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.7115 - acc: 0.8617 - val_loss: 4.7138 - val_acc: 0.5816\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.6579 - acc: 0.8534 - val_loss: 4.6574 - val_acc: 0.5887\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.5906 - acc: 0.8570 - val_loss: 4.6132 - val_acc: 0.6005\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.5221 - acc: 0.8647 - val_loss: 4.5527 - val_acc: 0.6005\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.4640 - acc: 0.8611 - val_loss: 4.5058 - val_acc: 0.5957\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.4000 - acc: 0.8794 - val_loss: 4.4644 - val_acc: 0.5981\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.3626 - acc: 0.8623 - val_loss: 4.4147 - val_acc: 0.5934\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.3093 - acc: 0.8522 - val_loss: 4.3617 - val_acc: 0.5957\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.2363 - acc: 0.8871 - val_loss: 4.3155 - val_acc: 0.5934\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.1874 - acc: 0.8741 - val_loss: 4.2776 - val_acc: 0.6028\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.1486 - acc: 0.8617 - val_loss: 4.2407 - val_acc: 0.6005\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.0958 - acc: 0.8818 - val_loss: 4.1853 - val_acc: 0.6005\n",
      "Epoch 59/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.0562 - acc: 0.8706 - val_loss: 4.1573 - val_acc: 0.6052\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.0049 - acc: 0.8747 - val_loss: 4.1230 - val_acc: 0.5981\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.9605 - acc: 0.8635 - val_loss: 4.0856 - val_acc: 0.6052\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.9160 - acc: 0.8783 - val_loss: 4.0477 - val_acc: 0.6052\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.8673 - acc: 0.8783 - val_loss: 4.0195 - val_acc: 0.6005\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.8418 - acc: 0.8658 - val_loss: 3.9904 - val_acc: 0.5934\n",
      "Epoch 65/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.7860 - acc: 0.8788 - val_loss: 3.9512 - val_acc: 0.6052\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.7453 - acc: 0.8853 - val_loss: 3.9263 - val_acc: 0.5981\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.7098 - acc: 0.8830 - val_loss: 3.8965 - val_acc: 0.6099\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.6755 - acc: 0.8800 - val_loss: 3.8584 - val_acc: 0.6123\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.6289 - acc: 0.8818 - val_loss: 3.8323 - val_acc: 0.6123\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.6125 - acc: 0.8717 - val_loss: 3.8237 - val_acc: 0.5981\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5557 - acc: 0.8948 - val_loss: 3.7831 - val_acc: 0.6052\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5354 - acc: 0.8747 - val_loss: 3.7558 - val_acc: 0.6076\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4913 - acc: 0.8842 - val_loss: 3.7348 - val_acc: 0.6076\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4504 - acc: 0.8865 - val_loss: 3.7032 - val_acc: 0.6099\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4231 - acc: 0.8853 - val_loss: 3.6915 - val_acc: 0.6147\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3755 - acc: 0.8978 - val_loss: 3.6590 - val_acc: 0.6099\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3549 - acc: 0.8883 - val_loss: 3.6374 - val_acc: 0.6076\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3342 - acc: 0.8865 - val_loss: 3.6089 - val_acc: 0.6099\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2941 - acc: 0.9025 - val_loss: 3.5897 - val_acc: 0.6052\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2633 - acc: 0.9078 - val_loss: 3.5742 - val_acc: 0.6028\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2319 - acc: 0.9013 - val_loss: 3.5510 - val_acc: 0.6076\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2074 - acc: 0.8989 - val_loss: 3.5336 - val_acc: 0.6147\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1932 - acc: 0.8942 - val_loss: 3.5157 - val_acc: 0.6099\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1513 - acc: 0.9054 - val_loss: 3.4945 - val_acc: 0.6076\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1339 - acc: 0.8995 - val_loss: 3.4808 - val_acc: 0.6076\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1022 - acc: 0.9037 - val_loss: 3.4574 - val_acc: 0.6052\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0804 - acc: 0.8966 - val_loss: 3.4465 - val_acc: 0.6052\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0553 - acc: 0.8978 - val_loss: 3.4238 - val_acc: 0.6052\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0257 - acc: 0.9048 - val_loss: 3.4010 - val_acc: 0.6028\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0024 - acc: 0.9031 - val_loss: 3.3883 - val_acc: 0.6028\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9835 - acc: 0.8978 - val_loss: 3.3701 - val_acc: 0.5981\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9752 - acc: 0.8907 - val_loss: 3.3640 - val_acc: 0.5934\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9501 - acc: 0.8918 - val_loss: 3.3448 - val_acc: 0.5957\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9095 - acc: 0.9131 - val_loss: 3.3241 - val_acc: 0.5934\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8930 - acc: 0.9054 - val_loss: 3.3153 - val_acc: 0.5887\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8832 - acc: 0.8983 - val_loss: 3.2955 - val_acc: 0.5934\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8376 - acc: 0.9048 - val_loss: 3.2835 - val_acc: 0.5934\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8235 - acc: 0.9131 - val_loss: 3.2714 - val_acc: 0.5863\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8119 - acc: 0.9108 - val_loss: 3.2493 - val_acc: 0.5934\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7884 - acc: 0.9131 - val_loss: 3.2294 - val_acc: 0.5957\n",
      "443/443 [==============================] - 0s 491us/sample - loss: 3.2823 - acc: 0.5576\n",
      "--- Starting trial: run-7\n",
      "{'TIME_WINDOW': 500, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.005, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 22, 500)]         0         \n",
      "_________________________________________________________________\n",
      "permute_7 (Permute)          (None, 500, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 500, 128)          77312     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 500, 128)          131584    \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               8192128   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,401,540\n",
      "Trainable params: 8,401,540\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 59.7072 - acc: 0.3097 - val_loss: 63.8543 - val_acc: 0.4326\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 56.8963 - acc: 0.4486 - val_loss: 60.5388 - val_acc: 0.4468\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 54.4542 - acc: 0.5662 - val_loss: 57.5692 - val_acc: 0.5390\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 52.3106 - acc: 0.6070 - val_loss: 54.9199 - val_acc: 0.5579\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 50.3550 - acc: 0.6318 - val_loss: 52.5091 - val_acc: 0.5721\n",
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 48.5614 - acc: 0.6761 - val_loss: 50.2578 - val_acc: 0.5745\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 46.8771 - acc: 0.6814 - val_loss: 48.1737 - val_acc: 0.5768\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 45.3178 - acc: 0.6944 - val_loss: 46.1924 - val_acc: 0.5957\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 43.8285 - acc: 0.7287 - val_loss: 44.3337 - val_acc: 0.5910\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 42.4271 - acc: 0.7305 - val_loss: 42.5791 - val_acc: 0.5981\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 41.1191 - acc: 0.7435 - val_loss: 40.9189 - val_acc: 0.6123\n",
      "Epoch 12/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 39.8488 - acc: 0.7606 - val_loss: 39.3478 - val_acc: 0.6076\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 38.6315 - acc: 0.7624 - val_loss: 37.8617 - val_acc: 0.6123\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 37.4939 - acc: 0.7689 - val_loss: 36.4587 - val_acc: 0.5934\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 36.4070 - acc: 0.7725 - val_loss: 35.1290 - val_acc: 0.6005\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 35.3688 - acc: 0.7861 - val_loss: 33.8620 - val_acc: 0.5981\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 34.3591 - acc: 0.7825 - val_loss: 32.6688 - val_acc: 0.6076\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 33.4175 - acc: 0.7914 - val_loss: 31.5250 - val_acc: 0.6147\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 32.5064 - acc: 0.7825 - val_loss: 30.4499 - val_acc: 0.6147\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 31.6253 - acc: 0.7991 - val_loss: 29.4289 - val_acc: 0.6052\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 30.7832 - acc: 0.8002 - val_loss: 28.4684 - val_acc: 0.6052\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 29.9852 - acc: 0.7996 - val_loss: 27.5493 - val_acc: 0.6147\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 29.2479 - acc: 0.7831 - val_loss: 26.6905 - val_acc: 0.6005\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 28.4904 - acc: 0.8085 - val_loss: 25.8734 - val_acc: 0.6005\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 27.7709 - acc: 0.7967 - val_loss: 25.1018 - val_acc: 0.5934\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 27.1064 - acc: 0.8044 - val_loss: 24.3731 - val_acc: 0.5957\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 26.4508 - acc: 0.7943 - val_loss: 23.6810 - val_acc: 0.6005\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 25.8376 - acc: 0.8126 - val_loss: 23.0305 - val_acc: 0.6052\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 25.2251 - acc: 0.8097 - val_loss: 22.4140 - val_acc: 0.6099\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 24.6560 - acc: 0.8067 - val_loss: 21.8335 - val_acc: 0.6123\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 24.1095 - acc: 0.8115 - val_loss: 21.2873 - val_acc: 0.6028\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 23.5672 - acc: 0.8174 - val_loss: 20.7613 - val_acc: 0.6028\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 23.0585 - acc: 0.8215 - val_loss: 20.2641 - val_acc: 0.6005\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 22.5676 - acc: 0.8121 - val_loss: 19.7971 - val_acc: 0.6052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 22.1031 - acc: 0.8227 - val_loss: 19.3468 - val_acc: 0.6052\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 21.6299 - acc: 0.8156 - val_loss: 18.9180 - val_acc: 0.6052\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 21.1823 - acc: 0.8292 - val_loss: 18.5102 - val_acc: 0.6099\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.7687 - acc: 0.8191 - val_loss: 18.1220 - val_acc: 0.5934\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.3596 - acc: 0.8322 - val_loss: 17.7432 - val_acc: 0.6099\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.9670 - acc: 0.8109 - val_loss: 17.3892 - val_acc: 0.5957\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.5886 - acc: 0.8126 - val_loss: 17.0451 - val_acc: 0.6052\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.2110 - acc: 0.8268 - val_loss: 16.7187 - val_acc: 0.5957\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.8583 - acc: 0.8239 - val_loss: 16.4062 - val_acc: 0.5957\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.5073 - acc: 0.8227 - val_loss: 16.1023 - val_acc: 0.6005\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.1898 - acc: 0.8144 - val_loss: 15.8138 - val_acc: 0.5957\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.8556 - acc: 0.8357 - val_loss: 15.5314 - val_acc: 0.6028\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.5447 - acc: 0.8156 - val_loss: 15.2616 - val_acc: 0.5981\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.2166 - acc: 0.8174 - val_loss: 14.9974 - val_acc: 0.6005\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.9459 - acc: 0.8168 - val_loss: 14.7421 - val_acc: 0.6005\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.6341 - acc: 0.8150 - val_loss: 14.4994 - val_acc: 0.5957\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.3733 - acc: 0.8268 - val_loss: 14.2592 - val_acc: 0.6028\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.1149 - acc: 0.8286 - val_loss: 14.0387 - val_acc: 0.5957\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.8542 - acc: 0.8280 - val_loss: 13.8195 - val_acc: 0.6028\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.6166 - acc: 0.8056 - val_loss: 13.6032 - val_acc: 0.6076\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.3436 - acc: 0.8298 - val_loss: 13.3986 - val_acc: 0.6005\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.1192 - acc: 0.8262 - val_loss: 13.1971 - val_acc: 0.5863\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.8705 - acc: 0.8345 - val_loss: 13.0007 - val_acc: 0.5981\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.6615 - acc: 0.8251 - val_loss: 12.8153 - val_acc: 0.5910\n",
      "Epoch 59/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.4554 - acc: 0.8209 - val_loss: 12.6292 - val_acc: 0.5910\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.2344 - acc: 0.8215 - val_loss: 12.4465 - val_acc: 0.6076\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.0286 - acc: 0.8381 - val_loss: 12.2741 - val_acc: 0.6099\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.8280 - acc: 0.8357 - val_loss: 12.1057 - val_acc: 0.6099\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.6266 - acc: 0.8339 - val_loss: 11.9362 - val_acc: 0.6123\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.4390 - acc: 0.8251 - val_loss: 11.7799 - val_acc: 0.6147\n",
      "Epoch 65/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.2535 - acc: 0.8345 - val_loss: 11.6198 - val_acc: 0.6170\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.0759 - acc: 0.8262 - val_loss: 11.4668 - val_acc: 0.6099\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.9021 - acc: 0.8239 - val_loss: 11.3192 - val_acc: 0.6099\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.7153 - acc: 0.8416 - val_loss: 11.1767 - val_acc: 0.6099\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.5642 - acc: 0.8286 - val_loss: 11.0354 - val_acc: 0.6076\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.4028 - acc: 0.8215 - val_loss: 10.8943 - val_acc: 0.6147\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.2303 - acc: 0.8381 - val_loss: 10.7624 - val_acc: 0.6099\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.0672 - acc: 0.8416 - val_loss: 10.6257 - val_acc: 0.6099\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.9194 - acc: 0.8292 - val_loss: 10.4970 - val_acc: 0.6099\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.7900 - acc: 0.8109 - val_loss: 10.3742 - val_acc: 0.6076\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.6292 - acc: 0.8245 - val_loss: 10.2457 - val_acc: 0.6099\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.4920 - acc: 0.8274 - val_loss: 10.1248 - val_acc: 0.6170\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.3466 - acc: 0.8310 - val_loss: 10.0041 - val_acc: 0.6099\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.2186 - acc: 0.8310 - val_loss: 9.8892 - val_acc: 0.6217\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.0564 - acc: 0.8446 - val_loss: 9.7734 - val_acc: 0.6147\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.9493 - acc: 0.8339 - val_loss: 9.6666 - val_acc: 0.6099\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.8173 - acc: 0.8357 - val_loss: 9.5590 - val_acc: 0.6147\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.6918 - acc: 0.8316 - val_loss: 9.4539 - val_acc: 0.6076\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.5691 - acc: 0.8233 - val_loss: 9.3440 - val_acc: 0.6099\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.4418 - acc: 0.8440 - val_loss: 9.2449 - val_acc: 0.6099\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.3284 - acc: 0.8274 - val_loss: 9.1452 - val_acc: 0.6147\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.2173 - acc: 0.8280 - val_loss: 9.0458 - val_acc: 0.6123\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.1032 - acc: 0.8316 - val_loss: 8.9455 - val_acc: 0.6194\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.0045 - acc: 0.8262 - val_loss: 8.8555 - val_acc: 0.6147\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.8810 - acc: 0.8404 - val_loss: 8.7668 - val_acc: 0.6147\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.7764 - acc: 0.8280 - val_loss: 8.6721 - val_acc: 0.6123\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.6601 - acc: 0.8298 - val_loss: 8.5840 - val_acc: 0.6028\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.5511 - acc: 0.8351 - val_loss: 8.4988 - val_acc: 0.6028\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.4541 - acc: 0.8410 - val_loss: 8.4065 - val_acc: 0.6099\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.3576 - acc: 0.8357 - val_loss: 8.3198 - val_acc: 0.6170\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.2610 - acc: 0.8345 - val_loss: 8.2417 - val_acc: 0.6052\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.1808 - acc: 0.8191 - val_loss: 8.1609 - val_acc: 0.6099\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.0672 - acc: 0.8381 - val_loss: 8.0777 - val_acc: 0.6052\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.9660 - acc: 0.8375 - val_loss: 7.9956 - val_acc: 0.6099\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.8806 - acc: 0.8398 - val_loss: 7.9195 - val_acc: 0.6123\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.7833 - acc: 0.8434 - val_loss: 7.8491 - val_acc: 0.6099\n",
      "443/443 [==============================] - 0s 503us/sample - loss: 7.8737 - acc: 0.5621\n",
      "Training data shape with slices: (1692, 22, 600)\n",
      "Training label shape with slice: (1692,)\n",
      "Validation data shape with slices: (423, 22, 600)\n",
      "Validation label shape with slice: (423,)\n",
      "Testing data shape with slices: (443, 22, 600)\n",
      "Testing label shape with slice: (443,)\n",
      "--- Starting trial: run-8\n",
      "{'TIME_WINDOW': 600, 'BATCH_SIZE': 32, 'HIDDEN': 64, 'DROPOUT': 0.2, 'REGULARIZER': 0.001, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 22, 600)]         0         \n",
      "_________________________________________________________________\n",
      "permute_8 (Permute)          (None, 600, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 600, 64)           22272     \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 600, 64)           33024     \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                2457664   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 2,513,220\n",
      "Trainable params: 2,513,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 9.5406 - acc: 0.3056 - val_loss: 10.1003 - val_acc: 0.3050\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.1843 - acc: 0.4226 - val_loss: 9.7669 - val_acc: 0.3853\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.9001 - acc: 0.4681 - val_loss: 9.4503 - val_acc: 0.4232\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.6307 - acc: 0.5248 - val_loss: 9.1816 - val_acc: 0.4303\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.3837 - acc: 0.5626 - val_loss: 8.9247 - val_acc: 0.4208\n",
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.1548 - acc: 0.5827 - val_loss: 8.6890 - val_acc: 0.4421\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.9460 - acc: 0.6087 - val_loss: 8.4646 - val_acc: 0.4586\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.7422 - acc: 0.6389 - val_loss: 8.2622 - val_acc: 0.4586\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.5607 - acc: 0.6460 - val_loss: 8.0520 - val_acc: 0.4775\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.3768 - acc: 0.6702 - val_loss: 7.8610 - val_acc: 0.4586\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.2054 - acc: 0.6868 - val_loss: 7.6798 - val_acc: 0.4728\n",
      "Epoch 12/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.0478 - acc: 0.6915 - val_loss: 7.5056 - val_acc: 0.4657\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.8885 - acc: 0.7169 - val_loss: 7.3379 - val_acc: 0.4657\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.7396 - acc: 0.7163 - val_loss: 7.1722 - val_acc: 0.4704\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.6057 - acc: 0.7128 - val_loss: 7.0161 - val_acc: 0.4704\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.4693 - acc: 0.7246 - val_loss: 6.8631 - val_acc: 0.4775\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.3334 - acc: 0.7216 - val_loss: 6.7151 - val_acc: 0.4775\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.2075 - acc: 0.7323 - val_loss: 6.5760 - val_acc: 0.4894\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.0787 - acc: 0.7541 - val_loss: 6.4389 - val_acc: 0.4941\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.9713 - acc: 0.7571 - val_loss: 6.3100 - val_acc: 0.4965\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.8437 - acc: 0.7612 - val_loss: 6.1840 - val_acc: 0.5035\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.7502 - acc: 0.7559 - val_loss: 6.0600 - val_acc: 0.5059\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.6373 - acc: 0.7671 - val_loss: 5.9453 - val_acc: 0.4894\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.5372 - acc: 0.7695 - val_loss: 5.8326 - val_acc: 0.4965\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.4393 - acc: 0.7748 - val_loss: 5.7208 - val_acc: 0.4988\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.3454 - acc: 0.7778 - val_loss: 5.6189 - val_acc: 0.5012\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.2551 - acc: 0.7748 - val_loss: 5.5111 - val_acc: 0.5035\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.1761 - acc: 0.7831 - val_loss: 5.4155 - val_acc: 0.5154\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.0852 - acc: 0.7931 - val_loss: 5.3155 - val_acc: 0.5059\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.0009 - acc: 0.7872 - val_loss: 5.2245 - val_acc: 0.5154\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.9149 - acc: 0.7943 - val_loss: 5.1403 - val_acc: 0.5130\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.8373 - acc: 0.8085 - val_loss: 5.0545 - val_acc: 0.5201\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.7708 - acc: 0.7973 - val_loss: 4.9742 - val_acc: 0.5177\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.6989 - acc: 0.8067 - val_loss: 4.8978 - val_acc: 0.5201\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.6282 - acc: 0.8085 - val_loss: 4.8202 - val_acc: 0.5272\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.5589 - acc: 0.8085 - val_loss: 4.7484 - val_acc: 0.5296\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.4950 - acc: 0.8026 - val_loss: 4.6784 - val_acc: 0.5296\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.4274 - acc: 0.8067 - val_loss: 4.6134 - val_acc: 0.5248\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.3624 - acc: 0.8239 - val_loss: 4.5499 - val_acc: 0.5225\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.3063 - acc: 0.8197 - val_loss: 4.4894 - val_acc: 0.5272\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.2629 - acc: 0.7967 - val_loss: 4.4308 - val_acc: 0.5296\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.1929 - acc: 0.8251 - val_loss: 4.3732 - val_acc: 0.5343\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.1380 - acc: 0.8191 - val_loss: 4.3195 - val_acc: 0.5343\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.0754 - acc: 0.8333 - val_loss: 4.2689 - val_acc: 0.5366\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.0368 - acc: 0.8156 - val_loss: 4.2159 - val_acc: 0.5296\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.9843 - acc: 0.8322 - val_loss: 4.1672 - val_acc: 0.5319\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.9267 - acc: 0.8233 - val_loss: 4.1185 - val_acc: 0.5390\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.8843 - acc: 0.8197 - val_loss: 4.0707 - val_acc: 0.5414\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.8392 - acc: 0.8333 - val_loss: 4.0277 - val_acc: 0.5437\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.7839 - acc: 0.8363 - val_loss: 3.9848 - val_acc: 0.5414\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.7381 - acc: 0.8245 - val_loss: 3.9432 - val_acc: 0.5461\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.6924 - acc: 0.8310 - val_loss: 3.9031 - val_acc: 0.5508\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.6559 - acc: 0.8310 - val_loss: 3.8638 - val_acc: 0.5485\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.6149 - acc: 0.8357 - val_loss: 3.8251 - val_acc: 0.5437\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5731 - acc: 0.8392 - val_loss: 3.7874 - val_acc: 0.5556\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5274 - acc: 0.8375 - val_loss: 3.7503 - val_acc: 0.5579\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4909 - acc: 0.8487 - val_loss: 3.7165 - val_acc: 0.5461\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4629 - acc: 0.8298 - val_loss: 3.6821 - val_acc: 0.5579\n",
      "Epoch 59/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4042 - acc: 0.8599 - val_loss: 3.6480 - val_acc: 0.5579\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3896 - acc: 0.8381 - val_loss: 3.6162 - val_acc: 0.5579\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3354 - acc: 0.8534 - val_loss: 3.5874 - val_acc: 0.5508\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2913 - acc: 0.8564 - val_loss: 3.5555 - val_acc: 0.5461\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2695 - acc: 0.8481 - val_loss: 3.5255 - val_acc: 0.5532\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2445 - acc: 0.8434 - val_loss: 3.4996 - val_acc: 0.5532\n",
      "Epoch 65/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2067 - acc: 0.8446 - val_loss: 3.4729 - val_acc: 0.5461\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1630 - acc: 0.8528 - val_loss: 3.4458 - val_acc: 0.5556\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1326 - acc: 0.8564 - val_loss: 3.4205 - val_acc: 0.5556\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1200 - acc: 0.8440 - val_loss: 3.3948 - val_acc: 0.5603\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0768 - acc: 0.8617 - val_loss: 3.3701 - val_acc: 0.5579\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0485 - acc: 0.8617 - val_loss: 3.3450 - val_acc: 0.5556\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0199 - acc: 0.8629 - val_loss: 3.3211 - val_acc: 0.5579\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9931 - acc: 0.8635 - val_loss: 3.3012 - val_acc: 0.5579\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9736 - acc: 0.8564 - val_loss: 3.2794 - val_acc: 0.5603\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9372 - acc: 0.8676 - val_loss: 3.2594 - val_acc: 0.5579\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9155 - acc: 0.8664 - val_loss: 3.2377 - val_acc: 0.5603\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8952 - acc: 0.8582 - val_loss: 3.2183 - val_acc: 0.5603\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8669 - acc: 0.8629 - val_loss: 3.1979 - val_acc: 0.5603\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8453 - acc: 0.8587 - val_loss: 3.1792 - val_acc: 0.5603\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8040 - acc: 0.8765 - val_loss: 3.1637 - val_acc: 0.5556\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7966 - acc: 0.8617 - val_loss: 3.1455 - val_acc: 0.5603\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7717 - acc: 0.8694 - val_loss: 3.1266 - val_acc: 0.5626\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7522 - acc: 0.8652 - val_loss: 3.1103 - val_acc: 0.5603\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7237 - acc: 0.8688 - val_loss: 3.0918 - val_acc: 0.5674\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7044 - acc: 0.8723 - val_loss: 3.0768 - val_acc: 0.5626\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6875 - acc: 0.8682 - val_loss: 3.0622 - val_acc: 0.5603\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6728 - acc: 0.8599 - val_loss: 3.0491 - val_acc: 0.5650\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6360 - acc: 0.8765 - val_loss: 3.0309 - val_acc: 0.5603\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6155 - acc: 0.8641 - val_loss: 3.0161 - val_acc: 0.5556\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6025 - acc: 0.8717 - val_loss: 3.0009 - val_acc: 0.5603\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5883 - acc: 0.8599 - val_loss: 2.9843 - val_acc: 0.5603\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5637 - acc: 0.8741 - val_loss: 2.9696 - val_acc: 0.5626\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5463 - acc: 0.8842 - val_loss: 2.9561 - val_acc: 0.5626\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5334 - acc: 0.8700 - val_loss: 2.9450 - val_acc: 0.5626\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5141 - acc: 0.8735 - val_loss: 2.9326 - val_acc: 0.5603\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5068 - acc: 0.8658 - val_loss: 2.9203 - val_acc: 0.5674\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4790 - acc: 0.8765 - val_loss: 2.9110 - val_acc: 0.5603\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4620 - acc: 0.8735 - val_loss: 2.8971 - val_acc: 0.5650\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4467 - acc: 0.8753 - val_loss: 2.8871 - val_acc: 0.5674\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4337 - acc: 0.8747 - val_loss: 2.8768 - val_acc: 0.5697\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.4172 - acc: 0.8924 - val_loss: 2.8645 - val_acc: 0.5603\n",
      "443/443 [==============================] - 0s 476us/sample - loss: 2.8895 - acc: 0.5214\n",
      "--- Starting trial: run-9\n",
      "{'TIME_WINDOW': 600, 'BATCH_SIZE': 32, 'HIDDEN': 64, 'DROPOUT': 0.2, 'REGULARIZER': 0.005, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 22, 600)]         0         \n",
      "_________________________________________________________________\n",
      "permute_9 (Permute)          (None, 600, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 600, 64)           22272     \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 600, 64)           33024     \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                2457664   \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 2,513,220\n",
      "Trainable params: 2,513,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 44.4268 - acc: 0.3050 - val_loss: 47.9083 - val_acc: 0.3310\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 42.9267 - acc: 0.3794 - val_loss: 46.2287 - val_acc: 0.4066\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 41.5647 - acc: 0.4781 - val_loss: 44.6955 - val_acc: 0.4374\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 40.3196 - acc: 0.4929 - val_loss: 43.2738 - val_acc: 0.4539\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 39.1439 - acc: 0.5414 - val_loss: 41.9228 - val_acc: 0.4563\n",
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 38.0348 - acc: 0.5691 - val_loss: 40.6672 - val_acc: 0.4563\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 36.9933 - acc: 0.5786 - val_loss: 39.4522 - val_acc: 0.4728\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 35.9919 - acc: 0.6200 - val_loss: 38.3020 - val_acc: 0.4657\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 35.0428 - acc: 0.6229 - val_loss: 37.2230 - val_acc: 0.4681\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 34.1159 - acc: 0.6531 - val_loss: 36.1571 - val_acc: 0.4870\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 33.2433 - acc: 0.6684 - val_loss: 35.1371 - val_acc: 0.4917\n",
      "Epoch 12/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 32.3978 - acc: 0.6684 - val_loss: 34.1609 - val_acc: 0.4870\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 31.5759 - acc: 0.6879 - val_loss: 33.2231 - val_acc: 0.4894\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 30.7894 - acc: 0.6944 - val_loss: 32.3008 - val_acc: 0.5083\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 30.0605 - acc: 0.6974 - val_loss: 31.4159 - val_acc: 0.4965\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 29.3226 - acc: 0.7163 - val_loss: 30.5527 - val_acc: 0.5012\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 28.6228 - acc: 0.7281 - val_loss: 29.7196 - val_acc: 0.5130\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 27.9202 - acc: 0.7234 - val_loss: 28.9111 - val_acc: 0.5106\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 27.2422 - acc: 0.7411 - val_loss: 28.1233 - val_acc: 0.5248\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 26.5850 - acc: 0.7547 - val_loss: 27.3581 - val_acc: 0.5225\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 25.9619 - acc: 0.7665 - val_loss: 26.6117 - val_acc: 0.5177\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 25.3557 - acc: 0.7411 - val_loss: 25.8828 - val_acc: 0.5319\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 24.7447 - acc: 0.7772 - val_loss: 25.1919 - val_acc: 0.5154\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 24.1699 - acc: 0.7719 - val_loss: 24.5173 - val_acc: 0.5319\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 23.6026 - acc: 0.7600 - val_loss: 23.8645 - val_acc: 0.5272\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 23.0568 - acc: 0.7872 - val_loss: 23.2378 - val_acc: 0.5272\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 22.5358 - acc: 0.7796 - val_loss: 22.6346 - val_acc: 0.5437\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 22.0349 - acc: 0.7713 - val_loss: 22.0555 - val_acc: 0.5437\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 21.5521 - acc: 0.7772 - val_loss: 21.4988 - val_acc: 0.5390\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 21.0865 - acc: 0.7772 - val_loss: 20.9648 - val_acc: 0.5343\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.6406 - acc: 0.7979 - val_loss: 20.4480 - val_acc: 0.5366\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.2050 - acc: 0.7807 - val_loss: 19.9520 - val_acc: 0.5343\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.7848 - acc: 0.7748 - val_loss: 19.4848 - val_acc: 0.5177\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.3873 - acc: 0.7896 - val_loss: 19.0342 - val_acc: 0.5177\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.0036 - acc: 0.7890 - val_loss: 18.6035 - val_acc: 0.5272\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.6434 - acc: 0.7790 - val_loss: 18.1912 - val_acc: 0.5225\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.2661 - acc: 0.7967 - val_loss: 17.7934 - val_acc: 0.5225\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.9042 - acc: 0.7943 - val_loss: 17.4143 - val_acc: 0.5201\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.5656 - acc: 0.7973 - val_loss: 17.0514 - val_acc: 0.5225\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.2361 - acc: 0.7979 - val_loss: 16.7010 - val_acc: 0.5248\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.9464 - acc: 0.7884 - val_loss: 16.3625 - val_acc: 0.5201\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.6286 - acc: 0.8056 - val_loss: 16.0371 - val_acc: 0.5248\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.3197 - acc: 0.8061 - val_loss: 15.7292 - val_acc: 0.5225\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.0379 - acc: 0.8079 - val_loss: 15.4297 - val_acc: 0.5201\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.7540 - acc: 0.7890 - val_loss: 15.1372 - val_acc: 0.5177\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.4876 - acc: 0.7931 - val_loss: 14.8599 - val_acc: 0.5201\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.2146 - acc: 0.8209 - val_loss: 14.5871 - val_acc: 0.5272\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.9503 - acc: 0.8085 - val_loss: 14.3302 - val_acc: 0.5390\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.7004 - acc: 0.8174 - val_loss: 14.0810 - val_acc: 0.5390\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.4643 - acc: 0.8262 - val_loss: 13.8378 - val_acc: 0.5414\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.2276 - acc: 0.7979 - val_loss: 13.6038 - val_acc: 0.5390\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.9988 - acc: 0.7979 - val_loss: 13.3774 - val_acc: 0.5485\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.7623 - acc: 0.8132 - val_loss: 13.1593 - val_acc: 0.5461\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.5424 - acc: 0.8044 - val_loss: 12.9460 - val_acc: 0.5485\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.3332 - acc: 0.8026 - val_loss: 12.7440 - val_acc: 0.5532\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.1308 - acc: 0.8091 - val_loss: 12.5402 - val_acc: 0.5579\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.9221 - acc: 0.8056 - val_loss: 12.3499 - val_acc: 0.5579\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.7341 - acc: 0.8067 - val_loss: 12.1618 - val_acc: 0.5556\n",
      "Epoch 59/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.5208 - acc: 0.8191 - val_loss: 11.9799 - val_acc: 0.5485\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.3485 - acc: 0.8197 - val_loss: 11.8005 - val_acc: 0.5556\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.1622 - acc: 0.8103 - val_loss: 11.6286 - val_acc: 0.5532\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.9900 - acc: 0.8203 - val_loss: 11.4643 - val_acc: 0.5532\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.8159 - acc: 0.8109 - val_loss: 11.3011 - val_acc: 0.5461\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.6490 - acc: 0.8138 - val_loss: 11.1407 - val_acc: 0.5461\n",
      "Epoch 65/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.4867 - acc: 0.8073 - val_loss: 10.9876 - val_acc: 0.5414\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.3312 - acc: 0.8097 - val_loss: 10.8365 - val_acc: 0.5366\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.1690 - acc: 0.8186 - val_loss: 10.6922 - val_acc: 0.5319\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.0131 - acc: 0.8221 - val_loss: 10.5460 - val_acc: 0.5366\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.8835 - acc: 0.8138 - val_loss: 10.4057 - val_acc: 0.5319\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.7241 - acc: 0.8150 - val_loss: 10.2719 - val_acc: 0.5414\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.5802 - acc: 0.8174 - val_loss: 10.1391 - val_acc: 0.5414\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.4509 - acc: 0.8162 - val_loss: 10.0068 - val_acc: 0.5485\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.3205 - acc: 0.8144 - val_loss: 9.8786 - val_acc: 0.5461\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.1719 - acc: 0.8138 - val_loss: 9.7563 - val_acc: 0.5532\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.0475 - acc: 0.8138 - val_loss: 9.6375 - val_acc: 0.5461\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.9292 - acc: 0.8162 - val_loss: 9.5187 - val_acc: 0.5508\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.8001 - acc: 0.8292 - val_loss: 9.4021 - val_acc: 0.5603\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.6895 - acc: 0.8245 - val_loss: 9.2903 - val_acc: 0.5532\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.5702 - acc: 0.8138 - val_loss: 9.1805 - val_acc: 0.5485\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.4575 - acc: 0.8221 - val_loss: 9.0716 - val_acc: 0.5556\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.3485 - acc: 0.8280 - val_loss: 8.9677 - val_acc: 0.5532\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.2405 - acc: 0.8203 - val_loss: 8.8648 - val_acc: 0.5579\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.1270 - acc: 0.8138 - val_loss: 8.7635 - val_acc: 0.5603\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.0307 - acc: 0.8245 - val_loss: 8.6690 - val_acc: 0.5579\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.9224 - acc: 0.8156 - val_loss: 8.5727 - val_acc: 0.5579\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.8233 - acc: 0.8186 - val_loss: 8.4766 - val_acc: 0.5508\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.7288 - acc: 0.8292 - val_loss: 8.3837 - val_acc: 0.5532\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.6451 - acc: 0.8138 - val_loss: 8.2945 - val_acc: 0.5532\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.5379 - acc: 0.8274 - val_loss: 8.2037 - val_acc: 0.5556\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.4613 - acc: 0.8097 - val_loss: 8.1171 - val_acc: 0.5508\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.3623 - acc: 0.8162 - val_loss: 8.0324 - val_acc: 0.5532\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.2703 - acc: 0.8061 - val_loss: 7.9492 - val_acc: 0.5532\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.1834 - acc: 0.8257 - val_loss: 7.8686 - val_acc: 0.5532\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.1004 - acc: 0.8215 - val_loss: 7.7907 - val_acc: 0.5508\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.0092 - acc: 0.8121 - val_loss: 7.7151 - val_acc: 0.5508\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.9286 - acc: 0.8286 - val_loss: 7.6373 - val_acc: 0.5556\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.8450 - acc: 0.8262 - val_loss: 7.5612 - val_acc: 0.5485\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.7742 - acc: 0.8209 - val_loss: 7.4846 - val_acc: 0.5556\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.7025 - acc: 0.8209 - val_loss: 7.4136 - val_acc: 0.5556\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.6158 - acc: 0.8239 - val_loss: 7.3427 - val_acc: 0.5556\n",
      "443/443 [==============================] - 0s 525us/sample - loss: 7.3792 - acc: 0.5372\n",
      "--- Starting trial: run-10\n",
      "{'TIME_WINDOW': 600, 'BATCH_SIZE': 32, 'HIDDEN': 64, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LEARNING': 1e-05, 'BETA': 0.9}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 22, 600)]         0         \n",
      "_________________________________________________________________\n",
      "permute_10 (Permute)         (None, 600, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 600, 64)           22272     \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 600, 64)           33024     \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                2457664   \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 2,513,220\n",
      "Trainable params: 2,513,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 9.1459 - acc: 0.2642 - val_loss: 9.9140 - val_acc: 0.3097\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.8743 - acc: 0.3546 - val_loss: 9.6241 - val_acc: 0.3735\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.6433 - acc: 0.4285 - val_loss: 9.3806 - val_acc: 0.4303\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.4347 - acc: 0.4740 - val_loss: 9.1530 - val_acc: 0.4232\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.2342 - acc: 0.5219 - val_loss: 8.9474 - val_acc: 0.4326\n",
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.0568 - acc: 0.5296 - val_loss: 8.7428 - val_acc: 0.4397\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.8776 - acc: 0.5656 - val_loss: 8.5534 - val_acc: 0.4681\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.7239 - acc: 0.5869 - val_loss: 8.3731 - val_acc: 0.4704\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.5706 - acc: 0.5863 - val_loss: 8.2011 - val_acc: 0.4728\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.4193 - acc: 0.6040 - val_loss: 8.0355 - val_acc: 0.4728\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.2764 - acc: 0.6342 - val_loss: 7.8695 - val_acc: 0.4657\n",
      "Epoch 12/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.1362 - acc: 0.6389 - val_loss: 7.7148 - val_acc: 0.4657\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.0019 - acc: 0.6395 - val_loss: 7.5565 - val_acc: 0.4704\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.8867 - acc: 0.6442 - val_loss: 7.4094 - val_acc: 0.4846\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.7556 - acc: 0.6454 - val_loss: 7.2642 - val_acc: 0.4799\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.6368 - acc: 0.6543 - val_loss: 7.1222 - val_acc: 0.4870\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.5073 - acc: 0.6726 - val_loss: 6.9851 - val_acc: 0.4941\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.4061 - acc: 0.6596 - val_loss: 6.8552 - val_acc: 0.5035\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.2876 - acc: 0.6803 - val_loss: 6.7197 - val_acc: 0.5035\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.1898 - acc: 0.6678 - val_loss: 6.5883 - val_acc: 0.5012\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 6.0899 - acc: 0.6814 - val_loss: 6.4641 - val_acc: 0.5106\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.9828 - acc: 0.6761 - val_loss: 6.3448 - val_acc: 0.5130\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.8873 - acc: 0.6826 - val_loss: 6.2287 - val_acc: 0.5201\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.7692 - acc: 0.7004 - val_loss: 6.1099 - val_acc: 0.5177\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.6792 - acc: 0.7086 - val_loss: 5.9976 - val_acc: 0.5201\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.6004 - acc: 0.7021 - val_loss: 5.8876 - val_acc: 0.5177\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.5101 - acc: 0.7063 - val_loss: 5.7827 - val_acc: 0.5130\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.4165 - acc: 0.7157 - val_loss: 5.6830 - val_acc: 0.5106\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.3250 - acc: 0.7216 - val_loss: 5.5800 - val_acc: 0.5130\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.2479 - acc: 0.7086 - val_loss: 5.4857 - val_acc: 0.5154\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.1763 - acc: 0.7199 - val_loss: 5.3961 - val_acc: 0.5154\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.0965 - acc: 0.7204 - val_loss: 5.3013 - val_acc: 0.5154\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 5.0211 - acc: 0.7234 - val_loss: 5.2153 - val_acc: 0.5177\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.9378 - acc: 0.7305 - val_loss: 5.1334 - val_acc: 0.5154\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.8656 - acc: 0.7453 - val_loss: 5.0512 - val_acc: 0.5177\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.7937 - acc: 0.7447 - val_loss: 4.9753 - val_acc: 0.5154\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.7327 - acc: 0.7423 - val_loss: 4.9010 - val_acc: 0.5106\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.6652 - acc: 0.7400 - val_loss: 4.8268 - val_acc: 0.5130\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.6016 - acc: 0.7400 - val_loss: 4.7571 - val_acc: 0.5154\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.5436 - acc: 0.7299 - val_loss: 4.6895 - val_acc: 0.5130\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.4711 - acc: 0.7394 - val_loss: 4.6252 - val_acc: 0.5154\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.4264 - acc: 0.7335 - val_loss: 4.5629 - val_acc: 0.5177\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.3577 - acc: 0.7394 - val_loss: 4.5043 - val_acc: 0.5201\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.2915 - acc: 0.7535 - val_loss: 4.4433 - val_acc: 0.5201\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.2491 - acc: 0.7488 - val_loss: 4.3867 - val_acc: 0.5225\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.1954 - acc: 0.7435 - val_loss: 4.3346 - val_acc: 0.5272\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.1506 - acc: 0.7429 - val_loss: 4.2823 - val_acc: 0.5248\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.0821 - acc: 0.7547 - val_loss: 4.2320 - val_acc: 0.5272\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 4.0407 - acc: 0.7553 - val_loss: 4.1840 - val_acc: 0.5177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.9930 - acc: 0.7423 - val_loss: 4.1361 - val_acc: 0.5319\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.9381 - acc: 0.7530 - val_loss: 4.0902 - val_acc: 0.5296\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.8894 - acc: 0.7719 - val_loss: 4.0464 - val_acc: 0.5272\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.8286 - acc: 0.7642 - val_loss: 4.0034 - val_acc: 0.5366\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.8061 - acc: 0.7530 - val_loss: 3.9602 - val_acc: 0.5414\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.7378 - acc: 0.7677 - val_loss: 3.9229 - val_acc: 0.5366\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.7084 - acc: 0.7719 - val_loss: 3.8852 - val_acc: 0.5225\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.6641 - acc: 0.7577 - val_loss: 3.8453 - val_acc: 0.5272\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.6189 - acc: 0.7677 - val_loss: 3.8101 - val_acc: 0.5272\n",
      "Epoch 59/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5898 - acc: 0.7671 - val_loss: 3.7741 - val_acc: 0.5296\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5495 - acc: 0.7813 - val_loss: 3.7411 - val_acc: 0.5319\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.5147 - acc: 0.7683 - val_loss: 3.7062 - val_acc: 0.5343\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4745 - acc: 0.7618 - val_loss: 3.6755 - val_acc: 0.5366\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4373 - acc: 0.7748 - val_loss: 3.6448 - val_acc: 0.5390\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.4052 - acc: 0.7713 - val_loss: 3.6100 - val_acc: 0.5343\n",
      "Epoch 65/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3645 - acc: 0.7736 - val_loss: 3.5803 - val_acc: 0.5414\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3485 - acc: 0.7683 - val_loss: 3.5499 - val_acc: 0.5437\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.3038 - acc: 0.7754 - val_loss: 3.5213 - val_acc: 0.5414\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2709 - acc: 0.7819 - val_loss: 3.4911 - val_acc: 0.5461\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2322 - acc: 0.7849 - val_loss: 3.4651 - val_acc: 0.5437\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.2092 - acc: 0.7766 - val_loss: 3.4385 - val_acc: 0.5414\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1677 - acc: 0.7819 - val_loss: 3.4118 - val_acc: 0.5414\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1456 - acc: 0.7937 - val_loss: 3.3835 - val_acc: 0.5414s: 3.1481 - acc: 0\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.1235 - acc: 0.7742 - val_loss: 3.3606 - val_acc: 0.5414\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0967 - acc: 0.7725 - val_loss: 3.3333 - val_acc: 0.5485\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0576 - acc: 0.7807 - val_loss: 3.3064 - val_acc: 0.5461\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0313 - acc: 0.7973 - val_loss: 3.2824 - val_acc: 0.5508\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 3.0010 - acc: 0.7837 - val_loss: 3.2628 - val_acc: 0.5414\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9797 - acc: 0.7831 - val_loss: 3.2428 - val_acc: 0.5343\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9553 - acc: 0.7920 - val_loss: 3.2181 - val_acc: 0.5414\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9329 - acc: 0.7949 - val_loss: 3.1929 - val_acc: 0.5437\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.9001 - acc: 0.7943 - val_loss: 3.1733 - val_acc: 0.5461\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8795 - acc: 0.7902 - val_loss: 3.1540 - val_acc: 0.5461\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8593 - acc: 0.7872 - val_loss: 3.1349 - val_acc: 0.5437\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8515 - acc: 0.7813 - val_loss: 3.1127 - val_acc: 0.5461\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.8216 - acc: 0.8002 - val_loss: 3.0958 - val_acc: 0.5532\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7905 - acc: 0.7955 - val_loss: 3.0824 - val_acc: 0.5390\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7746 - acc: 0.7908 - val_loss: 3.0604 - val_acc: 0.5485\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7547 - acc: 0.7991 - val_loss: 3.0386 - val_acc: 0.5532\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7381 - acc: 0.7861 - val_loss: 3.0276 - val_acc: 0.5437\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.7081 - acc: 0.7943 - val_loss: 3.0097 - val_acc: 0.5461\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6926 - acc: 0.8115 - val_loss: 2.9985 - val_acc: 0.5414\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6728 - acc: 0.7937 - val_loss: 2.9792 - val_acc: 0.5390\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6589 - acc: 0.7931 - val_loss: 2.9615 - val_acc: 0.5414\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6221 - acc: 0.8097 - val_loss: 2.9474 - val_acc: 0.5296\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.6184 - acc: 0.7996 - val_loss: 2.9340 - val_acc: 0.5319\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5939 - acc: 0.8044 - val_loss: 2.9150 - val_acc: 0.5343\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5793 - acc: 0.8020 - val_loss: 2.9011 - val_acc: 0.5296\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5567 - acc: 0.8091 - val_loss: 2.8815 - val_acc: 0.5414\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5383 - acc: 0.8191 - val_loss: 2.8726 - val_acc: 0.5296\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 2.5410 - acc: 0.7949 - val_loss: 2.8554 - val_acc: 0.5390\n",
      "443/443 [==============================] - 0s 498us/sample - loss: 2.8459 - acc: 0.5350\n",
      "--- Starting trial: run-11\n",
      "{'TIME_WINDOW': 600, 'BATCH_SIZE': 32, 'HIDDEN': 64, 'DROPOUT': 0.3, 'REGULARIZER': 0.005, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 22, 600)]         0         \n",
      "_________________________________________________________________\n",
      "permute_11 (Permute)         (None, 600, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 600, 64)           22272     \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 600, 64)           33024     \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                2457664   \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 2,513,220\n",
      "Trainable params: 2,513,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 37.8056 - acc: 0.2730 - val_loss: 41.2096 - val_acc: 0.3570\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 36.7446 - acc: 0.3676 - val_loss: 39.8325 - val_acc: 0.4113\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 35.7546 - acc: 0.4326 - val_loss: 38.5685 - val_acc: 0.4232\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 34.8607 - acc: 0.4645 - val_loss: 37.3923 - val_acc: 0.4421\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 33.9796 - acc: 0.5266 - val_loss: 36.2908 - val_acc: 0.4563\n",
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 33.1792 - acc: 0.5331 - val_loss: 35.2545 - val_acc: 0.4421\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 32.3890 - acc: 0.5632 - val_loss: 34.2636 - val_acc: 0.4492\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 31.6636 - acc: 0.5892 - val_loss: 33.3191 - val_acc: 0.4586\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 30.9458 - acc: 0.5987 - val_loss: 32.4141 - val_acc: 0.4657\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 30.2584 - acc: 0.6052 - val_loss: 31.5479 - val_acc: 0.4775\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 29.6006 - acc: 0.6152 - val_loss: 30.7185 - val_acc: 0.4657\n",
      "Epoch 12/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 28.9640 - acc: 0.6265 - val_loss: 29.9164 - val_acc: 0.4681\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 28.3574 - acc: 0.6348 - val_loss: 29.1451 - val_acc: 0.4634\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 27.7490 - acc: 0.6407 - val_loss: 28.4006 - val_acc: 0.4704\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 27.1759 - acc: 0.6525 - val_loss: 27.6827 - val_acc: 0.4704\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 26.6286 - acc: 0.6743 - val_loss: 26.9832 - val_acc: 0.4823\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 26.0786 - acc: 0.6673 - val_loss: 26.3125 - val_acc: 0.4894\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 25.5559 - acc: 0.6732 - val_loss: 25.6611 - val_acc: 0.496564\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 25.0435 - acc: 0.6862 - val_loss: 25.0294 - val_acc: 0.4965\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 24.5387 - acc: 0.6856 - val_loss: 24.4166 - val_acc: 0.4988\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 24.0561 - acc: 0.6779 - val_loss: 23.8263 - val_acc: 0.5035\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 23.5870 - acc: 0.6797 - val_loss: 23.2534 - val_acc: 0.4965\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 23.1098 - acc: 0.6980 - val_loss: 22.7026 - val_acc: 0.5059\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 22.6606 - acc: 0.6779 - val_loss: 22.1624 - val_acc: 0.5012\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 22.2142 - acc: 0.7092 - val_loss: 21.6409 - val_acc: 0.5012\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 21.8017 - acc: 0.7021 - val_loss: 21.1404 - val_acc: 0.5106\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 21.3872 - acc: 0.7051 - val_loss: 20.6565 - val_acc: 0.5059\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.9750 - acc: 0.7122 - val_loss: 20.1860 - val_acc: 0.5083\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.5699 - acc: 0.7340 - val_loss: 19.7291 - val_acc: 0.5083\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 20.1822 - acc: 0.7210 - val_loss: 19.2863 - val_acc: 0.5130\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.8272 - acc: 0.7104 - val_loss: 18.8537 - val_acc: 0.5225\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.4521 - acc: 0.7157 - val_loss: 18.4385 - val_acc: 0.5154\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 19.0896 - acc: 0.7134 - val_loss: 18.0356 - val_acc: 0.5225\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.7480 - acc: 0.7270 - val_loss: 17.6483 - val_acc: 0.5177\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.4352 - acc: 0.7027 - val_loss: 17.2680 - val_acc: 0.5201\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 18.0865 - acc: 0.7264 - val_loss: 16.9061 - val_acc: 0.5177\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.7819 - acc: 0.7116 - val_loss: 16.5567 - val_acc: 0.5225\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.4599 - acc: 0.7258 - val_loss: 16.2173 - val_acc: 0.5248\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 17.1652 - acc: 0.7264 - val_loss: 15.8935 - val_acc: 0.5225\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.8460 - acc: 0.7441 - val_loss: 15.5817 - val_acc: 0.5154\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.5813 - acc: 0.7169 - val_loss: 15.2793 - val_acc: 0.5201\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.2977 - acc: 0.7169 - val_loss: 14.9905 - val_acc: 0.5343\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 16.0346 - acc: 0.7317 - val_loss: 14.7083 - val_acc: 0.5296\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.7672 - acc: 0.7335 - val_loss: 14.4418 - val_acc: 0.5201\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.4947 - acc: 0.7394 - val_loss: 14.1824 - val_acc: 0.5201\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.2533 - acc: 0.7405 - val_loss: 13.9334 - val_acc: 0.5154\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 15.0271 - acc: 0.7246 - val_loss: 13.6870 - val_acc: 0.5248\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.7760 - acc: 0.7270 - val_loss: 13.4541 - val_acc: 0.5296\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.5421 - acc: 0.7352 - val_loss: 13.2305 - val_acc: 0.5296\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.3134 - acc: 0.7500 - val_loss: 13.0144 - val_acc: 0.5366\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 14.1075 - acc: 0.7470 - val_loss: 12.7993 - val_acc: 0.5296\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.8973 - acc: 0.7335 - val_loss: 12.5938 - val_acc: 0.5366\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.6770 - acc: 0.7411 - val_loss: 12.3946 - val_acc: 0.5319\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.4831 - acc: 0.7281 - val_loss: 12.2005 - val_acc: 0.5272\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.2752 - acc: 0.7417 - val_loss: 12.0120 - val_acc: 0.5225\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 13.0816 - acc: 0.7411 - val_loss: 11.8246 - val_acc: 0.5225\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.8939 - acc: 0.7470 - val_loss: 11.6459 - val_acc: 0.5201\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.6923 - acc: 0.7459 - val_loss: 11.4725 - val_acc: 0.5248\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.5304 - acc: 0.7465 - val_loss: 11.3045 - val_acc: 0.5225\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.3496 - acc: 0.7411 - val_loss: 11.1393 - val_acc: 0.5248\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.1662 - acc: 0.7435 - val_loss: 10.9789 - val_acc: 0.5225\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 12.0090 - acc: 0.7429 - val_loss: 10.8223 - val_acc: 0.5201\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.8407 - acc: 0.7423 - val_loss: 10.6697 - val_acc: 0.5154\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.6770 - acc: 0.7423 - val_loss: 10.5186 - val_acc: 0.5106\n",
      "Epoch 65/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.5239 - acc: 0.7465 - val_loss: 10.3734 - val_acc: 0.5083\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.3662 - acc: 0.7535 - val_loss: 10.2312 - val_acc: 0.5130\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.2277 - acc: 0.7317 - val_loss: 10.0921 - val_acc: 0.5130\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 11.0748 - acc: 0.7364 - val_loss: 9.9562 - val_acc: 0.5225\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.9341 - acc: 0.7400 - val_loss: 9.8275 - val_acc: 0.5201\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.7879 - acc: 0.7382 - val_loss: 9.6985 - val_acc: 0.5154\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.6359 - acc: 0.7376 - val_loss: 9.5742 - val_acc: 0.5201\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.5216 - acc: 0.7370 - val_loss: 9.4533 - val_acc: 0.5177\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.3915 - acc: 0.7429 - val_loss: 9.3344 - val_acc: 0.5272\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.2562 - acc: 0.7388 - val_loss: 9.2167 - val_acc: 0.5248\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.1345 - acc: 0.7482 - val_loss: 9.1056 - val_acc: 0.5201\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 10.0160 - acc: 0.7358 - val_loss: 8.9935 - val_acc: 0.5272\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.8838 - acc: 0.7447 - val_loss: 8.8857 - val_acc: 0.5272\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.7566 - acc: 0.7500 - val_loss: 8.7824 - val_acc: 0.5248\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.6436 - acc: 0.7494 - val_loss: 8.6797 - val_acc: 0.5319\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.5437 - acc: 0.7405 - val_loss: 8.5793 - val_acc: 0.5296\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.4208 - acc: 0.7530 - val_loss: 8.4820 - val_acc: 0.5296\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.3146 - acc: 0.7606 - val_loss: 8.3877 - val_acc: 0.5225\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.2178 - acc: 0.7488 - val_loss: 8.2958 - val_acc: 0.5272\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.1231 - acc: 0.7494 - val_loss: 8.2048 - val_acc: 0.5437\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 9.0290 - acc: 0.7346 - val_loss: 8.1113 - val_acc: 0.5414\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.9245 - acc: 0.7423 - val_loss: 8.0237 - val_acc: 0.5508\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.8148 - acc: 0.7530 - val_loss: 7.9387 - val_acc: 0.5485\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.7154 - acc: 0.7589 - val_loss: 7.8558 - val_acc: 0.5556\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.6251 - acc: 0.7506 - val_loss: 7.7731 - val_acc: 0.5579\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.5397 - acc: 0.7459 - val_loss: 7.6966 - val_acc: 0.5603\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.4480 - acc: 0.7553 - val_loss: 7.6186 - val_acc: 0.5650\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.3761 - acc: 0.7382 - val_loss: 7.5419 - val_acc: 0.5556\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.2757 - acc: 0.7476 - val_loss: 7.4668 - val_acc: 0.5532\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.1841 - acc: 0.7565 - val_loss: 7.3946 - val_acc: 0.5485\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.1178 - acc: 0.7447 - val_loss: 7.3229 - val_acc: 0.5532\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 8.0432 - acc: 0.7500 - val_loss: 7.2545 - val_acc: 0.5485\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.9499 - acc: 0.7530 - val_loss: 7.1851 - val_acc: 0.5603\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.8813 - acc: 0.7417 - val_loss: 7.1179 - val_acc: 0.5603\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.8117 - acc: 0.7311 - val_loss: 7.0515 - val_acc: 0.5626\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 2s 1ms/sample - loss: 7.7285 - acc: 0.7435 - val_loss: 6.9855 - val_acc: 0.5579\n",
      "443/443 [==============================] - 0s 528us/sample - loss: 6.9890 - acc: 0.5485\n",
      "--- Starting trial: run-12\n",
      "{'TIME_WINDOW': 600, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.2, 'REGULARIZER': 0.001, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 22, 600)]         0         \n",
      "_________________________________________________________________\n",
      "permute_12 (Permute)         (None, 600, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 600, 128)          77312     \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 600, 128)          131584    \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               9830528   \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 10,039,940\n",
      "Trainable params: 10,039,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1692/1692 [==============================] - 4s 3ms/sample - loss: 15.4319 - acc: 0.3209 - val_loss: 15.9311 - val_acc: 0.4303\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 14.5563 - acc: 0.4959 - val_loss: 15.0959 - val_acc: 0.4468\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.8573 - acc: 0.5940 - val_loss: 14.3866 - val_acc: 0.4846\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.2328 - acc: 0.6554 - val_loss: 13.7522 - val_acc: 0.4988\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.6906 - acc: 0.7027 - val_loss: 13.1878 - val_acc: 0.5035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.1932 - acc: 0.7476 - val_loss: 12.6784 - val_acc: 0.5035\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.7298 - acc: 0.7612 - val_loss: 12.1671 - val_acc: 0.5366\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.3165 - acc: 0.7866 - val_loss: 11.7227 - val_acc: 0.5366\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.9160 - acc: 0.8091 - val_loss: 11.3019 - val_acc: 0.5390\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.5577 - acc: 0.8109 - val_loss: 10.9039 - val_acc: 0.5390\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.2074 - acc: 0.8369 - val_loss: 10.5378 - val_acc: 0.5437\n",
      "Epoch 12/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.8774 - acc: 0.8410 - val_loss: 10.1923 - val_acc: 0.5414\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.5657 - acc: 0.8487 - val_loss: 9.8547 - val_acc: 0.5532\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.2784 - acc: 0.8605 - val_loss: 9.5457 - val_acc: 0.5626\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.0061 - acc: 0.8540 - val_loss: 9.2522 - val_acc: 0.5532\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 8.7364 - acc: 0.8747 - val_loss: 8.9608 - val_acc: 0.5650\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 8.4905 - acc: 0.8747 - val_loss: 8.7062 - val_acc: 0.5532\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 8.2563 - acc: 0.8818 - val_loss: 8.4523 - val_acc: 0.5579\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 8.0304 - acc: 0.8995 - val_loss: 8.2232 - val_acc: 0.5461\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 7.8272 - acc: 0.8948 - val_loss: 7.9880 - val_acc: 0.5556\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 7.6201 - acc: 0.8983 - val_loss: 7.7748 - val_acc: 0.5674\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 7.4339 - acc: 0.8978 - val_loss: 7.5785 - val_acc: 0.5556\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 7.2585 - acc: 0.9025 - val_loss: 7.3994 - val_acc: 0.5579\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 7.0785 - acc: 0.9072 - val_loss: 7.2089 - val_acc: 0.5626\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 6.9155 - acc: 0.9202 - val_loss: 7.0370 - val_acc: 0.5626\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 6.7646 - acc: 0.9167 - val_loss: 6.8836 - val_acc: 0.5745\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 6.6183 - acc: 0.9031 - val_loss: 6.7368 - val_acc: 0.5674\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 6.4669 - acc: 0.9173 - val_loss: 6.5906 - val_acc: 0.5697\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 6.3315 - acc: 0.9096 - val_loss: 6.4431 - val_acc: 0.5697\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 6.1866 - acc: 0.9214 - val_loss: 6.3133 - val_acc: 0.5768\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 6.0589 - acc: 0.9261 - val_loss: 6.1866 - val_acc: 0.5816\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.9300 - acc: 0.9291 - val_loss: 6.0751 - val_acc: 0.5721\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.8163 - acc: 0.9249 - val_loss: 5.9597 - val_acc: 0.5745\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.7026 - acc: 0.9303 - val_loss: 5.8527 - val_acc: 0.5697\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.5887 - acc: 0.9379 - val_loss: 5.7550 - val_acc: 0.5721\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.4790 - acc: 0.9350 - val_loss: 5.6595 - val_acc: 0.5745\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.3927 - acc: 0.9326 - val_loss: 5.5618 - val_acc: 0.5721\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.2908 - acc: 0.9326 - val_loss: 5.4824 - val_acc: 0.5674\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.1976 - acc: 0.9291 - val_loss: 5.3976 - val_acc: 0.5603\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.1099 - acc: 0.9279 - val_loss: 5.3176 - val_acc: 0.5650\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.0205 - acc: 0.9320 - val_loss: 5.2401 - val_acc: 0.5626\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.9353 - acc: 0.9379 - val_loss: 5.1712 - val_acc: 0.5579\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.8445 - acc: 0.9433 - val_loss: 5.1032 - val_acc: 0.5650\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.7651 - acc: 0.9397 - val_loss: 5.0268 - val_acc: 0.5721\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.7011 - acc: 0.9368 - val_loss: 4.9647 - val_acc: 0.5768\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.6159 - acc: 0.9474 - val_loss: 4.9055 - val_acc: 0.5697\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.5499 - acc: 0.9486 - val_loss: 4.8569 - val_acc: 0.5674\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.4861 - acc: 0.9326 - val_loss: 4.7899 - val_acc: 0.5768\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.4330 - acc: 0.9374 - val_loss: 4.7284 - val_acc: 0.5745\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.3561 - acc: 0.9480 - val_loss: 4.6795 - val_acc: 0.5745\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.2903 - acc: 0.9427 - val_loss: 4.6288 - val_acc: 0.5697\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.2348 - acc: 0.9456 - val_loss: 4.5790 - val_acc: 0.5745\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.1651 - acc: 0.9474 - val_loss: 4.5375 - val_acc: 0.5650\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.1180 - acc: 0.9486 - val_loss: 4.4815 - val_acc: 0.5721\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.0502 - acc: 0.9580 - val_loss: 4.4374 - val_acc: 0.5745\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.9989 - acc: 0.9545 - val_loss: 4.3978 - val_acc: 0.5745\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.9420 - acc: 0.9557 - val_loss: 4.3476 - val_acc: 0.5745\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.8995 - acc: 0.9563 - val_loss: 4.3105 - val_acc: 0.5626\n",
      "Epoch 59/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.8522 - acc: 0.9545 - val_loss: 4.2619 - val_acc: 0.5745\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.8057 - acc: 0.9569 - val_loss: 4.2257 - val_acc: 0.5674\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.7584 - acc: 0.9521 - val_loss: 4.1792 - val_acc: 0.5626\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.7062 - acc: 0.9604 - val_loss: 4.1473 - val_acc: 0.5603\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.6734 - acc: 0.9580 - val_loss: 4.1080 - val_acc: 0.5650\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.6333 - acc: 0.9533 - val_loss: 4.0758 - val_acc: 0.5674\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.5884 - acc: 0.9545 - val_loss: 4.0432 - val_acc: 0.5650\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.5522 - acc: 0.9504 - val_loss: 4.0051 - val_acc: 0.5674\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.5046 - acc: 0.9657 - val_loss: 3.9729 - val_acc: 0.5697\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.4756 - acc: 0.9533 - val_loss: 3.9506 - val_acc: 0.5626\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.4334 - acc: 0.9527 - val_loss: 3.9254 - val_acc: 0.5603\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.4039 - acc: 0.9557 - val_loss: 3.8899 - val_acc: 0.5626\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.3582 - acc: 0.9610 - val_loss: 3.8609 - val_acc: 0.5650\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.3314 - acc: 0.9616 - val_loss: 3.8335 - val_acc: 0.5626\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.2968 - acc: 0.9592 - val_loss: 3.7996 - val_acc: 0.5603\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.2622 - acc: 0.9586 - val_loss: 3.7761 - val_acc: 0.5650\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.2230 - acc: 0.9604 - val_loss: 3.7411 - val_acc: 0.5626\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.2041 - acc: 0.9622 - val_loss: 3.7249 - val_acc: 0.5626\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.1644 - acc: 0.9663 - val_loss: 3.6897 - val_acc: 0.5626\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.1318 - acc: 0.9710 - val_loss: 3.6701 - val_acc: 0.5626\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.1046 - acc: 0.9675 - val_loss: 3.6303 - val_acc: 0.5603\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.0802 - acc: 0.9639 - val_loss: 3.6121 - val_acc: 0.5626\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.0532 - acc: 0.9598 - val_loss: 3.5940 - val_acc: 0.5626\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.0275 - acc: 0.9645 - val_loss: 3.5712 - val_acc: 0.5697\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.9972 - acc: 0.9663 - val_loss: 3.5562 - val_acc: 0.5721\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.9766 - acc: 0.9663 - val_loss: 3.5256 - val_acc: 0.5674\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.9443 - acc: 0.9758 - val_loss: 3.5112 - val_acc: 0.5721\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.9125 - acc: 0.9704 - val_loss: 3.4835 - val_acc: 0.5697\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.8873 - acc: 0.9699 - val_loss: 3.4692 - val_acc: 0.5674\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.8680 - acc: 0.9734 - val_loss: 3.4419 - val_acc: 0.5626\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.8443 - acc: 0.9764 - val_loss: 3.4301 - val_acc: 0.5745\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.8249 - acc: 0.9675 - val_loss: 3.4117 - val_acc: 0.5697\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.7924 - acc: 0.9710 - val_loss: 3.3976 - val_acc: 0.5674\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.7769 - acc: 0.9693 - val_loss: 3.3830 - val_acc: 0.5697\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.7553 - acc: 0.9722 - val_loss: 3.3632 - val_acc: 0.5721\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.7260 - acc: 0.9693 - val_loss: 3.3430 - val_acc: 0.5721\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.6967 - acc: 0.9758 - val_loss: 3.3308 - val_acc: 0.5697\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.6885 - acc: 0.9704 - val_loss: 3.3072 - val_acc: 0.5721\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.6604 - acc: 0.9758 - val_loss: 3.2995 - val_acc: 0.5674\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.6392 - acc: 0.9740 - val_loss: 3.2831 - val_acc: 0.5721\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.6298 - acc: 0.9734 - val_loss: 3.2679 - val_acc: 0.5745\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.6016 - acc: 0.9734 - val_loss: 3.2528 - val_acc: 0.5721\n",
      "443/443 [==============================] - 0s 586us/sample - loss: 3.2690 - acc: 0.5418\n",
      "--- Starting trial: run-13\n",
      "{'TIME_WINDOW': 600, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.2, 'REGULARIZER': 0.005, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 22, 600)]         0         \n",
      "_________________________________________________________________\n",
      "permute_13 (Permute)         (None, 600, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 600, 128)          77312     \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 600, 128)          131584    \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               9830528   \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 10,039,940\n",
      "Trainable params: 10,039,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1692/1692 [==============================] - 4s 3ms/sample - loss: 73.2804 - acc: 0.3517 - val_loss: 76.1486 - val_acc: 0.4539\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 69.5906 - acc: 0.5431 - val_loss: 72.1427 - val_acc: 0.5012\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 66.3864 - acc: 0.6478 - val_loss: 68.6424 - val_acc: 0.5248\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 63.6129 - acc: 0.6862 - val_loss: 65.5121 - val_acc: 0.5508\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 61.0368 - acc: 0.7281 - val_loss: 62.6334 - val_acc: 0.5532\n",
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 58.6533 - acc: 0.7707 - val_loss: 59.9773 - val_acc: 0.5816\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 56.4600 - acc: 0.7778 - val_loss: 57.5064 - val_acc: 0.5532\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 54.4019 - acc: 0.8067 - val_loss: 55.1695 - val_acc: 0.5603\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 52.4613 - acc: 0.8310 - val_loss: 52.9650 - val_acc: 0.5816\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 50.6246 - acc: 0.8469 - val_loss: 50.8793 - val_acc: 0.5863\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 48.8621 - acc: 0.8434 - val_loss: 48.8998 - val_acc: 0.5839\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 47.2202 - acc: 0.8599 - val_loss: 47.0146 - val_acc: 0.5839\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 45.6552 - acc: 0.8664 - val_loss: 45.2291 - val_acc: 0.5839\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 44.1430 - acc: 0.8765 - val_loss: 43.5323 - val_acc: 0.5887\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 42.7341 - acc: 0.8830 - val_loss: 41.9193 - val_acc: 0.5839\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 41.3680 - acc: 0.8978 - val_loss: 40.3910 - val_acc: 0.5839\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 40.0719 - acc: 0.8913 - val_loss: 38.9378 - val_acc: 0.5957\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 38.8319 - acc: 0.9066 - val_loss: 37.5480 - val_acc: 0.5957\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 37.6744 - acc: 0.9037 - val_loss: 36.2354 - val_acc: 0.5981\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 36.5343 - acc: 0.8995 - val_loss: 34.9956 - val_acc: 0.6052\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 35.4597 - acc: 0.9066 - val_loss: 33.8195 - val_acc: 0.5934\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 34.4187 - acc: 0.9137 - val_loss: 32.7053 - val_acc: 0.5887\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 33.4425 - acc: 0.9054 - val_loss: 31.6524 - val_acc: 0.5981\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 32.5042 - acc: 0.9143 - val_loss: 30.6578 - val_acc: 0.6005\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 31.6071 - acc: 0.9108 - val_loss: 29.7144 - val_acc: 0.5839\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 30.7471 - acc: 0.9072 - val_loss: 28.8272 - val_acc: 0.5839\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 29.9320 - acc: 0.9155 - val_loss: 27.9893 - val_acc: 0.5887\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 29.1430 - acc: 0.9161 - val_loss: 27.1965 - val_acc: 0.5674\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 28.4016 - acc: 0.9232 - val_loss: 26.4392 - val_acc: 0.5863\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 27.6781 - acc: 0.9232 - val_loss: 25.7302 - val_acc: 0.5816\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 27.0012 - acc: 0.9249 - val_loss: 25.0615 - val_acc: 0.5839\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 26.3554 - acc: 0.9208 - val_loss: 24.4208 - val_acc: 0.5816\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 25.7092 - acc: 0.9238 - val_loss: 23.8098 - val_acc: 0.5863\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 25.1324 - acc: 0.9255 - val_loss: 23.2312 - val_acc: 0.5887\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 24.5402 - acc: 0.9303 - val_loss: 22.6796 - val_acc: 0.5745\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 23.9861 - acc: 0.9232 - val_loss: 22.1537 - val_acc: 0.5792\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 23.4659 - acc: 0.9149 - val_loss: 21.6466 - val_acc: 0.5863\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 22.9646 - acc: 0.9232 - val_loss: 21.1665 - val_acc: 0.5839\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 22.4766 - acc: 0.9309 - val_loss: 20.7002 - val_acc: 0.5910\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 22.0061 - acc: 0.9214 - val_loss: 20.2623 - val_acc: 0.5745\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 21.5437 - acc: 0.9255 - val_loss: 19.8356 - val_acc: 0.5816\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 21.0979 - acc: 0.9303 - val_loss: 19.4278 - val_acc: 0.5745\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 20.6897 - acc: 0.9261 - val_loss: 19.0387 - val_acc: 0.5697\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 20.2789 - acc: 0.9255 - val_loss: 18.6591 - val_acc: 0.5816\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 19.8889 - acc: 0.9249 - val_loss: 18.3014 - val_acc: 0.5697\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 19.4967 - acc: 0.9261 - val_loss: 17.9467 - val_acc: 0.5839\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 19.1452 - acc: 0.9267 - val_loss: 17.6079 - val_acc: 0.5910\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 18.7935 - acc: 0.9214 - val_loss: 17.2790 - val_acc: 0.5934\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 18.4371 - acc: 0.9338 - val_loss: 16.9644 - val_acc: 0.5957\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 18.1270 - acc: 0.9208 - val_loss: 16.6596 - val_acc: 0.5934\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 17.8130 - acc: 0.9232 - val_loss: 16.3690 - val_acc: 0.5863\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 17.4977 - acc: 0.9303 - val_loss: 16.0804 - val_acc: 0.6005\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 17.1955 - acc: 0.9350 - val_loss: 15.8047 - val_acc: 0.5934\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 16.9022 - acc: 0.9285 - val_loss: 15.5372 - val_acc: 0.5863\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 16.6228 - acc: 0.9261 - val_loss: 15.2802 - val_acc: 0.5816\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 16.3450 - acc: 0.9243 - val_loss: 15.0298 - val_acc: 0.5816\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 16.0672 - acc: 0.9332 - val_loss: 14.7870 - val_acc: 0.5839\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 15.8474 - acc: 0.9178 - val_loss: 14.5542 - val_acc: 0.5816\n",
      "Epoch 59/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 15.5693 - acc: 0.9273 - val_loss: 14.3232 - val_acc: 0.5816\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 15.3210 - acc: 0.9178 - val_loss: 14.0993 - val_acc: 0.5863\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 15.0816 - acc: 0.9314 - val_loss: 13.8826 - val_acc: 0.5887\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 14.8577 - acc: 0.9232 - val_loss: 13.6655 - val_acc: 0.5863\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 14.6107 - acc: 0.9297 - val_loss: 13.4628 - val_acc: 0.5887\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 14.4079 - acc: 0.9326 - val_loss: 13.2576 - val_acc: 0.5981\n",
      "Epoch 65/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 14.1886 - acc: 0.9220 - val_loss: 13.0630 - val_acc: 0.6005\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.9672 - acc: 0.9173 - val_loss: 12.8738 - val_acc: 0.5910\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.7613 - acc: 0.9273 - val_loss: 12.6852 - val_acc: 0.5957\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.5721 - acc: 0.9261 - val_loss: 12.5038 - val_acc: 0.5981\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.3694 - acc: 0.9261 - val_loss: 12.3297 - val_acc: 0.5910\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.1767 - acc: 0.9356 - val_loss: 12.1571 - val_acc: 0.5934\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.9921 - acc: 0.9267 - val_loss: 11.9913 - val_acc: 0.5934\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.8286 - acc: 0.9261 - val_loss: 11.8218 - val_acc: 0.6028\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.6447 - acc: 0.9362 - val_loss: 11.6621 - val_acc: 0.6005\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.4725 - acc: 0.9208 - val_loss: 11.5097 - val_acc: 0.5981\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.3033 - acc: 0.9267 - val_loss: 11.3510 - val_acc: 0.5981\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.1349 - acc: 0.9374 - val_loss: 11.2004 - val_acc: 0.6028\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.9857 - acc: 0.9208 - val_loss: 11.0620 - val_acc: 0.5934\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.8267 - acc: 0.9309 - val_loss: 10.9161 - val_acc: 0.6005\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.6758 - acc: 0.9267 - val_loss: 10.7855 - val_acc: 0.5934\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.5255 - acc: 0.9267 - val_loss: 10.6506 - val_acc: 0.5957\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.3724 - acc: 0.9249 - val_loss: 10.5222 - val_acc: 0.5887\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.2402 - acc: 0.9196 - val_loss: 10.3931 - val_acc: 0.5887\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.1038 - acc: 0.9196 - val_loss: 10.2699 - val_acc: 0.5887\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.9587 - acc: 0.9273 - val_loss: 10.1415 - val_acc: 0.5910\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.8300 - acc: 0.9226 - val_loss: 10.0219 - val_acc: 0.5887\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.7039 - acc: 0.9220 - val_loss: 9.9034 - val_acc: 0.5887\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.5586 - acc: 0.9267 - val_loss: 9.7904 - val_acc: 0.5910\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.4491 - acc: 0.9226 - val_loss: 9.6795 - val_acc: 0.5934\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.3193 - acc: 0.9238 - val_loss: 9.5705 - val_acc: 0.5981\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.2016 - acc: 0.9297 - val_loss: 9.4619 - val_acc: 0.5981\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.0812 - acc: 0.9303 - val_loss: 9.3544 - val_acc: 0.5981\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.9666 - acc: 0.9243 - val_loss: 9.2545 - val_acc: 0.5981\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.8561 - acc: 0.9220 - val_loss: 9.1508 - val_acc: 0.5981\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.7393 - acc: 0.9220 - val_loss: 9.0531 - val_acc: 0.5981\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.6314 - acc: 0.9190 - val_loss: 8.9553 - val_acc: 0.6052\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.5318 - acc: 0.9249 - val_loss: 8.8626 - val_acc: 0.5981\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.4108 - acc: 0.9279 - val_loss: 8.7638 - val_acc: 0.6099\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.3145 - acc: 0.9350 - val_loss: 8.6816 - val_acc: 0.5981\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.2247 - acc: 0.9291 - val_loss: 8.5879 - val_acc: 0.6028\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.1105 - acc: 0.9309 - val_loss: 8.5042 - val_acc: 0.5934\n",
      "443/443 [==============================] - 0s 561us/sample - loss: 8.4952 - acc: 0.5643\n",
      "--- Starting trial: run-14\n",
      "{'TIME_WINDOW': 600, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(None, 22, 600)]         0         \n",
      "_________________________________________________________________\n",
      "permute_14 (Permute)         (None, 600, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 600, 128)          77312     \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 600, 128)          131584    \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               9830528   \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 10,039,940\n",
      "Trainable params: 10,039,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1692/1692 [==============================] - 4s 3ms/sample - loss: 14.9807 - acc: 0.3067 - val_loss: 15.7669 - val_acc: 0.4043\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 14.2342 - acc: 0.4917 - val_loss: 14.9772 - val_acc: 0.4586\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.6269 - acc: 0.5561 - val_loss: 14.3045 - val_acc: 0.4728\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.1081 - acc: 0.6194 - val_loss: 13.7311 - val_acc: 0.4752\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.6341 - acc: 0.6613 - val_loss: 13.1613 - val_acc: 0.4988\n",
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.2058 - acc: 0.6903 - val_loss: 12.6461 - val_acc: 0.5390\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.8175 - acc: 0.7051 - val_loss: 12.1878 - val_acc: 0.5201\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.4438 - acc: 0.7441 - val_loss: 11.7664 - val_acc: 0.5296\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.0899 - acc: 0.7595 - val_loss: 11.3620 - val_acc: 0.5343\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.7748 - acc: 0.7671 - val_loss: 10.9935 - val_acc: 0.5414\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.4847 - acc: 0.7612 - val_loss: 10.6434 - val_acc: 0.5461\n",
      "Epoch 12/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.1954 - acc: 0.7837 - val_loss: 10.3255 - val_acc: 0.5437\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.9209 - acc: 0.7961 - val_loss: 9.9924 - val_acc: 0.5603\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.6530 - acc: 0.8144 - val_loss: 9.7018 - val_acc: 0.5579\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.4281 - acc: 0.8032 - val_loss: 9.4330 - val_acc: 0.5508\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 9.1858 - acc: 0.8174 - val_loss: 9.1748 - val_acc: 0.5390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 8.9627 - acc: 0.8215 - val_loss: 8.9057 - val_acc: 0.5579\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 8.7542 - acc: 0.8180 - val_loss: 8.6659 - val_acc: 0.5579\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 8.5375 - acc: 0.8227 - val_loss: 8.4409 - val_acc: 0.5603\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 8.3536 - acc: 0.8375 - val_loss: 8.2252 - val_acc: 0.5508\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 8.1580 - acc: 0.8357 - val_loss: 8.0159 - val_acc: 0.5674\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 7.9705 - acc: 0.8410 - val_loss: 7.8028 - val_acc: 0.5839\n",
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 7.7997 - acc: 0.8475 - val_loss: 7.6320 - val_acc: 0.5579\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 7.6328 - acc: 0.8469 - val_loss: 7.4443 - val_acc: 0.5768\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 7.4718 - acc: 0.8351 - val_loss: 7.2742 - val_acc: 0.5768\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 7.3053 - acc: 0.8623 - val_loss: 7.1121 - val_acc: 0.5792\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 7.1625 - acc: 0.8522 - val_loss: 6.9593 - val_acc: 0.5697\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 7.0161 - acc: 0.8493 - val_loss: 6.7976 - val_acc: 0.6028\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 6.8655 - acc: 0.8599 - val_loss: 6.6529 - val_acc: 0.5957\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 6.7237 - acc: 0.8611 - val_loss: 6.5225 - val_acc: 0.5910\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 6.6036 - acc: 0.8522 - val_loss: 6.3874 - val_acc: 0.5957\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 6.4661 - acc: 0.8475 - val_loss: 6.2629 - val_acc: 0.5981\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 6.3344 - acc: 0.8700 - val_loss: 6.1513 - val_acc: 0.5887\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 6.2234 - acc: 0.8676 - val_loss: 6.0348 - val_acc: 0.5910\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 6.1013 - acc: 0.8558 - val_loss: 5.9341 - val_acc: 0.5910\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.9965 - acc: 0.8540 - val_loss: 5.8350 - val_acc: 0.5934\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.8850 - acc: 0.8652 - val_loss: 5.7378 - val_acc: 0.5887\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.7777 - acc: 0.8664 - val_loss: 5.6395 - val_acc: 0.5887\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.6767 - acc: 0.8582 - val_loss: 5.5487 - val_acc: 0.5910\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.5809 - acc: 0.8788 - val_loss: 5.4686 - val_acc: 0.5863\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.4805 - acc: 0.8629 - val_loss: 5.3842 - val_acc: 0.5887\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.3940 - acc: 0.8712 - val_loss: 5.3026 - val_acc: 0.5934\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.3154 - acc: 0.8558 - val_loss: 5.2316 - val_acc: 0.5887\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.2164 - acc: 0.8765 - val_loss: 5.1664 - val_acc: 0.5768\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.1433 - acc: 0.8682 - val_loss: 5.1013 - val_acc: 0.5745\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 5.0799 - acc: 0.8664 - val_loss: 5.0248 - val_acc: 0.5863\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.9887 - acc: 0.8818 - val_loss: 4.9623 - val_acc: 0.5816\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.9046 - acc: 0.8865 - val_loss: 4.9039 - val_acc: 0.5816\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.8476 - acc: 0.8700 - val_loss: 4.8473 - val_acc: 0.5863\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.7690 - acc: 0.8771 - val_loss: 4.7924 - val_acc: 0.5934\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.6935 - acc: 0.8865 - val_loss: 4.7356 - val_acc: 0.5839\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.6503 - acc: 0.8759 - val_loss: 4.6809 - val_acc: 0.5957\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.5800 - acc: 0.8853 - val_loss: 4.6376 - val_acc: 0.5839\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.5225 - acc: 0.8783 - val_loss: 4.5936 - val_acc: 0.5887\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.4575 - acc: 0.8853 - val_loss: 4.5421 - val_acc: 0.5934\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.4083 - acc: 0.8877 - val_loss: 4.4895 - val_acc: 0.6005\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.3425 - acc: 0.8889 - val_loss: 4.4497 - val_acc: 0.5910\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.2905 - acc: 0.8883 - val_loss: 4.4117 - val_acc: 0.5839\n",
      "Epoch 59/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.2401 - acc: 0.8871 - val_loss: 4.3650 - val_acc: 0.5863\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.1836 - acc: 0.8901 - val_loss: 4.3281 - val_acc: 0.5792\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.1385 - acc: 0.8865 - val_loss: 4.2893 - val_acc: 0.5839\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.0786 - acc: 0.9037 - val_loss: 4.2529 - val_acc: 0.5934\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 4.0519 - acc: 0.8865 - val_loss: 4.2164 - val_acc: 0.5816\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.9857 - acc: 0.9025 - val_loss: 4.1883 - val_acc: 0.5839\n",
      "Epoch 65/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.9556 - acc: 0.8983 - val_loss: 4.1495 - val_acc: 0.5863\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.9075 - acc: 0.8966 - val_loss: 4.1183 - val_acc: 0.5839\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.8654 - acc: 0.9025 - val_loss: 4.0805 - val_acc: 0.5957\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.8249 - acc: 0.8924 - val_loss: 4.0445 - val_acc: 0.5957\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.7845 - acc: 0.8907 - val_loss: 4.0186 - val_acc: 0.5957\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.7324 - acc: 0.9025 - val_loss: 3.9880 - val_acc: 0.6005\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.7141 - acc: 0.8930 - val_loss: 3.9674 - val_acc: 0.5887\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.6506 - acc: 0.9001 - val_loss: 3.9299 - val_acc: 0.5910\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.6365 - acc: 0.8972 - val_loss: 3.9072 - val_acc: 0.5957\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.5985 - acc: 0.9007 - val_loss: 3.8697 - val_acc: 0.5934\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.5544 - acc: 0.9084 - val_loss: 3.8589 - val_acc: 0.5863\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.5316 - acc: 0.8983 - val_loss: 3.8189 - val_acc: 0.6028\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.4961 - acc: 0.8978 - val_loss: 3.8024 - val_acc: 0.5934\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.4593 - acc: 0.9090 - val_loss: 3.7790 - val_acc: 0.5957\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.4261 - acc: 0.9043 - val_loss: 3.7612 - val_acc: 0.5768\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.3918 - acc: 0.9060 - val_loss: 3.7355 - val_acc: 0.5910\n",
      "Epoch 81/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.3679 - acc: 0.9066 - val_loss: 3.7071 - val_acc: 0.5934\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.3441 - acc: 0.9043 - val_loss: 3.6904 - val_acc: 0.6005\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.3044 - acc: 0.9113 - val_loss: 3.6732 - val_acc: 0.6028\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.2696 - acc: 0.9178 - val_loss: 3.6499 - val_acc: 0.5981\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.2564 - acc: 0.9048 - val_loss: 3.6202 - val_acc: 0.5934\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.2240 - acc: 0.9048 - val_loss: 3.6144 - val_acc: 0.5887\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.1980 - acc: 0.9066 - val_loss: 3.5915 - val_acc: 0.5839\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.1656 - acc: 0.9143 - val_loss: 3.5683 - val_acc: 0.5768\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.1482 - acc: 0.9108 - val_loss: 3.5521 - val_acc: 0.5792\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.1161 - acc: 0.9054 - val_loss: 3.5358 - val_acc: 0.5768\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.0933 - acc: 0.9143 - val_loss: 3.5154 - val_acc: 0.5792\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.0649 - acc: 0.9173 - val_loss: 3.4940 - val_acc: 0.5839\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.0438 - acc: 0.9155 - val_loss: 3.4726 - val_acc: 0.5768\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 3.0193 - acc: 0.9196 - val_loss: 3.4639 - val_acc: 0.5792\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.9907 - acc: 0.9220 - val_loss: 3.4454 - val_acc: 0.5721\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.9716 - acc: 0.9108 - val_loss: 3.4371 - val_acc: 0.5697\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.9574 - acc: 0.9208 - val_loss: 3.4132 - val_acc: 0.5721\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.9241 - acc: 0.9344 - val_loss: 3.4031 - val_acc: 0.5721\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.9142 - acc: 0.9137 - val_loss: 3.3782 - val_acc: 0.5674\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 2.8848 - acc: 0.9184 - val_loss: 3.3683 - val_acc: 0.5674\n",
      "443/443 [==============================] - 0s 595us/sample - loss: 3.3558 - acc: 0.5463\n",
      "--- Starting trial: run-15\n",
      "{'TIME_WINDOW': 600, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.005, 'LEARNING': 1e-05, 'BETA': 0.9}\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 22, 600)]         0         \n",
      "_________________________________________________________________\n",
      "permute_15 (Permute)         (None, 600, 22)           0         \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 600, 128)          77312     \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 600, 128)          131584    \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 128)               9830528   \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 10,039,940\n",
      "Trainable params: 10,039,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1692/1692 [==============================] - 5s 3ms/sample - loss: 72.1232 - acc: 0.3227 - val_loss: 78.5597 - val_acc: 0.4326\n",
      "Epoch 2/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 68.7646 - acc: 0.4734 - val_loss: 74.4570 - val_acc: 0.4492\n",
      "Epoch 3/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 65.8914 - acc: 0.5449 - val_loss: 70.8123 - val_acc: 0.4752\n",
      "Epoch 4/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 63.2943 - acc: 0.6247 - val_loss: 67.4746 - val_acc: 0.5296\n",
      "Epoch 5/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 60.9397 - acc: 0.6590 - val_loss: 64.4086 - val_acc: 0.5343\n",
      "Epoch 6/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 58.7507 - acc: 0.6862 - val_loss: 61.5525 - val_acc: 0.5650\n",
      "Epoch 7/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 56.7064 - acc: 0.7128 - val_loss: 58.8938 - val_acc: 0.5674\n",
      "Epoch 8/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 54.7550 - acc: 0.7258 - val_loss: 56.3695 - val_acc: 0.5768\n",
      "Epoch 9/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 52.9362 - acc: 0.7506 - val_loss: 54.0206 - val_acc: 0.5674\n",
      "Epoch 10/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 51.2052 - acc: 0.7447 - val_loss: 51.7830 - val_acc: 0.5721\n",
      "Epoch 11/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 49.5593 - acc: 0.7665 - val_loss: 49.6785 - val_acc: 0.5745\n",
      "Epoch 12/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 47.9735 - acc: 0.7914 - val_loss: 47.6841 - val_acc: 0.5697\n",
      "Epoch 13/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 46.4848 - acc: 0.7837 - val_loss: 45.8218 - val_acc: 0.5863\n",
      "Epoch 14/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 45.0255 - acc: 0.8032 - val_loss: 44.0596 - val_acc: 0.5792\n",
      "Epoch 15/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 43.6697 - acc: 0.8020 - val_loss: 42.3924 - val_acc: 0.5863\n",
      "Epoch 16/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 42.3701 - acc: 0.7985 - val_loss: 40.8324 - val_acc: 0.5816\n",
      "Epoch 17/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 41.1391 - acc: 0.8121 - val_loss: 39.3608 - val_acc: 0.5887\n",
      "Epoch 18/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 39.9696 - acc: 0.8132 - val_loss: 37.9788 - val_acc: 0.5816\n",
      "Epoch 19/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 38.8295 - acc: 0.8221 - val_loss: 36.6732 - val_acc: 0.5863\n",
      "Epoch 20/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 37.7432 - acc: 0.8257 - val_loss: 35.4384 - val_acc: 0.5863\n",
      "Epoch 21/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 36.7209 - acc: 0.8310 - val_loss: 34.2710 - val_acc: 0.5934\n",
      "Epoch 22/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 35.7235 - acc: 0.8327 - val_loss: 33.1625 - val_acc: 0.5934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 34.7516 - acc: 0.8511 - val_loss: 32.1270 - val_acc: 0.5792\n",
      "Epoch 24/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 33.8743 - acc: 0.8387 - val_loss: 31.1411 - val_acc: 0.5863\n",
      "Epoch 25/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 33.0316 - acc: 0.8316 - val_loss: 30.2074 - val_acc: 0.5910\n",
      "Epoch 26/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 32.2016 - acc: 0.8221 - val_loss: 29.3274 - val_acc: 0.5981\n",
      "Epoch 27/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 31.3884 - acc: 0.8310 - val_loss: 28.4951 - val_acc: 0.5910\n",
      "Epoch 28/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 30.6465 - acc: 0.8398 - val_loss: 27.7062 - val_acc: 0.5957\n",
      "Epoch 29/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 29.9433 - acc: 0.8404 - val_loss: 26.9551 - val_acc: 0.5887\n",
      "Epoch 30/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 29.2441 - acc: 0.8304 - val_loss: 26.2379 - val_acc: 0.5934\n",
      "Epoch 31/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 28.5605 - acc: 0.8316 - val_loss: 25.5533 - val_acc: 0.5887\n",
      "Epoch 32/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 27.9033 - acc: 0.8446 - val_loss: 24.9034 - val_acc: 0.5934\n",
      "Epoch 33/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 27.2925 - acc: 0.8434 - val_loss: 24.2817 - val_acc: 0.5981\n",
      "Epoch 34/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 26.7102 - acc: 0.8422 - val_loss: 23.6896 - val_acc: 0.5957\n",
      "Epoch 35/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 26.1367 - acc: 0.8446 - val_loss: 23.1282 - val_acc: 0.5957\n",
      "Epoch 36/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 25.5950 - acc: 0.8387 - val_loss: 22.5908 - val_acc: 0.6005\n",
      "Epoch 37/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 25.0585 - acc: 0.8410 - val_loss: 22.0761 - val_acc: 0.5981\n",
      "Epoch 38/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 24.5396 - acc: 0.8434 - val_loss: 21.5882 - val_acc: 0.5981\n",
      "Epoch 39/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 24.0283 - acc: 0.8452 - val_loss: 21.1167 - val_acc: 0.5981\n",
      "Epoch 40/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 23.5592 - acc: 0.8457 - val_loss: 20.6635 - val_acc: 0.6028\n",
      "Epoch 41/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 23.0955 - acc: 0.8446 - val_loss: 20.2317 - val_acc: 0.6028\n",
      "Epoch 42/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 22.6255 - acc: 0.8481 - val_loss: 19.8181 - val_acc: 0.5910\n",
      "Epoch 43/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 22.2202 - acc: 0.8274 - val_loss: 19.4224 - val_acc: 0.5934\n",
      "Epoch 44/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 21.7960 - acc: 0.8493 - val_loss: 19.0412 - val_acc: 0.5887\n",
      "Epoch 45/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 21.3883 - acc: 0.8481 - val_loss: 18.6735 - val_acc: 0.5887\n",
      "Epoch 46/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 20.9970 - acc: 0.8481 - val_loss: 18.3200 - val_acc: 0.5816\n",
      "Epoch 47/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 20.6118 - acc: 0.8457 - val_loss: 17.9805 - val_acc: 0.5792\n",
      "Epoch 48/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 20.2230 - acc: 0.8617 - val_loss: 17.6538 - val_acc: 0.5792\n",
      "Epoch 49/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 19.8772 - acc: 0.8499 - val_loss: 17.3344 - val_acc: 0.5816\n",
      "Epoch 50/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 19.5395 - acc: 0.8570 - val_loss: 17.0267 - val_acc: 0.5863\n",
      "Epoch 51/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 19.1936 - acc: 0.8540 - val_loss: 16.7292 - val_acc: 0.5816\n",
      "Epoch 52/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 18.8629 - acc: 0.8558 - val_loss: 16.4467 - val_acc: 0.5816\n",
      "Epoch 53/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 18.5488 - acc: 0.8493 - val_loss: 16.1652 - val_acc: 0.5863\n",
      "Epoch 54/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 18.2465 - acc: 0.8587 - val_loss: 15.8953 - val_acc: 0.5816\n",
      "Epoch 55/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 17.9497 - acc: 0.8469 - val_loss: 15.6337 - val_acc: 0.5745\n",
      "Epoch 56/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 17.6601 - acc: 0.8552 - val_loss: 15.3858 - val_acc: 0.5721\n",
      "Epoch 57/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 17.3827 - acc: 0.8540 - val_loss: 15.1397 - val_acc: 0.5745\n",
      "Epoch 58/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 17.1235 - acc: 0.8452 - val_loss: 14.9056 - val_acc: 0.5816\n",
      "Epoch 59/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 16.8391 - acc: 0.8452 - val_loss: 14.6713 - val_acc: 0.5910\n",
      "Epoch 60/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 16.5844 - acc: 0.8517 - val_loss: 14.4485 - val_acc: 0.5887\n",
      "Epoch 61/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 16.3243 - acc: 0.8688 - val_loss: 14.2331 - val_acc: 0.5957\n",
      "Epoch 62/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 16.1009 - acc: 0.8398 - val_loss: 14.0219 - val_acc: 0.5981\n",
      "Epoch 63/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 15.8539 - acc: 0.8652 - val_loss: 13.8160 - val_acc: 0.5981\n",
      "Epoch 64/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 15.6343 - acc: 0.8475 - val_loss: 13.6140 - val_acc: 0.6052\n",
      "Epoch 65/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 15.4107 - acc: 0.8528 - val_loss: 13.4244 - val_acc: 0.6028\n",
      "Epoch 66/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 15.1772 - acc: 0.8404 - val_loss: 13.2316 - val_acc: 0.6076\n",
      "Epoch 67/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 14.9831 - acc: 0.8446 - val_loss: 13.0498 - val_acc: 0.6099\n",
      "Epoch 68/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 14.7642 - acc: 0.8641 - val_loss: 12.8681 - val_acc: 0.6147\n",
      "Epoch 69/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 14.5723 - acc: 0.8528 - val_loss: 12.6931 - val_acc: 0.6147\n",
      "Epoch 70/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 14.3675 - acc: 0.8605 - val_loss: 12.5211 - val_acc: 0.6052\n",
      "Epoch 71/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 14.1691 - acc: 0.8558 - val_loss: 12.3550 - val_acc: 0.6099\n",
      "Epoch 72/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.9843 - acc: 0.8652 - val_loss: 12.1892 - val_acc: 0.6099\n",
      "Epoch 73/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.7994 - acc: 0.8570 - val_loss: 12.0344 - val_acc: 0.6076\n",
      "Epoch 74/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.6266 - acc: 0.8582 - val_loss: 11.8753 - val_acc: 0.6005\n",
      "Epoch 75/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.4451 - acc: 0.8688 - val_loss: 11.7228 - val_acc: 0.5981\n",
      "Epoch 76/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.2707 - acc: 0.8558 - val_loss: 11.5748 - val_acc: 0.6028\n",
      "Epoch 77/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 13.1120 - acc: 0.8658 - val_loss: 11.4288 - val_acc: 0.6052\n",
      "Epoch 78/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.9328 - acc: 0.8676 - val_loss: 11.2874 - val_acc: 0.6005\n",
      "Epoch 79/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.7813 - acc: 0.8552 - val_loss: 11.1525 - val_acc: 0.6005\n",
      "Epoch 80/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.6383 - acc: 0.8546 - val_loss: 11.0139 - val_acc: 0.6076\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.4748 - acc: 0.8593 - val_loss: 10.8816 - val_acc: 0.6028\n",
      "Epoch 82/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.3178 - acc: 0.8688 - val_loss: 10.7504 - val_acc: 0.6028\n",
      "Epoch 83/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.1712 - acc: 0.8564 - val_loss: 10.6244 - val_acc: 0.5981\n",
      "Epoch 84/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 12.0292 - acc: 0.8605 - val_loss: 10.4995 - val_acc: 0.6076\n",
      "Epoch 85/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.8933 - acc: 0.8641 - val_loss: 10.3765 - val_acc: 0.6005\n",
      "Epoch 86/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.7567 - acc: 0.8570 - val_loss: 10.2585 - val_acc: 0.6005\n",
      "Epoch 87/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.6038 - acc: 0.8641 - val_loss: 10.1445 - val_acc: 0.5934\n",
      "Epoch 88/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.4755 - acc: 0.8664 - val_loss: 10.0300 - val_acc: 0.6028\n",
      "Epoch 89/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.3575 - acc: 0.8647 - val_loss: 9.9183 - val_acc: 0.6005\n",
      "Epoch 90/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.2373 - acc: 0.8552 - val_loss: 9.8103 - val_acc: 0.6005\n",
      "Epoch 91/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 11.1146 - acc: 0.8528 - val_loss: 9.7010 - val_acc: 0.6005\n",
      "Epoch 92/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.9872 - acc: 0.8534 - val_loss: 9.5948 - val_acc: 0.5934\n",
      "Epoch 93/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.8529 - acc: 0.8676 - val_loss: 9.4919 - val_acc: 0.6005\n",
      "Epoch 94/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.7400 - acc: 0.8564 - val_loss: 9.3925 - val_acc: 0.6028\n",
      "Epoch 95/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.6229 - acc: 0.8652 - val_loss: 9.2918 - val_acc: 0.6005\n",
      "Epoch 96/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.5169 - acc: 0.8599 - val_loss: 9.1991 - val_acc: 0.6052\n",
      "Epoch 97/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.3958 - acc: 0.8641 - val_loss: 9.1038 - val_acc: 0.6028\n",
      "Epoch 98/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.2991 - acc: 0.8605 - val_loss: 9.0080 - val_acc: 0.5981\n",
      "Epoch 99/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.1718 - acc: 0.8623 - val_loss: 8.9132 - val_acc: 0.6005\n",
      "Epoch 100/100\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 10.0696 - acc: 0.8528 - val_loss: 8.8225 - val_acc: 0.6005\n",
      "443/443 [==============================] - 0s 594us/sample - loss: 8.8557 - acc: 0.5553\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "TIME_STRIDE = 1000\n",
    "for window_size in HP_TIME_WINDOW.domain.values:\n",
    "            \n",
    "    X_train_slices, y_train_slices = sliding_window(X_train_norm, \n",
    "                                                    y_train, \n",
    "                                                    time_window=window_size,  \n",
    "                                                    time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "    X_valid_slices, y_valid_slices = sliding_window(X_valid_norm, \n",
    "                                                    y_valid, \n",
    "                                                    time_window=window_size, \n",
    "                                                    time_stride=TIME_STRIDE)\n",
    "\n",
    "    X_test_slices, y_test_slices = sliding_window(X_test_norm, \n",
    "                                                    y_test, \n",
    "                                                    time_window=window_size, \n",
    "                                                    time_stride=TIME_STRIDE)\n",
    "\n",
    "    print(\"Training data shape with slices: {}\".format(X_train_slices.shape))\n",
    "    print(\"Training label shape with slice: {}\".format(y_train_slices.shape))\n",
    "    print(\"Validation data shape with slices: {}\".format(X_valid_slices.shape))\n",
    "    print(\"Validation label shape with slice: {}\".format(y_valid_slices.shape))\n",
    "    print(\"Testing data shape with slices: {}\".format(X_test_slices.shape))\n",
    "    print(\"Testing label shape with slice: {}\".format(y_test_slices.shape))\n",
    "    for num_units in HP_HIDDEN.domain.values:\n",
    "        for batch_size in HP_BATCH_SIZE.domain.values:\n",
    "            for dropout_rate in HP_DROPOUT.domain.values:\n",
    "                for reg in HP_REGULARIZER.domain.values:\n",
    "                    for learning_rate in HP_LEARNING.domain.values:\n",
    "                        for beta in HP_BETA.domain.values:\n",
    "                            hparams = {\n",
    "                                HP_TIME_WINDOW: window_size,\n",
    "                                HP_BATCH_SIZE: batch_size,\n",
    "                                HP_HIDDEN: num_units,\n",
    "                                HP_DROPOUT: dropout_rate,\n",
    "                                HP_REGULARIZER: reg,\n",
    "                                HP_LEARNING: learning_rate,\n",
    "                                HP_BETA: beta,\n",
    "                            }\n",
    "\n",
    "                            run_name = \"run-%d\" % session_num\n",
    "                            print('--- Starting trial: %s' % run_name)\n",
    "                            print({h.name: hparams[h] for h in hparams})\n",
    "                            model = run('logs/hparam_tuning/' + run_name, hparams)\n",
    "                            session_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/20\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.7484 - acc: 0.9688\n",
      "Epoch 00001: val_loss improved from 2.60234 to 2.59535, saving model to ./model_checkpoints/rnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.7491 - acc: 0.9675 - val_loss: 2.5953 - val_acc: 0.5745\n",
      "Epoch 2/20\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7476 - acc: 0.9645\n",
      "Epoch 00002: val_loss improved from 2.59535 to 2.59124, saving model to ./model_checkpoints/rnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 4ms/sample - loss: 1.7472 - acc: 0.9645 - val_loss: 2.5912 - val_acc: 0.5816\n",
      "Epoch 3/20\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7467 - acc: 0.9579\n",
      "Epoch 00003: val_loss did not improve from 2.59124\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.7467 - acc: 0.9586 - val_loss: 2.5988 - val_acc: 0.5697\n",
      "Epoch 4/20\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7326 - acc: 0.9585\n",
      "Epoch 00004: val_loss improved from 2.59124 to 2.58648, saving model to ./model_checkpoints/rnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.7335 - acc: 0.9580 - val_loss: 2.5865 - val_acc: 0.5816\n",
      "Epoch 5/20\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7154 - acc: 0.9730\n",
      "Epoch 00005: val_loss did not improve from 2.58648\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.7150 - acc: 0.9734 - val_loss: 2.5994 - val_acc: 0.5745\n",
      "Epoch 6/20\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.7169 - acc: 0.9657\n",
      "Epoch 00006: val_loss improved from 2.58648 to 2.57722, saving model to ./model_checkpoints/rnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 1.7165 - acc: 0.9651 - val_loss: 2.5772 - val_acc: 0.5768\n",
      "Epoch 7/20\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7048 - acc: 0.9706\n",
      "Epoch 00007: val_loss did not improve from 2.57722\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.7055 - acc: 0.9704 - val_loss: 2.5788 - val_acc: 0.5697\n",
      "Epoch 8/20\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.7184 - acc: 0.9553\n",
      "Epoch 00008: val_loss improved from 2.57722 to 2.56956, saving model to ./model_checkpoints/rnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.7206 - acc: 0.9551 - val_loss: 2.5696 - val_acc: 0.5768\n",
      "Epoch 9/20\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7006 - acc: 0.9645\n",
      "Epoch 00009: val_loss did not improve from 2.56956\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.7016 - acc: 0.9634 - val_loss: 2.5820 - val_acc: 0.5697\n",
      "Epoch 10/20\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6824 - acc: 0.9706\n",
      "Epoch 00010: val_loss did not improve from 2.56956\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.6823 - acc: 0.9704 - val_loss: 2.5793 - val_acc: 0.5674\n",
      "Epoch 11/20\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6888 - acc: 0.9614\n",
      "Epoch 00011: val_loss did not improve from 2.56956\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.6892 - acc: 0.9622 - val_loss: 2.5739 - val_acc: 0.5626\n",
      "Epoch 12/20\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6865 - acc: 0.9700\n",
      "Epoch 00012: val_loss did not improve from 2.56956\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.6860 - acc: 0.9681 - val_loss: 2.5741 - val_acc: 0.5697\n",
      "Epoch 13/20\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6841 - acc: 0.9657\n",
      "Epoch 00013: val_loss improved from 2.56956 to 2.56250, saving model to ./model_checkpoints/rnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 4ms/sample - loss: 1.6827 - acc: 0.9669 - val_loss: 2.5625 - val_acc: 0.5674\n",
      "Epoch 14/20\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6758 - acc: 0.9638\n",
      "Epoch 00014: val_loss did not improve from 2.56250\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.6743 - acc: 0.9651 - val_loss: 2.5638 - val_acc: 0.5650\n",
      "Epoch 15/20\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6639 - acc: 0.9712\n",
      "Epoch 00015: val_loss did not improve from 2.56250\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.6651 - acc: 0.9699 - val_loss: 2.5683 - val_acc: 0.5650\n",
      "Epoch 16/20\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6653 - acc: 0.9638\n",
      "Epoch 00016: val_loss did not improve from 2.56250\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.6673 - acc: 0.9634 - val_loss: 2.5686 - val_acc: 0.5626\n",
      "Epoch 17/20\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6622 - acc: 0.9657\n",
      "Epoch 00017: val_loss improved from 2.56250 to 2.55027, saving model to ./model_checkpoints/rnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.6600 - acc: 0.9657 - val_loss: 2.5503 - val_acc: 0.5650\n",
      "Epoch 18/20\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.6485 - acc: 0.9663\n",
      "Epoch 00018: val_loss did not improve from 2.55027\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.6480 - acc: 0.9669 - val_loss: 2.5583 - val_acc: 0.5674\n",
      "Epoch 19/20\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6381 - acc: 0.9761\n",
      "Epoch 00019: val_loss did not improve from 2.55027\n",
      "1692/1692 [==============================] - 3s 2ms/sample - loss: 1.6383 - acc: 0.9752 - val_loss: 2.5657 - val_acc: 0.5650\n",
      "Epoch 20/20\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.6339 - acc: 0.9663\n",
      "Epoch 00020: val_loss improved from 2.55027 to 2.54316, saving model to ./model_checkpoints/rnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/rnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 1.6363 - acc: 0.9645 - val_loss: 2.5432 - val_acc: 0.5792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3c804a4810>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_slices, y_train_slices, validation_data = (X_valid_slices, y_valid_slices), epochs = 20, callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3b3da22f691df804\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3b3da22f691df804\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 4696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_shallow_model_300 = keras.models.load_model('./model_checkpoints/shallow_model_300')\n",
    "best_shallow_model_500 = keras.models.load_model('./model_checkpoints/shallow_model_500')\n",
    "best_shallow_model_600 = keras.models.load_model('./model_checkpoints/shallow_model_600')\n",
    "best_shallow_model_700 = keras.models.load_model('./model_checkpoints/shallow_model_700')\n",
    "best_shallow_model_800 = keras.models.load_model('./model_checkpoints/shallow_model_800')\n",
    "best_shallow_model_900 = keras.models.load_model('./model_checkpoints/shallow_model_900')\n",
    "best_shallow_model_1000 = keras.models.load_model('./model_checkpoints/shallow_model_1000')\n",
    "\n",
    "accuracies.append(best_shallow_model_1000.evaluate(X_valid_slices, y_valid_slices)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that naive RNN model suffers from overfit - training loss of the model continues to drop while validation loss saturates and accuracy does not go beyond 50%. This can be fixed in 3 ways - dataset augmentation, architecture improvements and regularization.\n",
    "\n",
    "1. Architecture improvements: add dropouts to our layers; try GRU instead of LSTM;\n",
    "2. Dataset augmentation: normilize inputs, decrease the length of the example and upsample using sliding window approach - that produces correlation between examples, but if traning/validation/test sets are separated beforehand, that should be fine.\n",
    "3. Regularizations: \n",
    "\n",
    "Results:\n",
    "\n",
    "1. Dropout layers do not help to improve accuracy. It stays around 45% anyway\n",
    "2. Augmentation allowed to achieve validation accuracy of 55% and test accuracy of 50% if std not squared. Trained for 2 epoch. If std is squared, 2 epochs give 52% validation and 50% test. After this accuracy satruates. From this moment on all data is presumed to be normilized without square.\n",
    "3. Usage of sliding window lead to dramatic decrease of accuracy. From this point on it is assumed that no sliding wndow is applied.\n",
    "4. a) With length of 1000 we can achieve 56% validation 54% test over 5 epochs. After this learning saturates.\n",
    "4. b) Decrease of the length to 800 allowed to achieve 58% validation 55% test over 5 epochs. After this learning saturates.\n",
    "4. c) Decrease of the length to 750 allowed to achieve 59% validation 53% test over 3 epochs. After this learning saturates. \n",
    "4. d) Decrease of the length to 600 allowed to achieve 65% validation 57% test over 5 epochs. After this learning saturates. \n",
    "4. e) Overall, the gain from smaller window is also small +/- 3% at best. This is why from now we will rn experiments with 1000 but in the end also try 750%.\n",
    "\n",
    "5. When network starts to overfit you can decrease learning rate to 1e-4 to get a couple of additional persents.\n",
    "6. Form this point on we use size of 1000 samples per example if reverse is not specified.\n",
    "7. a) Added regularization to hidden layer of 0.001. Validation 56% test 53%\n",
    "7. b) Added regularization to hidden layer of 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_rnn_model_results = aug_rnn_model.evaluate(X_test_slices, y_test_slices)\n",
    "\n",
    "print('RNN model test loss:', aug_rnn_model_results[0])\n",
    "print('RNN model test acc:', aug_rnn_model_results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 654.545454,
   "position": {
    "height": "40px",
    "left": "266.375px",
    "right": "20px",
    "top": "2px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
