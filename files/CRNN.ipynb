{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# import tf\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# import os functions\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"./EEG_data/X_test.npy\")\n",
    "y_test = np.load(\"./EEG_data/y_test.npy\") - 769\n",
    "person_train_valid = np.load(\"./EEG_data/person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"./EEG_data/X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"./EEG_data/y_train_valid.npy\") - 769\n",
    "person_test = np.load(\"./EEG_data/person_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/Valid data shape: (2115, 22, 1000)\n",
      "Test data shape: (443, 22, 1000)\n",
      "Training/Valid target shape: (2115,)\n",
      "Test target shape: (443,)\n",
      "Person train/valid  shape: (2115, 1)\n",
      "Person test shape: (443, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"training/Valid data shape: {}\".format(X_train_valid.shape))       # training data of many persons\n",
    "print(\"Test data shape: {}\".format(X_test.shape))                        # test data of many persons\n",
    "print(\"Training/Valid target shape: {}\".format(y_train_valid.shape))     # training labels of many persons\n",
    "print(\"Test target shape: {}\".format(y_test.shape))                      # test labels of many persons\n",
    "print(\"Person train/valid  shape: {}\".format(person_train_valid.shape))  # which person correspond to the trail in test set\n",
    "print(\"Person test shape: {}\".format(person_test.shape))                 # which person correspond to the trail in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### divide dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1692, 22, 1000)\n",
      "Training label shape: (1692,)\n",
      "Validation data shape: (423, 22, 1000)\n",
      "Validation label shape: (423,)\n",
      "Test data shape: (443, 22, 1000)\n",
      "Test label shape: (443,)\n"
     ]
    }
   ],
   "source": [
    "perm = np.random.permutation(X_train_valid.shape[0])\n",
    "num_train = int(0.8 * X_train_valid.shape[0])\n",
    "num_valid = X_train_valid.shape[0] - num_train\n",
    "X_train =  X_train_valid[perm[0:num_train]]\n",
    "y_train =  y_train_valid[perm[0:num_train]]\n",
    "X_valid = X_train_valid[perm[num_train: ]]\n",
    "y_valid = y_train_valid[perm[num_train: ]]\n",
    "\n",
    "\n",
    "print(\"Training data shape: {}\".format(X_train.shape))\n",
    "print(\"Training label shape: {}\".format(y_train.shape))\n",
    "print(\"Validation data shape: {}\".format(X_valid.shape))\n",
    "print(\"Validation label shape: {}\".format(y_valid.shape))\n",
    "print(\"Test data shape: {}\".format(X_test.shape))\n",
    "print(\"Test label shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(X_arr, y_arr, time_window=100, time_step=1, time_stride=1):\n",
    "    temp_x = np.moveaxis(X_arr, 2, 0)\n",
    "    temp_x = temp_x.astype(np.float32)\n",
    "    buff = []\n",
    "    \n",
    "    num_slices = (len(temp_x)-time_window*time_step) // time_stride + 1\n",
    "    \n",
    "    # get time slices for data\n",
    "    for i in range(num_slices):\n",
    "        buff.append(temp_x[i*time_stride:i*time_stride + time_window*time_step:time_step])\n",
    "        buff[i] = np.moveaxis(buff[i], 0, 2)\n",
    "        # uncomment this if additional dimension is needed\n",
    "        # buff[i] = buff[i].reshape(1, buff[i].shape[0], buff[i].shape[1], buff[i].shape[2])\n",
    "        \n",
    "    temp_x = np.concatenate(buff)\n",
    "        \n",
    "    # get time slice for labels\n",
    "    temp_y = np.ones((X_arr.shape[0],num_slices))\n",
    "    \n",
    "    for i in range(len(y_arr)):\n",
    "        temp_y[i] = temp_y[i] * y_arr[i]\n",
    "        \n",
    "    temp_y = temp_y.reshape((-1))\n",
    "    \n",
    "    return temp_x, temp_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: CRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we make a model inspired by \"Recurrent Deep Neural Networks for Real-Time Sleep Stage Classification From Single Channel EEG\" by E. Bresch, 2018. We reduce kernel size of the filters because our data has less timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_CRNN_model(TIME_WINDOW, dropout=0, regularizer=0):\n",
    "    # input\n",
    "    crnn_input = layers.Input(shape=(22, TIME_WINDOW))\n",
    "\n",
    "\n",
    "    # ================================== CONV1 ================================== #\n",
    "\n",
    "    t1 = tf.keras.layers.Permute((2, 1))(crnn_input)\n",
    "    c1 = layers.Conv1D(32, 4, strides=1)(t1)\n",
    "    new_size = TIME_WINDOW - 8 + 1\n",
    "    \n",
    "    bn1 = layers.BatchNormalization(axis=1)(c1)\n",
    "    a1 = layers.Activation(\"relu\")(bn1)\n",
    "    do1 = layers.Dropout(0.5)(a1)\n",
    "    maxpool1 = layers.MaxPooling1D(4)(do1)\n",
    "    new_size = new_size//8 + 1\n",
    "    \n",
    "    # =========================================================================== #\n",
    "\n",
    "    # ================================== CONV2 ================================== #\n",
    "\n",
    "    c2 = layers.Conv1D(64, 4, strides=1)(maxpool1)\n",
    "    new_size = new_size - 8 + 1\n",
    "    \n",
    "    bn2 = layers.BatchNormalization(axis=1)(c2)\n",
    "    a2 = layers.Activation(\"relu\")(bn2)\n",
    "    do2 = layers.Dropout(0.5)(a2)\n",
    "    maxpool2 = layers.MaxPooling1D(4)(do2)\n",
    "    new_size = new_size//8 + 1\n",
    "    \n",
    "    # =========================================================================== #\n",
    "\n",
    "\n",
    "    # ================================== LSTM ================================== #\n",
    "\n",
    "    lstm3 = layers.LSTM(64, \n",
    "                        return_sequences=True, \n",
    "                        dropout=dropout, \n",
    "                        kernel_regularizer=keras.regularizers.l2(regularizer))(maxpool2)\n",
    "    lstm4 = layers.LSTM(64, \n",
    "                        return_sequences=True, \n",
    "                        dropout=dropout,\n",
    "                        kernel_regularizer=keras.regularizers.l2(regularizer),\n",
    "                        recurrent_constraint=keras.regularizers.l2(regularizer),\n",
    "                        activity_regularizer=keras.regularizers.l2(regularizer))(lstm3)\n",
    "\n",
    "    \n",
    "    # =========================================================================== #\n",
    "\n",
    "    f7 = layers.Flatten()(lstm4)\n",
    "\n",
    "    # output\n",
    "    crnn_output = layers.Dense(4, activation=\"softmax\")(f7)\n",
    "    \n",
    "    return keras.Model(inputs = crnn_input, outputs = crnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn_model_1000 = construct_CRNN_model(1000, dropout=0.3, regularizer=0.001)\n",
    "crnn_model_1000.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute_9 (Permute)          (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 997, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 997, 32)           3988      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 246, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 246, 64)           984       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 61, 64)            33024     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 61, 64)            33024     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 3904)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 15620     \n",
      "=================================================================\n",
      "Total params: 97,744\n",
      "Trainable params: 95,258\n",
      "Non-trainable params: 2,486\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "crnn_model_1000.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/crnn_1000',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]### Make checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5707 - acc: 0.3047\n",
      "Epoch 00001: val_loss improved from inf to 1.50053, saving model to ./model_checkpoints/crnn_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_1000\\assets\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.5689 - acc: 0.3073 - val_loss: 1.5005 - val_acc: 0.3617\n",
      "Epoch 2/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4721 - acc: 0.3552\n",
      "Epoch 00002: val_loss improved from 1.50053 to 1.44891, saving model to ./model_checkpoints/crnn_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_1000\\assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 1.4718 - acc: 0.3546 - val_loss: 1.4489 - val_acc: 0.3806\n",
      "Epoch 3/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4006 - acc: 0.4159\n",
      "Epoch 00003: val_loss improved from 1.44891 to 1.35931, saving model to ./model_checkpoints/crnn_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_1000\\assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 1.4000 - acc: 0.4161 - val_loss: 1.3593 - val_acc: 0.4397\n",
      "Epoch 4/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3119 - acc: 0.4706\n",
      "Epoch 00004: val_loss improved from 1.35931 to 1.29201, saving model to ./model_checkpoints/crnn_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_1000\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.3081 - acc: 0.4716 - val_loss: 1.2920 - val_acc: 0.4586\n",
      "Epoch 5/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2632 - acc: 0.4922\n",
      "Epoch 00005: val_loss improved from 1.29201 to 1.27884, saving model to ./model_checkpoints/crnn_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_1000\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.2589 - acc: 0.4929 - val_loss: 1.2788 - val_acc: 0.4728\n",
      "Epoch 6/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2064 - acc: 0.5138\n",
      "Epoch 00006: val_loss did not improve from 1.27884\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 1.2015 - acc: 0.5154 - val_loss: 1.3308 - val_acc: 0.4468\n",
      "Epoch 7/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1790 - acc: 0.5379\n",
      "Epoch 00007: val_loss improved from 1.27884 to 1.24086, saving model to ./model_checkpoints/crnn_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_1000\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.1775 - acc: 0.5378 - val_loss: 1.2409 - val_acc: 0.4681\n",
      "Epoch 8/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1427 - acc: 0.5397\n",
      "Epoch 00008: val_loss improved from 1.24086 to 1.21503, saving model to ./model_checkpoints/crnn_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_1000\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.1455 - acc: 0.5378 - val_loss: 1.2150 - val_acc: 0.5343\n",
      "Epoch 9/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0929 - acc: 0.5715\n",
      "Epoch 00009: val_loss improved from 1.21503 to 1.17996, saving model to ./model_checkpoints/crnn_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_1000\\assets\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.0946 - acc: 0.5697 - val_loss: 1.1800 - val_acc: 0.5248\n",
      "Epoch 10/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0628 - acc: 0.6076\n",
      "Epoch 00010: val_loss did not improve from 1.17996\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.0621 - acc: 0.6070 - val_loss: 1.2365 - val_acc: 0.4610\n"
     ]
    }
   ],
   "source": [
    "crnn_model_1000_loss_hist = crnn_model_1000.fit(X_train, y_train,\n",
    "                                                validation_data = (X_valid, y_valid),\n",
    "                                                epochs = 10,\n",
    "                                                callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crnn_model_1000 = construct_CRNN_model(1000, dropout=0.3, regularizer=0.001) 55 % at best\n",
    "\n",
    "added activity regularizer.\n",
    "\n",
    "aug_crnn_model_1000 = construct_CRNN_model(1000, dropout=0.3, regularizer=0.01) 43% at best\n",
    "\n",
    "aug_crnn_model_1000 = construct_CRNN_model(1000, dropout=0.5, regularizer=0.005) 53% at best\n",
    "\n",
    "changed relu to elu, addded a new dense elu layer in the end, addded new lstm layer\n",
    "\n",
    "construct_aug_CRNN_model(1000, dropout=0.3, regularizer=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: augmented CRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we add a dense layer in the end. Intuitively, it should help to form better features out of LSTM output. We also add one more layer for improved resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_aug_CRNN_model(TIME_WINDOW, dropout=0, regularizer=0):\n",
    "    # input\n",
    "    crnn_input = layers.Input(shape=(22, TIME_WINDOW))\n",
    "\n",
    "\n",
    "    # ================================== CONV1 ================================== #\n",
    "\n",
    "    t1 = tf.keras.layers.Permute((2, 1))(crnn_input)\n",
    "    c1 = layers.Conv1D(32, 4, strides=1)(t1)\n",
    "    new_size = TIME_WINDOW - 8 + 1\n",
    "    \n",
    "    bn1 = layers.BatchNormalization(axis=1)(c1)\n",
    "    a1 = layers.Activation(\"elu\")(bn1)\n",
    "    do1 = layers.Dropout(0.5)(a1)\n",
    "    maxpool1 = layers.MaxPooling1D(4)(do1)\n",
    "    new_size = new_size//8 + 1\n",
    "    \n",
    "    # =========================================================================== #\n",
    "\n",
    "    # ================================== CONV2 ================================== #\n",
    "\n",
    "    c2 = layers.Conv1D(64, 4, strides=1)(maxpool1)\n",
    "    new_size = new_size - 8 + 1\n",
    "    \n",
    "    bn2 = layers.BatchNormalization(axis=1)(c2)\n",
    "    a2 = layers.Activation(\"elu\")(bn2)\n",
    "    do2 = layers.Dropout(0.5)(a2)\n",
    "    maxpool2 = layers.MaxPooling1D(4)(do2)\n",
    "    new_size = new_size//8 + 1\n",
    "    \n",
    "    # =========================================================================== #\n",
    "\n",
    "\n",
    "    # ================================== LSTM ================================== #\n",
    "\n",
    "    lstm3 = layers.LSTM(64, \n",
    "                        return_sequences=True, \n",
    "                        dropout=dropout, \n",
    "                        kernel_regularizer=keras.regularizers.l2(regularizer),\n",
    "                        activity_regularizer=keras.regularizers.l2(regularizer))(maxpool2)\n",
    "    lstm4 = layers.LSTM(64, \n",
    "                        return_sequences=True, \n",
    "                        dropout=dropout,\n",
    "                        kernel_regularizer=keras.regularizers.l2(regularizer),\n",
    "                        activity_regularizer=keras.regularizers.l2(regularizer))(lstm3)\n",
    "    \n",
    "    lstm5 = layers.LSTM(16, \n",
    "                        return_sequences=True, \n",
    "                        dropout=dropout,\n",
    "                        kernel_regularizer=keras.regularizers.l2(regularizer),\n",
    "                        activity_regularizer=keras.regularizers.l2(regularizer))(lstm4)\n",
    "\n",
    "\n",
    "    \n",
    "    # =========================================================================== #\n",
    "\n",
    "    f7 = layers.Flatten()(lstm5)\n",
    "    \n",
    "    elu7 = layers.Dense(64 * new_size, activation=\"elu\", kernel_regularizer=keras.regularizers.l2(regularizer))(f7)\n",
    "\n",
    "    # output\n",
    "    crnn_output = layers.Dense(4, activation=\"softmax\")(elu7)\n",
    "    \n",
    "    return keras.Model(inputs = crnn_input, outputs = crnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_crnn_model_1000.compile(keras.optimizers.Adam(), \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "aug_crnn_model_1000 = construct_aug_CRNN_model(1000, dropout=0.3, regularizer=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute_29 (Permute)         (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 997, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 997, 32)           3988      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 246, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 246, 64)           984       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_43 (LSTM)               (None, 61, 64)            33024     \n",
      "_________________________________________________________________\n",
      "lstm_44 (LSTM)               (None, 61, 64)            33024     \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, 61, 16)            5184      \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 976)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 960)               937920    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 3844      \n",
      "=================================================================\n",
      "Total params: 1,029,072\n",
      "Trainable params: 1,026,586\n",
      "Non-trainable params: 2,486\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "aug_crnn_model_1000.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/aug_crnn_1000',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]### Make checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9223 - acc: 0.6707\n",
      "Epoch 00001: val_loss did not improve from 1.10012\n",
      "1692/1692 [==============================] - 26s 15ms/sample - loss: 0.9258 - acc: 0.6684 - val_loss: 1.1466 - val_acc: 0.5721\n",
      "Epoch 2/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9012 - acc: 0.6863\n",
      "Epoch 00002: val_loss did not improve from 1.10012\n",
      "1692/1692 [==============================] - 18s 11ms/sample - loss: 0.8974 - acc: 0.6891 - val_loss: 1.1482 - val_acc: 0.5556\n",
      "Epoch 3/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8814 - acc: 0.6965\n",
      "Epoch 00003: val_loss did not improve from 1.10012\n",
      "1692/1692 [==============================] - 18s 11ms/sample - loss: 0.8788 - acc: 0.6974 - val_loss: 1.1237 - val_acc: 0.5792\n",
      "Epoch 4/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8812 - acc: 0.6845\n",
      "Epoch 00004: val_loss did not improve from 1.10012\n",
      "1692/1692 [==============================] - 21s 12ms/sample - loss: 0.8800 - acc: 0.6844 - val_loss: 1.1238 - val_acc: 0.5768\n",
      "Epoch 5/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8648 - acc: 0.6917\n",
      "Epoch 00005: val_loss improved from 1.10012 to 1.07414, saving model to ./model_checkpoints/aug_crnn_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/aug_crnn_1000\\assets\n",
      "1692/1692 [==============================] - 28s 16ms/sample - loss: 0.8645 - acc: 0.6915 - val_loss: 1.0741 - val_acc: 0.5981\n",
      "Epoch 6/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8721 - acc: 0.6995\n",
      "Epoch 00006: val_loss did not improve from 1.07414\n",
      "1692/1692 [==============================] - 18s 11ms/sample - loss: 0.8770 - acc: 0.6968 - val_loss: 1.1090 - val_acc: 0.6028\n",
      "Epoch 7/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8621 - acc: 0.6875\n",
      "Epoch 00007: val_loss did not improve from 1.07414\n",
      "1692/1692 [==============================] - 18s 11ms/sample - loss: 0.8608 - acc: 0.6897 - val_loss: 1.1618 - val_acc: 0.5556\n",
      "Epoch 8/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8272 - acc: 0.7260\n",
      "Epoch 00008: val_loss did not improve from 1.07414\n",
      "1692/1692 [==============================] - 18s 10ms/sample - loss: 0.8261 - acc: 0.7258 - val_loss: 1.1233 - val_acc: 0.5839\n",
      "Epoch 9/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8385 - acc: 0.7037\n",
      "Epoch 00009: val_loss did not improve from 1.07414\n",
      "1692/1692 [==============================] - 18s 11ms/sample - loss: 0.8392 - acc: 0.7033 - val_loss: 1.0855 - val_acc: 0.6147\n",
      "Epoch 10/10\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8614 - acc: 0.6863\n",
      "Epoch 00010: val_loss did not improve from 1.07414\n",
      "1692/1692 [==============================] - 18s 10ms/sample - loss: 0.8636 - acc: 0.6850 - val_loss: 1.0789 - val_acc: 0.6028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2cba18d6208>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_crnn_model_1000.fit(X_train, y_train,\n",
    "                        validation_data = (X_valid, y_valid),\n",
    "                        epochs = 10,\n",
    "                        callbacks = checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: optimization of augmented CRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we vary parameters of CRNN to achieve better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-131-9c95cd4297fe>, line 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-131-9c95cd4297fe>\"\u001b[1;36m, line \u001b[1;32m71\u001b[0m\n\u001b[1;33m    return keras.Model(inputs = crnn_input, outputs = crnn_output)\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def construct_opt_aug_CRNN_model(TIME_WINDOW,\n",
    "                                 filter_size=4,\n",
    "                                 dropout=0.3, \n",
    "                                 regularizer=0.001, \n",
    "                                 last_lstm_size=16, \n",
    "                                 last_hidden_layer=1000,\n",
    "                                 last_dropout=0.3):\n",
    "    # input\n",
    "    crnn_input = layers.Input(shape=(22, TIME_WINDOW))\n",
    "\n",
    "\n",
    "    # ================================== CONV1 ================================== #\n",
    "\n",
    "    t1 = tf.keras.layers.Permute((2, 1))(crnn_input)\n",
    "    c1 = layers.Conv1D(32, filter_size, strides=1)(t1)\n",
    "    new_size = TIME_WINDOW - filter_size + 1\n",
    "    \n",
    "    bn1 = layers.BatchNormalization(axis=1)(c1)\n",
    "    a1 = layers.Activation(\"elu\")(bn1)\n",
    "    do1 = layers.Dropout(0.4)(a1)\n",
    "    maxpool1 = layers.MaxPooling1D(filter_size)(do1)\n",
    "    new_size = new_size//filter_size + 1\n",
    "    \n",
    "    # =========================================================================== #\n",
    "\n",
    "    # ================================== CONV2 ================================== #\n",
    "\n",
    "    c2 = layers.Conv1D(64, filter_size, strides=1)(maxpool1)\n",
    "    new_size = new_size - filter_size + 1\n",
    "    \n",
    "    bn2 = layers.BatchNormalization(axis=1)(c2)\n",
    "    a2 = layers.Activation(\"elu\")(bn2)\n",
    "    do2 = layers.Dropout(0.4)(a2)\n",
    "    maxpool2 = layers.MaxPooling1D(filter_size)(do2)\n",
    "    new_size = new_size//filter_size + 1\n",
    "    \n",
    "    # =========================================================================== #\n",
    "\n",
    "\n",
    "    # ================================== LSTM ================================== #\n",
    "\n",
    "    lstm3 = layers.LSTM(64, \n",
    "                        return_sequences=True, \n",
    "                        dropout=dropout, \n",
    "                        kernel_regularizer=keras.regularizers.l2(regularizer),\n",
    "                        activity_regularizer=keras.regularizers.l2(regularizer))(maxpool2)\n",
    "    lstm4 = layers.LSTM(64, \n",
    "                        return_sequences=True, \n",
    "                        dropout=dropout,\n",
    "                        kernel_regularizer=keras.regularizers.l2(regularizer),\n",
    "                        activity_regularizer=keras.regularizers.l2(regularizer))(lstm3)\n",
    "    \n",
    "    lstm5 = layers.LSTM(last_lstm_size, \n",
    "                        return_sequences=True, \n",
    "                        dropout=dropout,\n",
    "                        kernel_regularizer=keras.regularizers.l2(regularizer),\n",
    "                        activity_regularizer=keras.regularizers.l2(regularizer))(lstm4)\n",
    "\n",
    "\n",
    "    \n",
    "    # =========================================================================== #\n",
    "\n",
    "    f7 = layers.Flatten()(lstm5)\n",
    "    \n",
    "    elu7 = layers.Dense(last_hidden_layer, activation=\"elu\", kernel_regularizer=keras.regularizers.l2(regularizer))(f7)\n",
    "    do7 = layers.Dropout(last_dropout)(elu7)\n",
    "\n",
    "    # output\n",
    "    crnn_output = layers.Dense(4, activation=\"softmax\")(do7)\n",
    "    \n",
    "    return keras.Model(inputs = crnn_input, outputs = crnn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.9761 - acc: 0.2680\n",
      "Epoch 00001: val_loss improved from inf to 2.52723, saving model to ./model_checkpoints/opt_aug_crnn_1000_32\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/opt_aug_crnn_1000_32\\assets\n",
      "1692/1692 [==============================] - 33s 20ms/sample - loss: 2.9697 - acc: 0.2701 - val_loss: 2.5272 - val_acc: 0.3310\n",
      "Epoch 2/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.2934 - acc: 0.3522\n",
      "Epoch 00002: val_loss improved from 2.52723 to 2.07675, saving model to ./model_checkpoints/opt_aug_crnn_1000_32\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/opt_aug_crnn_1000_32\\assets\n",
      "1692/1692 [==============================] - 32s 19ms/sample - loss: 2.2898 - acc: 0.3522 - val_loss: 2.0767 - val_acc: 0.3499\n",
      "Epoch 3/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.9435 - acc: 0.3504\n",
      "Epoch 00003: val_loss improved from 2.07675 to 1.82030, saving model to ./model_checkpoints/opt_aug_crnn_1000_32\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/opt_aug_crnn_1000_32\\assets\n",
      "1692/1692 [==============================] - 32s 19ms/sample - loss: 1.9384 - acc: 0.3534 - val_loss: 1.8203 - val_acc: 0.3664\n",
      "Epoch 4/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7310 - acc: 0.3624\n",
      "Epoch 00004: val_loss improved from 1.82030 to 1.76304, saving model to ./model_checkpoints/opt_aug_crnn_1000_32\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/opt_aug_crnn_1000_32\\assets\n",
      "1692/1692 [==============================] - 33s 20ms/sample - loss: 1.7282 - acc: 0.3629 - val_loss: 1.7630 - val_acc: 0.3239\n",
      "Epoch 5/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5954 - acc: 0.3924\n",
      "Epoch 00005: val_loss improved from 1.76304 to 1.54268, saving model to ./model_checkpoints/opt_aug_crnn_1000_32\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/opt_aug_crnn_1000_32\\assets\n",
      "1692/1692 [==============================] - 32s 19ms/sample - loss: 1.5967 - acc: 0.3918 - val_loss: 1.5427 - val_acc: 0.3972\n",
      "Epoch 6/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4978 - acc: 0.4171\n",
      "Epoch 00006: val_loss improved from 1.54268 to 1.45738, saving model to ./model_checkpoints/opt_aug_crnn_1000_32\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/opt_aug_crnn_1000_32\\assets\n",
      "1692/1692 [==============================] - 34s 20ms/sample - loss: 1.4976 - acc: 0.4167 - val_loss: 1.4574 - val_acc: 0.3972\n",
      "Epoch 7/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4168 - acc: 0.4447\n",
      "Epoch 00007: val_loss improved from 1.45738 to 1.37408, saving model to ./model_checkpoints/opt_aug_crnn_1000_32\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/opt_aug_crnn_1000_32\\assets\n",
      "1692/1692 [==============================] - 31s 18ms/sample - loss: 1.4169 - acc: 0.4439 - val_loss: 1.3741 - val_acc: 0.4775\n",
      "Epoch 8/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3915 - acc: 0.4669\n",
      "Epoch 00008: val_loss did not improve from 1.37408\n",
      "1692/1692 [==============================] - 23s 14ms/sample - loss: 1.3928 - acc: 0.4657 - val_loss: 1.3743 - val_acc: 0.4232\n",
      "Epoch 9/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3469 - acc: 0.4856\n",
      "Epoch 00009: val_loss did not improve from 1.37408\n",
      "1692/1692 [==============================] - 24s 14ms/sample - loss: 1.3450 - acc: 0.4864 - val_loss: 1.4895 - val_acc: 0.4303\n",
      "Epoch 10/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3179 - acc: 0.4970\n",
      "Epoch 00010: val_loss improved from 1.37408 to 1.37372, saving model to ./model_checkpoints/opt_aug_crnn_1000_32\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/opt_aug_crnn_1000_32\\assets\n",
      "1692/1692 [==============================] - 34s 20ms/sample - loss: 1.3193 - acc: 0.4959 - val_loss: 1.3737 - val_acc: 0.4775\n",
      "Epoch 11/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2888 - acc: 0.5252\n",
      "Epoch 00011: val_loss did not improve from 1.37372\n",
      "1692/1692 [==============================] - 22s 13ms/sample - loss: 1.2861 - acc: 0.5254 - val_loss: 1.3995 - val_acc: 0.4208\n",
      "Epoch 12/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2593 - acc: 0.5252\n",
      "Epoch 00012: val_loss improved from 1.37372 to 1.29892, saving model to ./model_checkpoints/opt_aug_crnn_1000_32\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/opt_aug_crnn_1000_32\\assets\n",
      "1692/1692 [==============================] - 31s 18ms/sample - loss: 1.2606 - acc: 0.5260 - val_loss: 1.2989 - val_acc: 0.4965\n",
      "Epoch 13/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2503 - acc: 0.5264\n",
      "Epoch 00013: val_loss did not improve from 1.29892\n",
      "1692/1692 [==============================] - 22s 13ms/sample - loss: 1.2483 - acc: 0.5290 - val_loss: 1.3181 - val_acc: 0.4799\n",
      "Epoch 14/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2369 - acc: 0.5319\n",
      "Epoch 00014: val_loss did not improve from 1.29892\n",
      "1692/1692 [==============================] - 22s 13ms/sample - loss: 1.2374 - acc: 0.5307 - val_loss: 1.3177 - val_acc: 0.4728\n",
      "Epoch 15/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2256 - acc: 0.5481\n",
      "Epoch 00015: val_loss did not improve from 1.29892\n",
      "1692/1692 [==============================] - 22s 13ms/sample - loss: 1.2223 - acc: 0.5496 - val_loss: 1.3395 - val_acc: 0.5201\n",
      "Epoch 16/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1919 - acc: 0.5631\n",
      "Epoch 00016: val_loss did not improve from 1.29892\n",
      "1692/1692 [==============================] - 22s 13ms/sample - loss: 1.1951 - acc: 0.5603 - val_loss: 1.3955 - val_acc: 0.4752\n",
      "Epoch 17/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1930 - acc: 0.5535\n",
      "Epoch 00017: val_loss improved from 1.29892 to 1.23346, saving model to ./model_checkpoints/opt_aug_crnn_1000_32\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/opt_aug_crnn_1000_32\\assets\n",
      "1692/1692 [==============================] - 30s 18ms/sample - loss: 1.1983 - acc: 0.5508 - val_loss: 1.2335 - val_acc: 0.5556\n",
      "Epoch 18/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1659 - acc: 0.5613\n",
      "Epoch 00018: val_loss did not improve from 1.23346\n",
      "1692/1692 [==============================] - 23s 13ms/sample - loss: 1.1631 - acc: 0.5615 - val_loss: 1.2870 - val_acc: 0.5461\n",
      "Epoch 19/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1386 - acc: 0.5913\n",
      "Epoch 00019: val_loss did not improve from 1.23346\n",
      "1692/1692 [==============================] - 24s 14ms/sample - loss: 1.1400 - acc: 0.5916 - val_loss: 1.3188 - val_acc: 0.4846\n",
      "Epoch 20/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1397 - acc: 0.5847\n",
      "Epoch 00020: val_loss did not improve from 1.23346\n",
      "1692/1692 [==============================] - 25s 15ms/sample - loss: 1.1438 - acc: 0.5833 - val_loss: 1.3669 - val_acc: 0.5059\n",
      "Epoch 21/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1466 - acc: 0.5691\n",
      "Epoch 00021: val_loss did not improve from 1.23346\n",
      "1692/1692 [==============================] - 24s 14ms/sample - loss: 1.1449 - acc: 0.5709 - val_loss: 1.3274 - val_acc: 0.4917\n",
      "Epoch 22/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1030 - acc: 0.5944\n",
      "Epoch 00022: val_loss did not improve from 1.23346\n",
      "1692/1692 [==============================] - 24s 14ms/sample - loss: 1.1040 - acc: 0.5928 - val_loss: 1.3491 - val_acc: 0.5437\n",
      "Epoch 23/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1086 - acc: 0.6028\n",
      "Epoch 00023: val_loss improved from 1.23346 to 1.21499, saving model to ./model_checkpoints/opt_aug_crnn_1000_32\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/opt_aug_crnn_1000_32\\assets\n",
      "1692/1692 [==============================] - 34s 20ms/sample - loss: 1.1055 - acc: 0.6046 - val_loss: 1.2150 - val_acc: 0.5579\n",
      "Epoch 24/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0609 - acc: 0.6154\n",
      "Epoch 00024: val_loss did not improve from 1.21499\n",
      "1692/1692 [==============================] - 23s 14ms/sample - loss: 1.0645 - acc: 0.6141 - val_loss: 1.2643 - val_acc: 0.5508\n",
      "Epoch 25/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0611 - acc: 0.6262\n",
      "Epoch 00025: val_loss improved from 1.21499 to 1.19693, saving model to ./model_checkpoints/opt_aug_crnn_1000_32\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/opt_aug_crnn_1000_32\\assets\n",
      "1692/1692 [==============================] - 32s 19ms/sample - loss: 1.0605 - acc: 0.6265 - val_loss: 1.1969 - val_acc: 0.5745\n",
      "Epoch 26/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0389 - acc: 0.6352\n",
      "Epoch 00026: val_loss did not improve from 1.19693\n",
      "1692/1692 [==============================] - 23s 14ms/sample - loss: 1.0362 - acc: 0.6377 - val_loss: 1.2746 - val_acc: 0.5390\n",
      "Epoch 27/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0579 - acc: 0.6232\n",
      "Epoch 00027: val_loss improved from 1.19693 to 1.12696, saving model to ./model_checkpoints/opt_aug_crnn_1000_32\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/opt_aug_crnn_1000_32\\assets\n",
      "1692/1692 [==============================] - 32s 19ms/sample - loss: 1.0550 - acc: 0.6265 - val_loss: 1.1270 - val_acc: 0.6123\n",
      "Epoch 28/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0313 - acc: 0.6334\n",
      "Epoch 00028: val_loss did not improve from 1.12696\n",
      "1692/1692 [==============================] - 24s 14ms/sample - loss: 1.0346 - acc: 0.6324 - val_loss: 1.2148 - val_acc: 0.5556\n",
      "Epoch 29/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0465 - acc: 0.6256\n",
      "Epoch 00029: val_loss did not improve from 1.12696\n",
      "1692/1692 [==============================] - 23s 14ms/sample - loss: 1.0456 - acc: 0.6259 - val_loss: 1.1506 - val_acc: 0.5887\n",
      "Epoch 30/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9943 - acc: 0.6587\n",
      "Epoch 00030: val_loss did not improve from 1.12696\n",
      "1692/1692 [==============================] - 23s 14ms/sample - loss: 0.9991 - acc: 0.6548 - val_loss: 1.1610 - val_acc: 0.5957\n",
      "Epoch 31/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9874 - acc: 0.6538\n",
      "Epoch 00031: val_loss did not improve from 1.12696\n",
      "1692/1692 [==============================] - 23s 14ms/sample - loss: 0.9913 - acc: 0.6543 - val_loss: 1.2229 - val_acc: 0.5863\n",
      "Epoch 32/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9605 - acc: 0.6556\n",
      "Epoch 00032: val_loss did not improve from 1.12696\n",
      "1692/1692 [==============================] - 22s 13ms/sample - loss: 0.9634 - acc: 0.6548 - val_loss: 1.1803 - val_acc: 0.5981\n",
      "Epoch 33/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9821 - acc: 0.6484\n",
      "Epoch 00033: val_loss did not improve from 1.12696\n",
      "1692/1692 [==============================] - 24s 14ms/sample - loss: 0.9841 - acc: 0.6478 - val_loss: 1.1405 - val_acc: 0.5910\n",
      "Epoch 34/75\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9219 - acc: 0.6875\n",
      "Epoch 00034: val_loss did not improve from 1.12696\n",
      "1692/1692 [==============================] - 24s 14ms/sample - loss: 0.9236 - acc: 0.6856 - val_loss: 1.1736 - val_acc: 0.5957\n",
      "Epoch 35/75\n",
      "1280/1692 [=====================>........] - ETA: 5s - loss: 0.9113 - acc: 0.6811WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-14874565c95a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m                       \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                       \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                       callbacks=checkpoint_callback)\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Desktop\\UCLA\\Cources\\winter_2020\\C247\\project_C247\\venv_prj_C247\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TIME_WINDOW = 1000\n",
    "\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/opt_aug_crnn_1000_32' ,\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "opt_aug_crnn_1000 = construct_opt_aug_CRNN_model(TIME_WINDOW, \n",
    "                                                 dropout=0.3, \n",
    "                                                 regularizer=0.001, \n",
    "                                                 last_lstm_size=32, \n",
    "                                                 last_hidden_layer=1000,\n",
    "                                                 last_dropout=0.3)\n",
    "\n",
    "opt_aug_crnn_1000.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "opt_aug_crnn_1000.fit(X_train, y_train,\n",
    "                      validation_data = (X_valid, y_valid),\n",
    "                      epochs = 75,\n",
    "                      callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOW = 1000\n",
    "\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/opt_aug_crnn_1000_64' ,\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "opt_aug_crnn_1000 = construct_opt_aug_CRNN_model(TIME_WINDOW, \n",
    "                                                 dropout=0.3, \n",
    "                                                 regularizer=0.001, \n",
    "                                                 last_lstm_size=64, \n",
    "                                                 last_hidden_layer=1000,\n",
    "                                                 last_dropout=0.3)\n",
    "\n",
    "opt_aug_crnn_1000.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "opt_aug_crnn_1000.fit(X_train, y_train,\n",
    "                      validation_data = (X_valid, y_valid),\n",
    "                      epochs = 75,\n",
    "                      callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW EXPERIMENTS HERE\n",
    "Vary last_lstm_size,  last_hidden_layer, last_dropout and TIME WINDOW. \n",
    "Leave dropout=0.3 and regularization=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_TIME_WINDOW = hp.HParam('TIME_WINDOW', hp.Discrete([750, 1000])) \n",
    "HP_BATCH_SIZE = hp.HParam('BATCH_SIZE', hp.Discrete([32])) \n",
    "HP_HIDDEN = hp.HParam('HIDDEN', hp.Discrete([128]))\n",
    "HP_DROPOUT = hp.HParam('DROPOUT', hp.Discrete([0.3]))\n",
    "HP_REGULARIZER = hp.HParam('REGULARIZER', hp.Discrete([.001]))\n",
    "HP_last_lstm_size = hp.HParam('LAST_LSTM', hp.Discrete([64, 128]))\n",
    "HP_last_hidden_layer = hp.HParam('LAST_HIDDEN', hp.Discrete([500, 1000]))\n",
    "HP_last_dropout = hp.HParam('LAST_DROPOUT', hp.Discrete([0.2, 0.3]))\n",
    "HP_LEARNING = hp.HParam('LEARNING', hp.Discrete([.0001]))\n",
    "HP_BETA = hp.HParam('BETA', hp.Discrete([.9]))\n",
    "VAL_ACCURACY = 'val_accuracy'\n",
    "TEST_ACCURACY = 'test accuracy'\n",
    "\n",
    "\n",
    "X_train_norm = X_train - np.mean(X_train)/np.std(X_train)\n",
    "X_valid_norm = X_valid - np.mean(X_train)/np.std(X_train)\n",
    "X_test_norm = X_test - np.mean(X_train)/np.std(X_train)\n",
    "\n",
    "with tf.summary.create_file_writer('logs/crnn_hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_TIME_WINDOW, \n",
    "#              HP_BATCH_SIZE, \n",
    "#              HP_HIDDEN, \n",
    "#              HP_DROPOUT, \n",
    "#              HP_REGULARIZER, \n",
    "             HP_last_lstm_size,\n",
    "             HP_last_hidden_layer,\n",
    "             HP_last_dropout,\n",
    "#              HP_LEARNING, \n",
    "#              HP_BETA\n",
    "            ],\n",
    "    metrics=[hp.Metric(VAL_ACCURACY, display_name='Val Accuracy'), \n",
    "             hp.Metric(TEST_ACCURACY, display_name='Test Accuracy')],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_opt_aug_CRNN_model(hparams,\n",
    "                                 filter_size=4):\n",
    "    # input\n",
    "    crnn_input = layers.Input(shape=(22, hparams[HP_TIME_WINDOW]))\n",
    "\n",
    "\n",
    "    # ================================== CONV1 ================================== #\n",
    "\n",
    "    t1 = tf.keras.layers.Permute((2, 1))(crnn_input)\n",
    "    c1 = layers.Conv1D(32, filter_size, strides=1)(t1)\n",
    "    new_size = hparams[HP_TIME_WINDOW] - filter_size + 1\n",
    "    \n",
    "    bn1 = layers.BatchNormalization(axis=1)(c1)\n",
    "    a1 = layers.Activation(\"elu\")(bn1)\n",
    "    do1 = layers.Dropout(hparams[HP_DROPOUT])(a1)\n",
    "    maxpool1 = layers.MaxPooling1D(filter_size)(do1)\n",
    "    new_size = new_size//filter_size + 1\n",
    "    \n",
    "    # =========================================================================== #\n",
    "\n",
    "    # ================================== CONV2 ================================== #\n",
    "\n",
    "    c2 = layers.Conv1D(64, filter_size, strides=1)(maxpool1)\n",
    "    new_size = new_size - filter_size + 1\n",
    "    \n",
    "    bn2 = layers.BatchNormalization(axis=1)(c2)\n",
    "    a2 = layers.Activation(\"elu\")(bn2)\n",
    "    do2 = layers.Dropout(hparams[HP_DROPOUT])(a2)\n",
    "    maxpool2 = layers.MaxPooling1D(filter_size)(do2)\n",
    "    new_size = new_size//filter_size + 1\n",
    "    \n",
    "    # =========================================================================== #\n",
    "\n",
    "\n",
    "    # ================================== LSTM ================================== #\n",
    "\n",
    "    lstm3 = layers.LSTM(hparams[HP_HIDDEN], \n",
    "                        return_sequences=True, \n",
    "                        dropout=hparams[HP_DROPOUT], \n",
    "                        kernel_regularizer=keras.regularizers.l2(hparams[HP_REGULARIZER]),\n",
    "                        activity_regularizer=keras.regularizers.l2(hparams[HP_REGULARIZER]))(maxpool2)\n",
    "    lstm4 = layers.LSTM(hparams[HP_HIDDEN], \n",
    "                        return_sequences=True, \n",
    "                        dropout=hparams[HP_DROPOUT],\n",
    "                        kernel_regularizer=keras.regularizers.l2(hparams[HP_REGULARIZER]),\n",
    "                        activity_regularizer=keras.regularizers.l2(hparams[HP_REGULARIZER]))(lstm3)\n",
    "    \n",
    "    lstm5 = layers.LSTM(hparams[HP_last_lstm_size], \n",
    "                        return_sequences=True, \n",
    "                        dropout=hparams[HP_DROPOUT],\n",
    "                        kernel_regularizer=keras.regularizers.l2(hparams[HP_REGULARIZER]),\n",
    "                        activity_regularizer=keras.regularizers.l2(hparams[HP_REGULARIZER]))(lstm4)\n",
    "\n",
    "\n",
    "    \n",
    "    # =========================================================================== #\n",
    "\n",
    "    f7 = layers.Flatten()(lstm5)\n",
    "    \n",
    "    elu7 = layers.Dense(hparams[HP_last_hidden_layer], activation=\"elu\", kernel_regularizer=keras.regularizers.l2(hparams[HP_REGULARIZER]))(f7)\n",
    "    do7 = layers.Dropout(hparams[HP_last_dropout])(elu7)\n",
    "\n",
    "    # output\n",
    "    crnn_output = layers.Dense(4, activation=\"softmax\")(do7)\n",
    "    \n",
    "    return keras.Model(inputs = crnn_input, outputs = crnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/crnn_lstm_model' ,\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "def train_test_crnn_model(hparams):\n",
    "    rnn_lstm_model = construct_opt_aug_CRNN_model(hparams)\n",
    "    \n",
    "    adam = keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING], beta_1=hparams[HP_BETA])\n",
    "\n",
    "    rnn_lstm_model.compile(optimizer=adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    rnn_lstm_model.summary()\n",
    "    hist = rnn_lstm_model.fit(X_train_slices, y_train_slices, validation_data = (X_valid_slices, y_valid_slices), \n",
    "                              epochs = 100, callbacks=checkpoint_callback, batch_size=hparams[HP_BATCH_SIZE]).history\n",
    "    _, accuracy = rnn_lstm_model.evaluate(X_test_slices, y_test_slices)\n",
    "    return accuracy, hist, rnn_lstm_model\n",
    "\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    test_accuracy, hist, model = train_test_crnn_model(hparams)\n",
    "    tf.summary.scalar(VAL_ACCURACY, max(hist['val_acc']), step=1)\n",
    "    tf.summary.scalar(TEST_ACCURACY, test_accuracy, step=1)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape with slices: (1692, 22, 750)\n",
      "Training label shape with slice: (1692,)\n",
      "Validation data shape with slices: (423, 22, 750)\n",
      "Validation label shape with slice: (423,)\n",
      "Testing data shape with slices: (443, 22, 750)\n",
      "Testing label shape with slice: (443,)\n",
      "--- Starting trial: run-0\n",
      "{'TIME_WINDOW': 750, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 64, 'LAST_HIDDEN': 500, 'LAST_DROPOUT': 0.2, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_86 (InputLayer)        [(None, 22, 750)]         0         \n",
      "_________________________________________________________________\n",
      "permute_85 (Permute)         (None, 750, 22)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 747, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_170 (Bat (None, 747, 32)           2988      \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_255 (Dropout)        (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_170 (MaxPoolin (None, 186, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 183, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_171 (Bat (None, 183, 64)           732       \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_256 (Dropout)        (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_171 (MaxPoolin (None, 45, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_255 (LSTM)              (None, 45, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_256 (LSTM)              (None, 45, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_257 (LSTM)              (None, 45, 64)            49408     \n",
      "_________________________________________________________________\n",
      "flatten_85 (Flatten)         (None, 2880)              0         \n",
      "_________________________________________________________________\n",
      "dense_170 (Dense)            (None, 500)               1440500   \n",
      "_________________________________________________________________\n",
      "dropout_257 (Dropout)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_171 (Dense)            (None, 4)                 2004      \n",
      "=================================================================\n",
      "Total params: 1,737,136\n",
      "Trainable params: 1,735,276\n",
      "Non-trainable params: 1,860\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.9881 - acc: 0.3048\n",
      "Epoch 00001: val_loss improved from inf to 2.71781, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 11s 6ms/sample - loss: 2.9742 - acc: 0.3079 - val_loss: 2.7178 - val_acc: 0.3806\n",
      "Epoch 2/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.6965 - acc: 0.3572\n",
      "Epoch 00002: val_loss improved from 2.71781 to 2.54571, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 2.6919 - acc: 0.3576 - val_loss: 2.5457 - val_acc: 0.3901\n",
      "Epoch 3/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.5631 - acc: 0.3705\n",
      "Epoch 00003: val_loss improved from 2.54571 to 2.43038, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 2.5581 - acc: 0.3735 - val_loss: 2.4304 - val_acc: 0.4303\n",
      "Epoch 4/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 2.4455 - acc: 0.4206\n",
      "Epoch 00004: val_loss improved from 2.43038 to 2.33245, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 2.4375 - acc: 0.4214 - val_loss: 2.3325 - val_acc: 0.4634\n",
      "Epoch 5/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.3327 - acc: 0.4405\n",
      "Epoch 00005: val_loss improved from 2.33245 to 2.25319, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 2.3327 - acc: 0.4403 - val_loss: 2.2532 - val_acc: 0.4444\n",
      "Epoch 6/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.2468 - acc: 0.4608\n",
      "Epoch 00006: val_loss improved from 2.25319 to 2.15984, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 2.2422 - acc: 0.4634 - val_loss: 2.1598 - val_acc: 0.4775\n",
      "Epoch 7/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.1691 - acc: 0.4719\n",
      "Epoch 00007: val_loss improved from 2.15984 to 2.09698, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 2.1642 - acc: 0.4716 - val_loss: 2.0970 - val_acc: 0.4894\n",
      "Epoch 8/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.0918 - acc: 0.4748\n",
      "Epoch 00008: val_loss improved from 2.09698 to 2.04114, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 2.0903 - acc: 0.4746 - val_loss: 2.0411 - val_acc: 0.4728\n",
      "Epoch 9/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.0305 - acc: 0.4726\n",
      "Epoch 00009: val_loss improved from 2.04114 to 1.96724, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 2.0308 - acc: 0.4728 - val_loss: 1.9672 - val_acc: 0.4704\n",
      "Epoch 10/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.9685 - acc: 0.4837\n",
      "Epoch 00010: val_loss improved from 1.96724 to 1.92196, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.9681 - acc: 0.4846 - val_loss: 1.9220 - val_acc: 0.4823\n",
      "Epoch 11/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.8897 - acc: 0.5135\n",
      "Epoch 00011: val_loss improved from 1.92196 to 1.87060, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.8900 - acc: 0.5118 - val_loss: 1.8706 - val_acc: 0.4681\n",
      "Epoch 12/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.8665 - acc: 0.5102\n",
      "Epoch 00012: val_loss improved from 1.87060 to 1.81159, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.8660 - acc: 0.5154 - val_loss: 1.8116 - val_acc: 0.5083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.8130 - acc: 0.5121\n",
      "Epoch 00013: val_loss improved from 1.81159 to 1.80665, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.8159 - acc: 0.5065 - val_loss: 1.8067 - val_acc: 0.4752\n",
      "Epoch 14/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.7615 - acc: 0.5356\n",
      "Epoch 00014: val_loss improved from 1.80665 to 1.72905, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.7663 - acc: 0.5313 - val_loss: 1.7290 - val_acc: 0.5201\n",
      "Epoch 15/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.7208 - acc: 0.5181\n",
      "Epoch 00015: val_loss improved from 1.72905 to 1.72069, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.7208 - acc: 0.5201 - val_loss: 1.7207 - val_acc: 0.4775\n",
      "Epoch 16/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.6897 - acc: 0.5299\n",
      "Epoch 00016: val_loss improved from 1.72069 to 1.67140, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.6908 - acc: 0.5313 - val_loss: 1.6714 - val_acc: 0.5059\n",
      "Epoch 17/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.6563 - acc: 0.5331\n",
      "Epoch 00017: val_loss improved from 1.67140 to 1.65456, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.6530 - acc: 0.5325 - val_loss: 1.6546 - val_acc: 0.4681\n",
      "Epoch 18/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.6206 - acc: 0.5387\n",
      "Epoch 00018: val_loss improved from 1.65456 to 1.61137, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.6186 - acc: 0.5396 - val_loss: 1.6114 - val_acc: 0.5390\n",
      "Epoch 19/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.5717 - acc: 0.5669\n",
      "Epoch 00019: val_loss improved from 1.61137 to 1.59968, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.5726 - acc: 0.5632 - val_loss: 1.5997 - val_acc: 0.5012\n",
      "Epoch 20/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.5524 - acc: 0.5644\n",
      "Epoch 00020: val_loss improved from 1.59968 to 1.56736, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.5490 - acc: 0.5650 - val_loss: 1.5674 - val_acc: 0.5177\n",
      "Epoch 21/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.5420 - acc: 0.5404\n",
      "Epoch 00021: val_loss improved from 1.56736 to 1.52845, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.5461 - acc: 0.5378 - val_loss: 1.5284 - val_acc: 0.5319\n",
      "Epoch 22/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5166 - acc: 0.5504\n",
      "Epoch 00022: val_loss improved from 1.52845 to 1.52501, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.5101 - acc: 0.5538 - val_loss: 1.5250 - val_acc: 0.5579\n",
      "Epoch 23/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4802 - acc: 0.5691\n",
      "Epoch 00023: val_loss improved from 1.52501 to 1.48554, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.4754 - acc: 0.5715 - val_loss: 1.4855 - val_acc: 0.5603\n",
      "Epoch 24/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4523 - acc: 0.5748\n",
      "Epoch 00024: val_loss improved from 1.48554 to 1.46720, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.4543 - acc: 0.5733 - val_loss: 1.4672 - val_acc: 0.5792\n",
      "Epoch 25/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.4413 - acc: 0.5781\n",
      "Epoch 00025: val_loss did not improve from 1.46720\n",
      "1692/1692 [==============================] - 1s 547us/sample - loss: 1.4452 - acc: 0.5751 - val_loss: 1.4745 - val_acc: 0.5508\n",
      "Epoch 26/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.4140 - acc: 0.5844\n",
      "Epoch 00026: val_loss improved from 1.46720 to 1.44966, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.4180 - acc: 0.5827 - val_loss: 1.4497 - val_acc: 0.5721\n",
      "Epoch 27/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3841 - acc: 0.5874\n",
      "Epoch 00027: val_loss did not improve from 1.44966\n",
      "1692/1692 [==============================] - 1s 451us/sample - loss: 1.3932 - acc: 0.5839 - val_loss: 1.4504 - val_acc: 0.5485\n",
      "Epoch 28/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3704 - acc: 0.5850\n",
      "Epoch 00028: val_loss improved from 1.44966 to 1.40426, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.3678 - acc: 0.5839 - val_loss: 1.4043 - val_acc: 0.5532\n",
      "Epoch 29/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3478 - acc: 0.5963\n",
      "Epoch 00029: val_loss improved from 1.40426 to 1.39855, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.3488 - acc: 0.5940 - val_loss: 1.3986 - val_acc: 0.5792\n",
      "Epoch 30/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3524 - acc: 0.5888\n",
      "Epoch 00030: val_loss improved from 1.39855 to 1.38855, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.3461 - acc: 0.5934 - val_loss: 1.3885 - val_acc: 0.5981\n",
      "Epoch 31/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3033 - acc: 0.6078\n",
      "Epoch 00031: val_loss improved from 1.38855 to 1.36181, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.3157 - acc: 0.6005 - val_loss: 1.3618 - val_acc: 0.5934\n",
      "Epoch 32/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3030 - acc: 0.6020\n",
      "Epoch 00032: val_loss improved from 1.36181 to 1.36088, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.3059 - acc: 0.6028 - val_loss: 1.3609 - val_acc: 0.5957\n",
      "Epoch 33/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3003 - acc: 0.6044\n",
      "Epoch 00033: val_loss did not improve from 1.36088\n",
      "1692/1692 [==============================] - 1s 547us/sample - loss: 1.2955 - acc: 0.6028 - val_loss: 1.3662 - val_acc: 0.5839\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2855 - acc: 0.6052\n",
      "Epoch 00034: val_loss improved from 1.36088 to 1.34766, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.2846 - acc: 0.6058 - val_loss: 1.3477 - val_acc: 0.5887\n",
      "Epoch 35/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2792 - acc: 0.6167\n",
      "Epoch 00035: val_loss did not improve from 1.34766\n",
      "1692/1692 [==============================] - 1s 513us/sample - loss: 1.2816 - acc: 0.6135 - val_loss: 1.3585 - val_acc: 0.5650\n",
      "Epoch 36/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2451 - acc: 0.6225\n",
      "Epoch 00036: val_loss improved from 1.34766 to 1.29982, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 6ms/sample - loss: 1.2424 - acc: 0.6253 - val_loss: 1.2998 - val_acc: 0.6217\n",
      "Epoch 37/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2505 - acc: 0.6175\n",
      "Epoch 00037: val_loss improved from 1.29982 to 1.28887, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.2455 - acc: 0.6212 - val_loss: 1.2889 - val_acc: 0.6052\n",
      "Epoch 38/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2118 - acc: 0.6213\n",
      "Epoch 00038: val_loss did not improve from 1.28887\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 1.2183 - acc: 0.6212 - val_loss: 1.3047 - val_acc: 0.6005\n",
      "Epoch 39/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2323 - acc: 0.6127\n",
      "Epoch 00039: val_loss did not improve from 1.28887\n",
      "1692/1692 [==============================] - 1s 388us/sample - loss: 1.2320 - acc: 0.6129 - val_loss: 1.2954 - val_acc: 0.5887\n",
      "Epoch 40/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2102 - acc: 0.6224\n",
      "Epoch 00040: val_loss improved from 1.28887 to 1.28136, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.2169 - acc: 0.6206 - val_loss: 1.2814 - val_acc: 0.6170\n",
      "Epoch 41/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.1748 - acc: 0.6497\n",
      "Epoch 00041: val_loss improved from 1.28136 to 1.27236, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.1841 - acc: 0.6448 - val_loss: 1.2724 - val_acc: 0.6123\n",
      "Epoch 42/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1834 - acc: 0.6394\n",
      "Epoch 00042: val_loss did not improve from 1.27236\n",
      "1692/1692 [==============================] - 1s 540us/sample - loss: 1.1828 - acc: 0.6401 - val_loss: 1.2889 - val_acc: 0.6052\n",
      "Epoch 43/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1775 - acc: 0.6406\n",
      "Epoch 00043: val_loss improved from 1.27236 to 1.25395, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.1787 - acc: 0.6395 - val_loss: 1.2539 - val_acc: 0.6170\n",
      "Epoch 44/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1505 - acc: 0.6394\n",
      "Epoch 00044: val_loss did not improve from 1.25395\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 1.1493 - acc: 0.6401 - val_loss: 1.2992 - val_acc: 0.6052\n",
      "Epoch 45/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1401 - acc: 0.6469\n",
      "Epoch 00045: val_loss did not improve from 1.25395\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 1.1428 - acc: 0.6430 - val_loss: 1.2546 - val_acc: 0.6194\n",
      "Epoch 46/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1430 - acc: 0.6544\n",
      "Epoch 00046: val_loss improved from 1.25395 to 1.24712, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.1376 - acc: 0.6572 - val_loss: 1.2471 - val_acc: 0.6265\n",
      "Epoch 47/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1309 - acc: 0.6514\n",
      "Epoch 00047: val_loss did not improve from 1.24712\n",
      "1692/1692 [==============================] - 1s 439us/sample - loss: 1.1330 - acc: 0.6519 - val_loss: 1.3169 - val_acc: 0.5957\n",
      "Epoch 48/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1315 - acc: 0.6562\n",
      "Epoch 00048: val_loss improved from 1.24712 to 1.24327, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.1294 - acc: 0.6596 - val_loss: 1.2433 - val_acc: 0.6312\n",
      "Epoch 49/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0982 - acc: 0.6642\n",
      "Epoch 00049: val_loss did not improve from 1.24327\n",
      "1692/1692 [==============================] - 1s 545us/sample - loss: 1.0985 - acc: 0.6613 - val_loss: 1.2717 - val_acc: 0.6052\n",
      "Epoch 50/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1202 - acc: 0.6562\n",
      "Epoch 00050: val_loss did not improve from 1.24327\n",
      "1692/1692 [==============================] - 1s 528us/sample - loss: 1.1157 - acc: 0.6584 - val_loss: 1.2435 - val_acc: 0.6217\n",
      "Epoch 51/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1097 - acc: 0.6635\n",
      "Epoch 00051: val_loss did not improve from 1.24327\n",
      "1692/1692 [==============================] - 1s 537us/sample - loss: 1.1090 - acc: 0.6637 - val_loss: 1.2433 - val_acc: 0.6241\n",
      "Epoch 52/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0963 - acc: 0.6556\n",
      "Epoch 00052: val_loss improved from 1.24327 to 1.23801, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.0922 - acc: 0.6578 - val_loss: 1.2380 - val_acc: 0.6147\n",
      "Epoch 53/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0657 - acc: 0.6926\n",
      "Epoch 00053: val_loss improved from 1.23801 to 1.22021, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.0634 - acc: 0.6933 - val_loss: 1.2202 - val_acc: 0.6359\n",
      "Epoch 54/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0796 - acc: 0.6669\n",
      "Epoch 00054: val_loss did not improve from 1.22021\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 1.0716 - acc: 0.6684 - val_loss: 1.2583 - val_acc: 0.6336\n",
      "Epoch 55/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.0649 - acc: 0.6686\n",
      "Epoch 00055: val_loss did not improve from 1.22021\n",
      "1692/1692 [==============================] - 1s 413us/sample - loss: 1.0624 - acc: 0.6732 - val_loss: 1.2586 - val_acc: 0.6241\n",
      "Epoch 56/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0728 - acc: 0.6773\n",
      "Epoch 00056: val_loss did not improve from 1.22021\n",
      "1692/1692 [==============================] - 1s 407us/sample - loss: 1.0694 - acc: 0.6773 - val_loss: 1.2293 - val_acc: 0.6454\n",
      "Epoch 57/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0563 - acc: 0.6863\n",
      "Epoch 00057: val_loss improved from 1.22021 to 1.19543, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.0580 - acc: 0.6856 - val_loss: 1.1954 - val_acc: 0.6336\n",
      "Epoch 58/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0535 - acc: 0.6800\n",
      "Epoch 00058: val_loss did not improve from 1.19543\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 1.0464 - acc: 0.6856 - val_loss: 1.2257 - val_acc: 0.6359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0384 - acc: 0.6995\n",
      "Epoch 00059: val_loss did not improve from 1.19543\n",
      "1692/1692 [==============================] - 1s 397us/sample - loss: 1.0391 - acc: 0.7004 - val_loss: 1.2278 - val_acc: 0.6430\n",
      "Epoch 60/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0171 - acc: 0.7085\n",
      "Epoch 00060: val_loss did not improve from 1.19543\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 1.0221 - acc: 0.7033 - val_loss: 1.2570 - val_acc: 0.6288\n",
      "Epoch 61/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0292 - acc: 0.6869\n",
      "Epoch 00061: val_loss did not improve from 1.19543\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 1.0212 - acc: 0.6915 - val_loss: 1.2312 - val_acc: 0.6265\n",
      "Epoch 62/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9851 - acc: 0.6929\n",
      "Epoch 00062: val_loss did not improve from 1.19543\n",
      "1692/1692 [==============================] - 1s 408us/sample - loss: 0.9889 - acc: 0.6927 - val_loss: 1.2163 - val_acc: 0.6501\n",
      "Epoch 63/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0130 - acc: 0.6958\n",
      "Epoch 00063: val_loss improved from 1.19543 to 1.18538, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 9s 6ms/sample - loss: 1.0147 - acc: 0.6944 - val_loss: 1.1854 - val_acc: 0.6478\n",
      "Epoch 64/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9907 - acc: 0.6956\n",
      "Epoch 00064: val_loss did not improve from 1.18538\n",
      "1692/1692 [==============================] - 1s 465us/sample - loss: 0.9971 - acc: 0.6921 - val_loss: 1.1970 - val_acc: 0.6548\n",
      "Epoch 65/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9867 - acc: 0.7083\n",
      "Epoch 00065: val_loss did not improve from 1.18538\n",
      "1692/1692 [==============================] - 1s 566us/sample - loss: 0.9841 - acc: 0.7086 - val_loss: 1.2295 - val_acc: 0.6359\n",
      "Epoch 66/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9760 - acc: 0.7022\n",
      "Epoch 00066: val_loss did not improve from 1.18538\n",
      "1692/1692 [==============================] - 1s 567us/sample - loss: 0.9803 - acc: 0.7004 - val_loss: 1.2144 - val_acc: 0.6454\n",
      "Epoch 67/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9828 - acc: 0.7001\n",
      "Epoch 00067: val_loss did not improve from 1.18538\n",
      "1692/1692 [==============================] - 1s 567us/sample - loss: 0.9795 - acc: 0.7027 - val_loss: 1.2175 - val_acc: 0.6288\n",
      "Epoch 68/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9819 - acc: 0.7037\n",
      "Epoch 00068: val_loss did not improve from 1.18538\n",
      "1692/1692 [==============================] - 1s 570us/sample - loss: 0.9870 - acc: 0.7027 - val_loss: 1.2164 - val_acc: 0.6359\n",
      "Epoch 69/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9571 - acc: 0.7230\n",
      "Epoch 00069: val_loss did not improve from 1.18538\n",
      "1692/1692 [==============================] - 1s 543us/sample - loss: 0.9536 - acc: 0.7228 - val_loss: 1.1933 - val_acc: 0.6501\n",
      "Epoch 70/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9513 - acc: 0.7162\n",
      "Epoch 00070: val_loss improved from 1.18538 to 1.17312, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 0.9527 - acc: 0.7187 - val_loss: 1.1731 - val_acc: 0.6407\n",
      "Epoch 71/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9491 - acc: 0.7175\n",
      "Epoch 00071: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 555us/sample - loss: 0.9525 - acc: 0.7169 - val_loss: 1.1952 - val_acc: 0.6619\n",
      "Epoch 72/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9513 - acc: 0.7077\n",
      "Epoch 00072: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 552us/sample - loss: 0.9544 - acc: 0.7051 - val_loss: 1.1967 - val_acc: 0.6454\n",
      "Epoch 73/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9575 - acc: 0.7121\n",
      "Epoch 00073: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 0.9597 - acc: 0.7116 - val_loss: 1.2188 - val_acc: 0.6383\n",
      "Epoch 74/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9268 - acc: 0.7310\n",
      "Epoch 00074: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 569us/sample - loss: 0.9264 - acc: 0.7323 - val_loss: 1.2466 - val_acc: 0.6548\n",
      "Epoch 75/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9404 - acc: 0.7225\n",
      "Epoch 00075: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 564us/sample - loss: 0.9389 - acc: 0.7210 - val_loss: 1.1933 - val_acc: 0.6596\n",
      "Epoch 76/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9235 - acc: 0.7344\n",
      "Epoch 00076: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 527us/sample - loss: 0.9230 - acc: 0.7335 - val_loss: 1.2047 - val_acc: 0.6383\n",
      "Epoch 77/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9221 - acc: 0.7331\n",
      "Epoch 00077: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 575us/sample - loss: 0.9185 - acc: 0.7335 - val_loss: 1.2550 - val_acc: 0.6407\n",
      "Epoch 78/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9051 - acc: 0.7404\n",
      "Epoch 00078: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 0.9080 - acc: 0.7388 - val_loss: 1.2124 - val_acc: 0.6596\n",
      "Epoch 79/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8941 - acc: 0.7449\n",
      "Epoch 00079: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 565us/sample - loss: 0.9043 - acc: 0.7405 - val_loss: 1.1824 - val_acc: 0.6572\n",
      "Epoch 80/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9046 - acc: 0.7321\n",
      "Epoch 00080: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 528us/sample - loss: 0.8982 - acc: 0.7364 - val_loss: 1.2893 - val_acc: 0.6383\n",
      "Epoch 81/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9104 - acc: 0.7292\n",
      "Epoch 00081: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 551us/sample - loss: 0.9115 - acc: 0.7264 - val_loss: 1.2117 - val_acc: 0.6690\n",
      "Epoch 82/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8840 - acc: 0.7518\n",
      "Epoch 00082: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 560us/sample - loss: 0.8894 - acc: 0.7494 - val_loss: 1.2079 - val_acc: 0.6501\n",
      "Epoch 83/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9077 - acc: 0.7423\n",
      "Epoch 00083: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 0.9063 - acc: 0.7429 - val_loss: 1.2108 - val_acc: 0.6501\n",
      "Epoch 84/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8815 - acc: 0.7416\n",
      "Epoch 00084: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 572us/sample - loss: 0.8767 - acc: 0.7447 - val_loss: 1.2280 - val_acc: 0.6690\n",
      "Epoch 85/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8985 - acc: 0.7277\n",
      "Epoch 00085: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 565us/sample - loss: 0.8976 - acc: 0.7311 - val_loss: 1.2330 - val_acc: 0.6548\n",
      "Epoch 86/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8947 - acc: 0.7414\n",
      "Epoch 00086: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 572us/sample - loss: 0.8984 - acc: 0.7400 - val_loss: 1.1948 - val_acc: 0.6596\n",
      "Epoch 87/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8811 - acc: 0.7475\n",
      "Epoch 00087: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 551us/sample - loss: 0.8770 - acc: 0.7500 - val_loss: 1.2569 - val_acc: 0.6241\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8488 - acc: 0.7525\n",
      "Epoch 00088: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 549us/sample - loss: 0.8494 - acc: 0.7512 - val_loss: 1.2550 - val_acc: 0.6430\n",
      "Epoch 89/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8538 - acc: 0.7570\n",
      "Epoch 00089: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 0.8505 - acc: 0.7589 - val_loss: 1.3095 - val_acc: 0.6430\n",
      "Epoch 90/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8756 - acc: 0.7423\n",
      "Epoch 00090: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 569us/sample - loss: 0.8753 - acc: 0.7441 - val_loss: 1.2046 - val_acc: 0.6643\n",
      "Epoch 91/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8550 - acc: 0.7506\n",
      "Epoch 00091: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 573us/sample - loss: 0.8488 - acc: 0.7541 - val_loss: 1.2869 - val_acc: 0.6288\n",
      "Epoch 92/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8738 - acc: 0.7425\n",
      "Epoch 00092: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 558us/sample - loss: 0.8757 - acc: 0.7394 - val_loss: 1.1817 - val_acc: 0.6714\n",
      "Epoch 93/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8382 - acc: 0.7598\n",
      "Epoch 00093: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 569us/sample - loss: 0.8387 - acc: 0.7595 - val_loss: 1.2479 - val_acc: 0.6619\n",
      "Epoch 94/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8293 - acc: 0.7531\n",
      "Epoch 00094: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 555us/sample - loss: 0.8322 - acc: 0.7530 - val_loss: 1.2641 - val_acc: 0.6738\n",
      "Epoch 95/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8173 - acc: 0.7680\n",
      "Epoch 00095: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 0.8180 - acc: 0.7683 - val_loss: 1.2512 - val_acc: 0.6572\n",
      "Epoch 96/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8554 - acc: 0.7500\n",
      "Epoch 00096: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 567us/sample - loss: 0.8548 - acc: 0.7512 - val_loss: 1.2791 - val_acc: 0.6501\n",
      "Epoch 97/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8302 - acc: 0.7650\n",
      "Epoch 00097: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 551us/sample - loss: 0.8308 - acc: 0.7642 - val_loss: 1.2582 - val_acc: 0.6619\n",
      "Epoch 98/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8305 - acc: 0.7586\n",
      "Epoch 00098: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 0.8328 - acc: 0.7577 - val_loss: 1.2674 - val_acc: 0.6501\n",
      "Epoch 99/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8119 - acc: 0.7799\n",
      "Epoch 00099: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 476us/sample - loss: 0.8066 - acc: 0.7801 - val_loss: 1.2610 - val_acc: 0.6478\n",
      "Epoch 100/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8132 - acc: 0.7678\n",
      "Epoch 00100: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 0.8127 - acc: 0.7677 - val_loss: 1.2588 - val_acc: 0.6667\n",
      "443/443 [==============================] - 0s 164us/sample - loss: 1.2735 - acc: 0.6366\n",
      "--- Starting trial: run-1\n",
      "{'TIME_WINDOW': 750, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 64, 'LAST_HIDDEN': 500, 'LAST_DROPOUT': 0.3, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_86\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_87 (InputLayer)        [(None, 22, 750)]         0         \n",
      "_________________________________________________________________\n",
      "permute_86 (Permute)         (None, 750, 22)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 747, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_172 (Bat (None, 747, 32)           2988      \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_258 (Dropout)        (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_172 (MaxPoolin (None, 186, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 183, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_173 (Bat (None, 183, 64)           732       \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_259 (Dropout)        (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_173 (MaxPoolin (None, 45, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_258 (LSTM)              (None, 45, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_259 (LSTM)              (None, 45, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_260 (LSTM)              (None, 45, 64)            49408     \n",
      "_________________________________________________________________\n",
      "flatten_86 (Flatten)         (None, 2880)              0         \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 500)               1440500   \n",
      "_________________________________________________________________\n",
      "dropout_260 (Dropout)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_173 (Dense)            (None, 4)                 2004      \n",
      "=================================================================\n",
      "Total params: 1,737,136\n",
      "Trainable params: 1,735,276\n",
      "Non-trainable params: 1,860\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.0210 - acc: 0.2812\n",
      "Epoch 00001: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 3.0066 - acc: 0.2843 - val_loss: 2.7553 - val_acc: 0.3570\n",
      "Epoch 2/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.7233 - acc: 0.3370\n",
      "Epoch 00002: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 548us/sample - loss: 2.7193 - acc: 0.3387 - val_loss: 2.5924 - val_acc: 0.3877\n",
      "Epoch 3/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.5900 - acc: 0.3678\n",
      "Epoch 00003: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 578us/sample - loss: 2.5870 - acc: 0.3700 - val_loss: 2.4831 - val_acc: 0.4137\n",
      "Epoch 4/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.4989 - acc: 0.3762\n",
      "Epoch 00004: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 578us/sample - loss: 2.4954 - acc: 0.3812 - val_loss: 2.4013 - val_acc: 0.4137\n",
      "Epoch 5/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.4060 - acc: 0.4069\n",
      "Epoch 00005: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 573us/sample - loss: 2.4062 - acc: 0.4072 - val_loss: 2.3381 - val_acc: 0.4350\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.3310 - acc: 0.3950\n",
      "Epoch 00006: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 577us/sample - loss: 2.3275 - acc: 0.3960 - val_loss: 2.2586 - val_acc: 0.4184\n",
      "Epoch 7/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.2454 - acc: 0.4222\n",
      "Epoch 00007: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 565us/sample - loss: 2.2409 - acc: 0.4232 - val_loss: 2.1892 - val_acc: 0.4303\n",
      "Epoch 8/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.1732 - acc: 0.4394\n",
      "Epoch 00008: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 557us/sample - loss: 2.1761 - acc: 0.4356 - val_loss: 2.1549 - val_acc: 0.3877\n",
      "Epoch 9/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.1054 - acc: 0.4541\n",
      "Epoch 00009: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 555us/sample - loss: 2.1073 - acc: 0.4498 - val_loss: 2.0653 - val_acc: 0.4492\n",
      "Epoch 10/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.0462 - acc: 0.4544\n",
      "Epoch 00010: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 2.0477 - acc: 0.4533 - val_loss: 2.0067 - val_acc: 0.4468\n",
      "Epoch 11/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.0018 - acc: 0.4560\n",
      "Epoch 00011: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 420us/sample - loss: 1.9930 - acc: 0.4616 - val_loss: 1.9512 - val_acc: 0.4468\n",
      "Epoch 12/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.9453 - acc: 0.4591\n",
      "Epoch 00012: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 404us/sample - loss: 1.9434 - acc: 0.4592 - val_loss: 1.9104 - val_acc: 0.4444\n",
      "Epoch 13/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.9105 - acc: 0.4613\n",
      "Epoch 00013: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 412us/sample - loss: 1.9046 - acc: 0.4651 - val_loss: 1.8849 - val_acc: 0.4326\n",
      "Epoch 14/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.8427 - acc: 0.4826\n",
      "Epoch 00014: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 404us/sample - loss: 1.8427 - acc: 0.4811 - val_loss: 1.8338 - val_acc: 0.4681\n",
      "Epoch 15/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.7849 - acc: 0.4963\n",
      "Epoch 00015: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 1.7939 - acc: 0.4923 - val_loss: 1.8000 - val_acc: 0.4799\n",
      "Epoch 16/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.7484 - acc: 0.5115\n",
      "Epoch 00016: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 1.7474 - acc: 0.5118 - val_loss: 1.7976 - val_acc: 0.4704\n",
      "Epoch 17/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.7089 - acc: 0.5070\n",
      "Epoch 00017: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 1.7084 - acc: 0.5095 - val_loss: 1.7780 - val_acc: 0.4657\n",
      "Epoch 18/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6851 - acc: 0.5104\n",
      "Epoch 00018: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 409us/sample - loss: 1.6846 - acc: 0.5106 - val_loss: 1.7253 - val_acc: 0.4704\n",
      "Epoch 19/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6459 - acc: 0.5355\n",
      "Epoch 00019: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 411us/sample - loss: 1.6439 - acc: 0.5361 - val_loss: 1.6734 - val_acc: 0.5059\n",
      "Epoch 20/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.6229 - acc: 0.5150\n",
      "Epoch 00020: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 1.6209 - acc: 0.5171 - val_loss: 1.6560 - val_acc: 0.4965\n",
      "Epoch 21/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5899 - acc: 0.5391\n",
      "Epoch 00021: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 408us/sample - loss: 1.5908 - acc: 0.5372 - val_loss: 1.7072 - val_acc: 0.4539\n",
      "Epoch 22/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.5317 - acc: 0.5553\n",
      "Epoch 00022: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 420us/sample - loss: 1.5412 - acc: 0.5467 - val_loss: 1.6513 - val_acc: 0.4728\n",
      "Epoch 23/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5275 - acc: 0.5568\n",
      "Epoch 00023: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 1.5247 - acc: 0.5561 - val_loss: 1.6123 - val_acc: 0.5130\n",
      "Epoch 24/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.5095 - acc: 0.5521\n",
      "Epoch 00024: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 413us/sample - loss: 1.5054 - acc: 0.5550 - val_loss: 1.6221 - val_acc: 0.4894\n",
      "Epoch 25/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4701 - acc: 0.5643\n",
      "Epoch 00025: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 397us/sample - loss: 1.4735 - acc: 0.5621 - val_loss: 1.5540 - val_acc: 0.5319\n",
      "Epoch 26/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4463 - acc: 0.5711\n",
      "Epoch 00026: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 402us/sample - loss: 1.4430 - acc: 0.5715 - val_loss: 1.5691 - val_acc: 0.5390\n",
      "Epoch 27/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.4453 - acc: 0.5703\n",
      "Epoch 00027: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 387us/sample - loss: 1.4436 - acc: 0.5703 - val_loss: 1.5307 - val_acc: 0.5272\n",
      "Epoch 28/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3935 - acc: 0.5956\n",
      "Epoch 00028: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 404us/sample - loss: 1.3943 - acc: 0.5940 - val_loss: 1.5110 - val_acc: 0.5225\n",
      "Epoch 29/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4189 - acc: 0.5760\n",
      "Epoch 00029: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 416us/sample - loss: 1.4132 - acc: 0.5798 - val_loss: 1.4620 - val_acc: 0.5674\n",
      "Epoch 30/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3895 - acc: 0.5803\n",
      "Epoch 00030: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 408us/sample - loss: 1.3883 - acc: 0.5786 - val_loss: 1.5003 - val_acc: 0.5154\n",
      "Epoch 31/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3637 - acc: 0.5950\n",
      "Epoch 00031: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 404us/sample - loss: 1.3661 - acc: 0.5910 - val_loss: 1.4157 - val_acc: 0.5721\n",
      "Epoch 32/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3374 - acc: 0.6094\n",
      "Epoch 00032: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 399us/sample - loss: 1.3362 - acc: 0.6105 - val_loss: 1.4236 - val_acc: 0.5792\n",
      "Epoch 33/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3281 - acc: 0.5968\n",
      "Epoch 00033: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 400us/sample - loss: 1.3257 - acc: 0.5975 - val_loss: 1.4383 - val_acc: 0.5485\n",
      "Epoch 34/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.3027 - acc: 0.6107\n",
      "Epoch 00034: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 415us/sample - loss: 1.3065 - acc: 0.6105 - val_loss: 1.4346 - val_acc: 0.5461\n",
      "Epoch 35/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3104 - acc: 0.6020\n",
      "Epoch 00035: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 398us/sample - loss: 1.3026 - acc: 0.6058 - val_loss: 1.3623 - val_acc: 0.5863\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2722 - acc: 0.6218\n",
      "Epoch 00036: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 406us/sample - loss: 1.2675 - acc: 0.6206 - val_loss: 1.4237 - val_acc: 0.5319\n",
      "Epoch 37/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2471 - acc: 0.6388\n",
      "Epoch 00037: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 408us/sample - loss: 1.2475 - acc: 0.6389 - val_loss: 1.3563 - val_acc: 0.5816\n",
      "Epoch 38/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2544 - acc: 0.6112\n",
      "Epoch 00038: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 410us/sample - loss: 1.2536 - acc: 0.6123 - val_loss: 1.3582 - val_acc: 0.5839\n",
      "Epoch 39/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2255 - acc: 0.6219\n",
      "Epoch 00039: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 1.2334 - acc: 0.6164 - val_loss: 1.3340 - val_acc: 0.5981\n",
      "Epoch 40/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.2448 - acc: 0.6152\n",
      "Epoch 00040: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 422us/sample - loss: 1.2399 - acc: 0.6200 - val_loss: 1.3486 - val_acc: 0.6123\n",
      "Epoch 41/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2074 - acc: 0.6298\n",
      "Epoch 00041: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 1.2039 - acc: 0.6318 - val_loss: 1.3347 - val_acc: 0.5863\n",
      "Epoch 42/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1986 - acc: 0.6562\n",
      "Epoch 00042: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 409us/sample - loss: 1.1976 - acc: 0.6560 - val_loss: 1.3150 - val_acc: 0.5792\n",
      "Epoch 43/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1958 - acc: 0.6281\n",
      "Epoch 00043: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 406us/sample - loss: 1.2035 - acc: 0.6283 - val_loss: 1.2955 - val_acc: 0.6123\n",
      "Epoch 44/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1859 - acc: 0.6456\n",
      "Epoch 00044: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 410us/sample - loss: 1.1835 - acc: 0.6466 - val_loss: 1.2915 - val_acc: 0.5887\n",
      "Epoch 45/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1816 - acc: 0.6352\n",
      "Epoch 00045: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 416us/sample - loss: 1.1846 - acc: 0.6330 - val_loss: 1.2861 - val_acc: 0.5863\n",
      "Epoch 46/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1613 - acc: 0.6488\n",
      "Epoch 00046: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 420us/sample - loss: 1.1724 - acc: 0.6418 - val_loss: 1.3137 - val_acc: 0.6052\n",
      "Epoch 47/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1454 - acc: 0.6494\n",
      "Epoch 00047: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 404us/sample - loss: 1.1491 - acc: 0.6483 - val_loss: 1.2961 - val_acc: 0.6241\n",
      "Epoch 48/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1472 - acc: 0.6518\n",
      "Epoch 00048: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 416us/sample - loss: 1.1438 - acc: 0.6525 - val_loss: 1.2694 - val_acc: 0.6194\n",
      "Epoch 49/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1421 - acc: 0.6581\n",
      "Epoch 00049: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 1.1366 - acc: 0.6608 - val_loss: 1.2733 - val_acc: 0.6336\n",
      "Epoch 50/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1023 - acc: 0.6808\n",
      "Epoch 00050: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 405us/sample - loss: 1.1072 - acc: 0.6785 - val_loss: 1.2779 - val_acc: 0.6241\n",
      "Epoch 51/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1257 - acc: 0.6562\n",
      "Epoch 00051: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 409us/sample - loss: 1.1267 - acc: 0.6560 - val_loss: 1.2575 - val_acc: 0.6288\n",
      "Epoch 52/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1096 - acc: 0.6741\n",
      "Epoch 00052: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 408us/sample - loss: 1.1115 - acc: 0.6720 - val_loss: 1.2389 - val_acc: 0.6288\n",
      "Epoch 53/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0827 - acc: 0.6820\n",
      "Epoch 00053: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 413us/sample - loss: 1.0823 - acc: 0.6809 - val_loss: 1.2976 - val_acc: 0.6265\n",
      "Epoch 54/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0903 - acc: 0.6750\n",
      "Epoch 00054: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 1.0895 - acc: 0.6761 - val_loss: 1.2881 - val_acc: 0.6005\n",
      "Epoch 55/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0952 - acc: 0.6538\n",
      "Epoch 00055: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 412us/sample - loss: 1.0925 - acc: 0.6560 - val_loss: 1.2539 - val_acc: 0.6430\n",
      "Epoch 56/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0655 - acc: 0.6863\n",
      "Epoch 00056: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 556us/sample - loss: 1.0634 - acc: 0.6862 - val_loss: 1.2708 - val_acc: 0.6194\n",
      "Epoch 57/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0599 - acc: 0.6875\n",
      "Epoch 00057: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 574us/sample - loss: 1.0628 - acc: 0.6856 - val_loss: 1.3087 - val_acc: 0.6170\n",
      "Epoch 58/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0451 - acc: 0.6857\n",
      "Epoch 00058: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 549us/sample - loss: 1.0446 - acc: 0.6838 - val_loss: 1.2537 - val_acc: 0.6454\n",
      "Epoch 59/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0374 - acc: 0.6754\n",
      "Epoch 00059: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 1.0324 - acc: 0.6809 - val_loss: 1.2329 - val_acc: 0.6454\n",
      "Epoch 60/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0338 - acc: 0.6913\n",
      "Epoch 00060: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 1.0375 - acc: 0.6909 - val_loss: 1.2369 - val_acc: 0.6596\n",
      "Epoch 61/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0278 - acc: 0.7013\n",
      "Epoch 00061: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 419us/sample - loss: 1.0268 - acc: 0.7015 - val_loss: 1.2351 - val_acc: 0.6478\n",
      "Epoch 62/100\n",
      "1504/1692 [=========================>....] - ETA: 0s - loss: 1.0466 - acc: 0.6902\n",
      "Epoch 00062: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 397us/sample - loss: 1.0310 - acc: 0.7004 - val_loss: 1.2322 - val_acc: 0.6336\n",
      "Epoch 63/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0049 - acc: 0.6998\n",
      "Epoch 00063: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 412us/sample - loss: 1.0064 - acc: 0.6992 - val_loss: 1.2239 - val_acc: 0.6454\n",
      "Epoch 64/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0032 - acc: 0.6938\n",
      "Epoch 00064: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 1.0074 - acc: 0.6933 - val_loss: 1.2145 - val_acc: 0.6619\n",
      "Epoch 65/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9871 - acc: 0.7034\n",
      "Epoch 00065: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 565us/sample - loss: 0.9813 - acc: 0.7086 - val_loss: 1.2341 - val_acc: 0.6525\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9821 - acc: 0.7114\n",
      "Epoch 00066: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 0.9792 - acc: 0.7122 - val_loss: 1.2470 - val_acc: 0.6501\n",
      "Epoch 67/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9720 - acc: 0.7175\n",
      "Epoch 00067: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 570us/sample - loss: 0.9786 - acc: 0.7157 - val_loss: 1.2692 - val_acc: 0.6383\n",
      "Epoch 68/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9695 - acc: 0.7206\n",
      "Epoch 00068: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 575us/sample - loss: 0.9686 - acc: 0.7210 - val_loss: 1.3085 - val_acc: 0.6241\n",
      "Epoch 69/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9665 - acc: 0.7145\n",
      "Epoch 00069: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 539us/sample - loss: 0.9698 - acc: 0.7104 - val_loss: 1.2648 - val_acc: 0.6407\n",
      "Epoch 70/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9548 - acc: 0.7181\n",
      "Epoch 00070: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 552us/sample - loss: 0.9569 - acc: 0.7175 - val_loss: 1.2125 - val_acc: 0.6785\n",
      "Epoch 71/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9564 - acc: 0.7175\n",
      "Epoch 00071: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 520us/sample - loss: 0.9559 - acc: 0.7163 - val_loss: 1.2364 - val_acc: 0.6501\n",
      "Epoch 72/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9634 - acc: 0.7175\n",
      "Epoch 00072: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 576us/sample - loss: 0.9611 - acc: 0.7169 - val_loss: 1.2402 - val_acc: 0.6572\n",
      "Epoch 73/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9242 - acc: 0.7344\n",
      "Epoch 00073: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 535us/sample - loss: 0.9262 - acc: 0.7335 - val_loss: 1.2300 - val_acc: 0.6478\n",
      "Epoch 74/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9210 - acc: 0.7362\n",
      "Epoch 00074: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 585us/sample - loss: 0.9210 - acc: 0.7340 - val_loss: 1.2246 - val_acc: 0.6596\n",
      "Epoch 75/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9235 - acc: 0.7341\n",
      "Epoch 00075: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 558us/sample - loss: 0.9225 - acc: 0.7335 - val_loss: 1.2543 - val_acc: 0.6667\n",
      "Epoch 76/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8987 - acc: 0.7443\n",
      "Epoch 00076: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 562us/sample - loss: 0.9128 - acc: 0.7400 - val_loss: 1.2656 - val_acc: 0.6501\n",
      "Epoch 77/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8980 - acc: 0.7500\n",
      "Epoch 00077: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 570us/sample - loss: 0.9065 - acc: 0.7441 - val_loss: 1.2910 - val_acc: 0.6478\n",
      "Epoch 78/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9280 - acc: 0.7275\n",
      "Epoch 00078: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 537us/sample - loss: 0.9266 - acc: 0.7270 - val_loss: 1.2322 - val_acc: 0.6738\n",
      "Epoch 79/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9070 - acc: 0.7380\n",
      "Epoch 00079: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 543us/sample - loss: 0.9053 - acc: 0.7394 - val_loss: 1.2458 - val_acc: 0.6501\n",
      "Epoch 80/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9189 - acc: 0.7292\n",
      "Epoch 00080: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 549us/sample - loss: 0.9173 - acc: 0.7311 - val_loss: 1.2576 - val_acc: 0.6714\n",
      "Epoch 81/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9096 - acc: 0.7400\n",
      "Epoch 00081: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 586us/sample - loss: 0.9093 - acc: 0.7411 - val_loss: 1.2645 - val_acc: 0.6714\n",
      "Epoch 82/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9076 - acc: 0.7381\n",
      "Epoch 00082: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 0.9041 - acc: 0.7370 - val_loss: 1.2617 - val_acc: 0.6690\n",
      "Epoch 83/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8810 - acc: 0.7506\n",
      "Epoch 00083: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 579us/sample - loss: 0.8759 - acc: 0.7541 - val_loss: 1.2947 - val_acc: 0.6478\n",
      "Epoch 84/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8920 - acc: 0.7518\n",
      "Epoch 00084: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 488us/sample - loss: 0.8889 - acc: 0.7524 - val_loss: 1.2513 - val_acc: 0.6596\n",
      "Epoch 85/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8908 - acc: 0.7420\n",
      "Epoch 00085: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 410us/sample - loss: 0.8925 - acc: 0.7447 - val_loss: 1.3185 - val_acc: 0.6478\n",
      "Epoch 86/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8582 - acc: 0.7506\n",
      "Epoch 00086: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 0.8546 - acc: 0.7524 - val_loss: 1.2714 - val_acc: 0.6832\n",
      "Epoch 87/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8706 - acc: 0.7589\n",
      "Epoch 00087: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 422us/sample - loss: 0.8672 - acc: 0.7606 - val_loss: 1.3239 - val_acc: 0.6548\n",
      "Epoch 88/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8711 - acc: 0.7500\n",
      "Epoch 00088: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 449us/sample - loss: 0.8777 - acc: 0.7470 - val_loss: 1.2487 - val_acc: 0.6714\n",
      "Epoch 89/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8436 - acc: 0.7719\n",
      "Epoch 00089: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 0.8522 - acc: 0.7677 - val_loss: 1.3237 - val_acc: 0.6478\n",
      "Epoch 90/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8677 - acc: 0.7451\n",
      "Epoch 00090: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 551us/sample - loss: 0.8619 - acc: 0.7470 - val_loss: 1.2884 - val_acc: 0.6619\n",
      "Epoch 91/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8770 - acc: 0.7551\n",
      "Epoch 00091: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 572us/sample - loss: 0.8715 - acc: 0.7583 - val_loss: 1.2336 - val_acc: 0.6856\n",
      "Epoch 92/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8386 - acc: 0.7692\n",
      "Epoch 00092: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 538us/sample - loss: 0.8431 - acc: 0.7677 - val_loss: 1.3663 - val_acc: 0.6454\n",
      "Epoch 93/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8465 - acc: 0.7620\n",
      "Epoch 00093: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 558us/sample - loss: 0.8450 - acc: 0.7630 - val_loss: 1.2627 - val_acc: 0.6643\n",
      "Epoch 94/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8382 - acc: 0.7696\n",
      "Epoch 00094: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 556us/sample - loss: 0.8396 - acc: 0.7677 - val_loss: 1.3092 - val_acc: 0.6596\n",
      "Epoch 95/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8242 - acc: 0.7763\n",
      "Epoch 00095: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 0.8282 - acc: 0.7754 - val_loss: 1.2844 - val_acc: 0.6596\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8334 - acc: 0.7659\n",
      "Epoch 00096: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 452us/sample - loss: 0.8302 - acc: 0.7660 - val_loss: 1.2805 - val_acc: 0.6738\n",
      "Epoch 97/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8253 - acc: 0.7806\n",
      "Epoch 00097: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 0.8280 - acc: 0.7796 - val_loss: 1.2788 - val_acc: 0.6596\n",
      "Epoch 98/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8310 - acc: 0.7532\n",
      "Epoch 00098: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 0.8284 - acc: 0.7547 - val_loss: 1.3302 - val_acc: 0.6501\n",
      "Epoch 99/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7873 - acc: 0.7812\n",
      "Epoch 00099: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 0.8003 - acc: 0.7778 - val_loss: 1.3064 - val_acc: 0.6738\n",
      "Epoch 100/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8029 - acc: 0.7758\n",
      "Epoch 00100: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 405us/sample - loss: 0.8019 - acc: 0.7766 - val_loss: 1.2759 - val_acc: 0.6832\n",
      "443/443 [==============================] - 0s 188us/sample - loss: 1.3102 - acc: 0.6343\n",
      "--- Starting trial: run-2\n",
      "{'TIME_WINDOW': 750, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 64, 'LAST_HIDDEN': 1000, 'LAST_DROPOUT': 0.2, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_88 (InputLayer)        [(None, 22, 750)]         0         \n",
      "_________________________________________________________________\n",
      "permute_87 (Permute)         (None, 750, 22)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 747, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_174 (Bat (None, 747, 32)           2988      \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_261 (Dropout)        (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_174 (MaxPoolin (None, 186, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 183, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_175 (Bat (None, 183, 64)           732       \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_262 (Dropout)        (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_175 (MaxPoolin (None, 45, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_261 (LSTM)              (None, 45, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_262 (LSTM)              (None, 45, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_263 (LSTM)              (None, 45, 64)            49408     \n",
      "_________________________________________________________________\n",
      "flatten_87 (Flatten)         (None, 2880)              0         \n",
      "_________________________________________________________________\n",
      "dense_174 (Dense)            (None, 1000)              2881000   \n",
      "_________________________________________________________________\n",
      "dropout_263 (Dropout)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            (None, 4)                 4004      \n",
      "=================================================================\n",
      "Total params: 3,179,636\n",
      "Trainable params: 3,177,776\n",
      "Non-trainable params: 1,860\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.5847 - acc: 0.2966\n",
      "Epoch 00001: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 5s 3ms/sample - loss: 3.5707 - acc: 0.2973 - val_loss: 3.3016 - val_acc: 0.3641\n",
      "Epoch 2/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.2519 - acc: 0.3712\n",
      "Epoch 00002: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 3.2450 - acc: 0.3723 - val_loss: 3.0794 - val_acc: 0.3995\n",
      "Epoch 3/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.0696 - acc: 0.3705\n",
      "Epoch 00003: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 3.0663 - acc: 0.3670 - val_loss: 2.9277 - val_acc: 0.4208\n",
      "Epoch 4/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.9129 - acc: 0.3878\n",
      "Epoch 00004: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 2.9067 - acc: 0.3883 - val_loss: 2.7841 - val_acc: 0.4255\n",
      "Epoch 5/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.7640 - acc: 0.4038\n",
      "Epoch 00005: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 2.7623 - acc: 0.4066 - val_loss: 2.6637 - val_acc: 0.4444\n",
      "Epoch 6/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.6309 - acc: 0.4216\n",
      "Epoch 00006: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 415us/sample - loss: 2.6279 - acc: 0.4214 - val_loss: 2.5382 - val_acc: 0.4350\n",
      "Epoch 7/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.5177 - acc: 0.4273\n",
      "Epoch 00007: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 437us/sample - loss: 2.5137 - acc: 0.4279 - val_loss: 2.4199 - val_acc: 0.4492\n",
      "Epoch 8/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.3927 - acc: 0.4549\n",
      "Epoch 00008: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 2.3913 - acc: 0.4557 - val_loss: 2.3187 - val_acc: 0.4539\n",
      "Epoch 9/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 2.2991 - acc: 0.4577\n",
      "Epoch 00009: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 2.2919 - acc: 0.4616 - val_loss: 2.2319 - val_acc: 0.4586\n",
      "Epoch 10/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 2.1907 - acc: 0.4805\n",
      "Epoch 00010: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 2.1906 - acc: 0.4781 - val_loss: 2.1608 - val_acc: 0.4279\n",
      "Epoch 11/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.0970 - acc: 0.4920\n",
      "Epoch 00011: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 2.0947 - acc: 0.4935 - val_loss: 2.0500 - val_acc: 0.4917\n",
      "Epoch 12/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.0209 - acc: 0.5119\n",
      "Epoch 00012: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 411us/sample - loss: 2.0206 - acc: 0.5136 - val_loss: 2.0287 - val_acc: 0.4634\n",
      "Epoch 13/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.9560 - acc: 0.4994\n",
      "Epoch 00013: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 1.9531 - acc: 0.5012 - val_loss: 1.9189 - val_acc: 0.4965\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.8814 - acc: 0.5125\n",
      "Epoch 00014: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 420us/sample - loss: 1.8825 - acc: 0.5077 - val_loss: 1.8705 - val_acc: 0.4657\n",
      "Epoch 15/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.8214 - acc: 0.5172\n",
      "Epoch 00015: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 1.8200 - acc: 0.5183 - val_loss: 1.8273 - val_acc: 0.4917\n",
      "Epoch 16/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.7529 - acc: 0.5351\n",
      "Epoch 00016: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 422us/sample - loss: 1.7540 - acc: 0.5361 - val_loss: 1.7899 - val_acc: 0.4917\n",
      "Epoch 17/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.7128 - acc: 0.5339\n",
      "Epoch 00017: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 1.7151 - acc: 0.5313 - val_loss: 1.6940 - val_acc: 0.5319\n",
      "Epoch 18/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.6670 - acc: 0.5459\n",
      "Epoch 00018: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 1.6713 - acc: 0.5396 - val_loss: 1.6810 - val_acc: 0.5106\n",
      "Epoch 19/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.6230 - acc: 0.5625\n",
      "Epoch 00019: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 1.6228 - acc: 0.5626 - val_loss: 1.6254 - val_acc: 0.5437\n",
      "Epoch 20/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5848 - acc: 0.5733\n",
      "Epoch 00020: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 433us/sample - loss: 1.5896 - acc: 0.5703 - val_loss: 1.5736 - val_acc: 0.5461\n",
      "Epoch 21/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5352 - acc: 0.5655\n",
      "Epoch 00021: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 433us/sample - loss: 1.5310 - acc: 0.5674 - val_loss: 1.5085 - val_acc: 0.5650\n",
      "Epoch 22/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.5038 - acc: 0.5778\n",
      "Epoch 00022: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 1.5047 - acc: 0.5768 - val_loss: 1.4852 - val_acc: 0.5674\n",
      "Epoch 23/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.4717 - acc: 0.5671\n",
      "Epoch 00023: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 416us/sample - loss: 1.4727 - acc: 0.5686 - val_loss: 1.4724 - val_acc: 0.5508\n",
      "Epoch 24/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4428 - acc: 0.5893\n",
      "Epoch 00024: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 1.4483 - acc: 0.5839 - val_loss: 1.4751 - val_acc: 0.5626\n",
      "Epoch 25/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4368 - acc: 0.5835\n",
      "Epoch 00025: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 1.4346 - acc: 0.5845 - val_loss: 1.4198 - val_acc: 0.5792\n",
      "Epoch 26/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3833 - acc: 0.6023\n",
      "Epoch 00026: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 1.3833 - acc: 0.6005 - val_loss: 1.3919 - val_acc: 0.5910\n",
      "Epoch 27/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3857 - acc: 0.5841\n",
      "Epoch 00027: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 1.3857 - acc: 0.5851 - val_loss: 1.4128 - val_acc: 0.5745\n",
      "Epoch 28/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3420 - acc: 0.6052\n",
      "Epoch 00028: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 1.3388 - acc: 0.6052 - val_loss: 1.3711 - val_acc: 0.6005\n",
      "Epoch 29/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3454 - acc: 0.5886\n",
      "Epoch 00029: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 1.3405 - acc: 0.5916 - val_loss: 1.3611 - val_acc: 0.5957\n",
      "Epoch 30/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.2942 - acc: 0.6178\n",
      "Epoch 00030: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 1.2967 - acc: 0.6141 - val_loss: 1.3683 - val_acc: 0.5626\n",
      "Epoch 31/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.2826 - acc: 0.6159\n",
      "Epoch 00031: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 435us/sample - loss: 1.2789 - acc: 0.6194 - val_loss: 1.3466 - val_acc: 0.5839\n",
      "Epoch 32/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2795 - acc: 0.6087\n",
      "Epoch 00032: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 1.2817 - acc: 0.6093 - val_loss: 1.3169 - val_acc: 0.6123\n",
      "Epoch 33/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2722 - acc: 0.6019\n",
      "Epoch 00033: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 1.2720 - acc: 0.6046 - val_loss: 1.2940 - val_acc: 0.5981\n",
      "Epoch 34/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2361 - acc: 0.6342\n",
      "Epoch 00034: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 413us/sample - loss: 1.2349 - acc: 0.6348 - val_loss: 1.2794 - val_acc: 0.6147\n",
      "Epoch 35/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2194 - acc: 0.6280\n",
      "Epoch 00035: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 1.2174 - acc: 0.6288 - val_loss: 1.2701 - val_acc: 0.6028\n",
      "Epoch 36/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2211 - acc: 0.6375\n",
      "Epoch 00036: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 1.2124 - acc: 0.6395 - val_loss: 1.3333 - val_acc: 0.5697\n",
      "Epoch 37/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1973 - acc: 0.6388\n",
      "Epoch 00037: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 1.1965 - acc: 0.6401 - val_loss: 1.2639 - val_acc: 0.6359\n",
      "Epoch 38/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1910 - acc: 0.6369\n",
      "Epoch 00038: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 415us/sample - loss: 1.1967 - acc: 0.6330 - val_loss: 1.2846 - val_acc: 0.6194\n",
      "Epoch 39/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1825 - acc: 0.6324\n",
      "Epoch 00039: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 1.1889 - acc: 0.6277 - val_loss: 1.2969 - val_acc: 0.6076\n",
      "Epoch 40/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1621 - acc: 0.6519\n",
      "Epoch 00040: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 1.1586 - acc: 0.6519 - val_loss: 1.2541 - val_acc: 0.6312\n",
      "Epoch 41/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1449 - acc: 0.6484\n",
      "Epoch 00041: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 1.1434 - acc: 0.6489 - val_loss: 1.2096 - val_acc: 0.6359\n",
      "Epoch 42/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1314 - acc: 0.6599\n",
      "Epoch 00042: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 436us/sample - loss: 1.1286 - acc: 0.6613 - val_loss: 1.1992 - val_acc: 0.6359\n",
      "Epoch 43/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1327 - acc: 0.6581\n",
      "Epoch 00043: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 1.1345 - acc: 0.6584 - val_loss: 1.2539 - val_acc: 0.6099\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1015 - acc: 0.6532\n",
      "Epoch 00044: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 419us/sample - loss: 1.0970 - acc: 0.6543 - val_loss: 1.2743 - val_acc: 0.6123\n",
      "Epoch 45/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1061 - acc: 0.6531\n",
      "Epoch 00045: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 1.1099 - acc: 0.6495 - val_loss: 1.1780 - val_acc: 0.6690\n",
      "Epoch 46/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0893 - acc: 0.6679\n",
      "Epoch 00046: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 415us/sample - loss: 1.0900 - acc: 0.6649 - val_loss: 1.2626 - val_acc: 0.6241\n",
      "Epoch 47/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1169 - acc: 0.6412\n",
      "Epoch 00047: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 433us/sample - loss: 1.1172 - acc: 0.6401 - val_loss: 1.2123 - val_acc: 0.6123\n",
      "Epoch 48/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0639 - acc: 0.6747\n",
      "Epoch 00048: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 1.0619 - acc: 0.6755 - val_loss: 1.2065 - val_acc: 0.6572\n",
      "Epoch 49/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0697 - acc: 0.6665\n",
      "Epoch 00049: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 1.0686 - acc: 0.6667 - val_loss: 1.2552 - val_acc: 0.6241\n",
      "Epoch 50/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0562 - acc: 0.6779\n",
      "Epoch 00050: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 416us/sample - loss: 1.0559 - acc: 0.6773 - val_loss: 1.2182 - val_acc: 0.6407\n",
      "Epoch 51/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.0344 - acc: 0.6816\n",
      "Epoch 00051: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 1.0370 - acc: 0.6820 - val_loss: 1.2195 - val_acc: 0.6407\n",
      "Epoch 52/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0458 - acc: 0.6800\n",
      "Epoch 00052: val_loss did not improve from 1.17312\n",
      "1692/1692 [==============================] - 1s 420us/sample - loss: 1.0508 - acc: 0.6761 - val_loss: 1.2233 - val_acc: 0.6336\n",
      "Epoch 53/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0539 - acc: 0.6875\n",
      "Epoch 00053: val_loss improved from 1.17312 to 1.15800, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 1.0546 - acc: 0.6874 - val_loss: 1.1580 - val_acc: 0.6596\n",
      "Epoch 54/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0214 - acc: 0.6971\n",
      "Epoch 00054: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 1.0296 - acc: 0.6915 - val_loss: 1.2169 - val_acc: 0.6501\n",
      "Epoch 55/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0229 - acc: 0.6881\n",
      "Epoch 00055: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 1.0239 - acc: 0.6891 - val_loss: 1.2736 - val_acc: 0.6312\n",
      "Epoch 56/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0081 - acc: 0.6979\n",
      "Epoch 00056: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 560us/sample - loss: 1.0071 - acc: 0.6986 - val_loss: 1.2266 - val_acc: 0.6430\n",
      "Epoch 57/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0063 - acc: 0.6911\n",
      "Epoch 00057: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 529us/sample - loss: 1.0081 - acc: 0.6903 - val_loss: 1.1904 - val_acc: 0.6548\n",
      "Epoch 58/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9909 - acc: 0.6994\n",
      "Epoch 00058: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 571us/sample - loss: 1.0003 - acc: 0.6956 - val_loss: 1.2791 - val_acc: 0.6217\n",
      "Epoch 59/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0152 - acc: 0.6900\n",
      "Epoch 00059: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 546us/sample - loss: 1.0160 - acc: 0.6903 - val_loss: 1.2567 - val_acc: 0.6454\n",
      "Epoch 60/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0009 - acc: 0.6900\n",
      "Epoch 00060: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 543us/sample - loss: 0.9979 - acc: 0.6903 - val_loss: 1.2843 - val_acc: 0.6359\n",
      "Epoch 61/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9914 - acc: 0.6959\n",
      "Epoch 00061: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 576us/sample - loss: 0.9928 - acc: 0.6962 - val_loss: 1.2850 - val_acc: 0.6076\n",
      "Epoch 62/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9844 - acc: 0.6983\n",
      "Epoch 00062: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 0.9876 - acc: 0.6962 - val_loss: 1.2535 - val_acc: 0.6194\n",
      "Epoch 63/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9767 - acc: 0.7006\n",
      "Epoch 00063: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 552us/sample - loss: 0.9757 - acc: 0.7033 - val_loss: 1.2505 - val_acc: 0.6241\n",
      "Epoch 64/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9821 - acc: 0.7028\n",
      "Epoch 00064: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 547us/sample - loss: 0.9808 - acc: 0.7033 - val_loss: 1.1828 - val_acc: 0.6548\n",
      "Epoch 65/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9676 - acc: 0.7127\n",
      "Epoch 00065: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 493us/sample - loss: 0.9653 - acc: 0.7128 - val_loss: 1.3120 - val_acc: 0.6170\n",
      "Epoch 66/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9487 - acc: 0.7175\n",
      "Epoch 00066: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 531us/sample - loss: 0.9498 - acc: 0.7187 - val_loss: 1.3152 - val_acc: 0.6170\n",
      "Epoch 67/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9394 - acc: 0.7181\n",
      "Epoch 00067: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 520us/sample - loss: 0.9496 - acc: 0.7128 - val_loss: 1.2170 - val_acc: 0.6478\n",
      "Epoch 68/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9404 - acc: 0.7132\n",
      "Epoch 00068: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 534us/sample - loss: 0.9499 - acc: 0.7098 - val_loss: 1.2355 - val_acc: 0.6478\n",
      "Epoch 69/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9216 - acc: 0.7169\n",
      "Epoch 00069: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 449us/sample - loss: 0.9250 - acc: 0.7145 - val_loss: 1.3140 - val_acc: 0.6170\n",
      "Epoch 70/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9421 - acc: 0.7119\n",
      "Epoch 00070: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 0.9453 - acc: 0.7110 - val_loss: 1.2076 - val_acc: 0.6619\n",
      "Epoch 71/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9290 - acc: 0.7105\n",
      "Epoch 00071: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 0.9329 - acc: 0.7080 - val_loss: 1.2367 - val_acc: 0.6501\n",
      "Epoch 72/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.9255 - acc: 0.7318\n",
      "Epoch 00072: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 431us/sample - loss: 0.9243 - acc: 0.7299 - val_loss: 1.1993 - val_acc: 0.6430\n",
      "Epoch 73/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9170 - acc: 0.7294\n",
      "Epoch 00073: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 0.9172 - acc: 0.7299 - val_loss: 1.2234 - val_acc: 0.6572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9202 - acc: 0.7200\n",
      "Epoch 00074: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 0.9176 - acc: 0.7222 - val_loss: 1.1764 - val_acc: 0.6619\n",
      "Epoch 75/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9209 - acc: 0.7207\n",
      "Epoch 00075: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 0.9135 - acc: 0.7228 - val_loss: 1.1747 - val_acc: 0.6478\n",
      "Epoch 76/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8999 - acc: 0.7394\n",
      "Epoch 00076: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 0.9122 - acc: 0.7323 - val_loss: 1.2675 - val_acc: 0.6572\n",
      "Epoch 77/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9082 - acc: 0.7325\n",
      "Epoch 00077: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 0.9053 - acc: 0.7346 - val_loss: 1.3816 - val_acc: 0.6099\n",
      "Epoch 78/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8866 - acc: 0.7344\n",
      "Epoch 00078: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 433us/sample - loss: 0.8871 - acc: 0.7329 - val_loss: 1.2669 - val_acc: 0.6430\n",
      "Epoch 79/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8757 - acc: 0.7422\n",
      "Epoch 00079: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 0.8823 - acc: 0.7388 - val_loss: 1.3100 - val_acc: 0.6478\n",
      "Epoch 80/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8824 - acc: 0.7371\n",
      "Epoch 00080: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 416us/sample - loss: 0.8821 - acc: 0.7346 - val_loss: 1.2634 - val_acc: 0.6478\n",
      "Epoch 81/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8653 - acc: 0.7428\n",
      "Epoch 00081: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 0.8720 - acc: 0.7376 - val_loss: 1.1913 - val_acc: 0.6643\n",
      "Epoch 82/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8758 - acc: 0.7374\n",
      "Epoch 00082: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 0.8774 - acc: 0.7370 - val_loss: 1.2525 - val_acc: 0.6525\n",
      "Epoch 83/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8840 - acc: 0.7415\n",
      "Epoch 00083: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 419us/sample - loss: 0.8808 - acc: 0.7400 - val_loss: 1.2078 - val_acc: 0.6785\n",
      "Epoch 84/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8638 - acc: 0.7500\n",
      "Epoch 00084: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 0.8636 - acc: 0.7506 - val_loss: 1.2530 - val_acc: 0.6643\n",
      "Epoch 85/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8659 - acc: 0.7398\n",
      "Epoch 00085: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 0.8599 - acc: 0.7423 - val_loss: 1.2529 - val_acc: 0.6430\n",
      "Epoch 86/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8657 - acc: 0.7400\n",
      "Epoch 00086: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 0.8618 - acc: 0.7417 - val_loss: 1.4148 - val_acc: 0.6123\n",
      "Epoch 87/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8778 - acc: 0.7387\n",
      "Epoch 00087: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 0.8809 - acc: 0.7382 - val_loss: 1.2318 - val_acc: 0.6572\n",
      "Epoch 88/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8202 - acc: 0.7615\n",
      "Epoch 00088: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 0.8302 - acc: 0.7553 - val_loss: 1.2827 - val_acc: 0.6454\n",
      "Epoch 89/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8534 - acc: 0.7436\n",
      "Epoch 00089: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 0.8526 - acc: 0.7465 - val_loss: 1.2214 - val_acc: 0.6714\n",
      "Epoch 90/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8273 - acc: 0.7588\n",
      "Epoch 00090: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 0.8303 - acc: 0.7571 - val_loss: 1.2428 - val_acc: 0.6572\n",
      "Epoch 91/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8034 - acc: 0.7812\n",
      "Epoch 00091: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 0.8138 - acc: 0.7754 - val_loss: 1.2871 - val_acc: 0.6454\n",
      "Epoch 92/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8585 - acc: 0.7398\n",
      "Epoch 00092: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 0.8604 - acc: 0.7382 - val_loss: 1.2633 - val_acc: 0.6596\n",
      "Epoch 93/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.7976 - acc: 0.7706\n",
      "Epoch 00093: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 0.7996 - acc: 0.7707 - val_loss: 1.2692 - val_acc: 0.6596\n",
      "Epoch 94/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8494 - acc: 0.7392\n",
      "Epoch 00094: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 0.8469 - acc: 0.7417 - val_loss: 1.3138 - val_acc: 0.6359\n",
      "Epoch 95/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.7995 - acc: 0.7650\n",
      "Epoch 00095: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 0.8106 - acc: 0.7583 - val_loss: 1.2336 - val_acc: 0.6832\n",
      "Epoch 96/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8548 - acc: 0.7530\n",
      "Epoch 00096: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 0.8534 - acc: 0.7530 - val_loss: 1.2861 - val_acc: 0.6407\n",
      "Epoch 97/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7972 - acc: 0.7742\n",
      "Epoch 00097: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 0.7989 - acc: 0.7736 - val_loss: 1.3264 - val_acc: 0.6596\n",
      "Epoch 98/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8336 - acc: 0.7619\n",
      "Epoch 00098: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 0.8300 - acc: 0.7648 - val_loss: 1.3805 - val_acc: 0.6478\n",
      "Epoch 99/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8034 - acc: 0.7647\n",
      "Epoch 00099: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 0.8005 - acc: 0.7648 - val_loss: 1.4408 - val_acc: 0.6312\n",
      "Epoch 100/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.7981 - acc: 0.7794\n",
      "Epoch 00100: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 422us/sample - loss: 0.7949 - acc: 0.7801 - val_loss: 1.3835 - val_acc: 0.6383\n",
      "443/443 [==============================] - 0s 190us/sample - loss: 1.4740 - acc: 0.5959\n",
      "--- Starting trial: run-3\n",
      "{'TIME_WINDOW': 750, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 64, 'LAST_HIDDEN': 1000, 'LAST_DROPOUT': 0.3, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_88\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_89 (InputLayer)        [(None, 22, 750)]         0         \n",
      "_________________________________________________________________\n",
      "permute_88 (Permute)         (None, 750, 22)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 747, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_176 (Bat (None, 747, 32)           2988      \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_264 (Dropout)        (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_176 (MaxPoolin (None, 186, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 183, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_177 (Bat (None, 183, 64)           732       \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_265 (Dropout)        (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_177 (MaxPoolin (None, 45, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_264 (LSTM)              (None, 45, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_265 (LSTM)              (None, 45, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_266 (LSTM)              (None, 45, 64)            49408     \n",
      "_________________________________________________________________\n",
      "flatten_88 (Flatten)         (None, 2880)              0         \n",
      "_________________________________________________________________\n",
      "dense_176 (Dense)            (None, 1000)              2881000   \n",
      "_________________________________________________________________\n",
      "dropout_266 (Dropout)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_177 (Dense)            (None, 4)                 4004      \n",
      "=================================================================\n",
      "Total params: 3,179,636\n",
      "Trainable params: 3,177,776\n",
      "Non-trainable params: 1,860\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536/1692 [==========================>...] - ETA: 0s - loss: 3.6642 - acc: 0.2754\n",
      "Epoch 00001: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 3.6405 - acc: 0.2807 - val_loss: 3.3443 - val_acc: 0.3499\n",
      "Epoch 2/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 3.2704 - acc: 0.3634\n",
      "Epoch 00002: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 3.2693 - acc: 0.3599 - val_loss: 3.0874 - val_acc: 0.4279\n",
      "Epoch 3/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 3.0690 - acc: 0.3958\n",
      "Epoch 00003: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 410us/sample - loss: 3.0657 - acc: 0.3930 - val_loss: 2.9389 - val_acc: 0.3924\n",
      "Epoch 4/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.9229 - acc: 0.4069\n",
      "Epoch 00004: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 2.9234 - acc: 0.4037 - val_loss: 2.7965 - val_acc: 0.4374\n",
      "Epoch 5/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.7709 - acc: 0.4279\n",
      "Epoch 00005: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 419us/sample - loss: 2.7674 - acc: 0.4279 - val_loss: 2.6705 - val_acc: 0.4208\n",
      "Epoch 6/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.6542 - acc: 0.4349\n",
      "Epoch 00006: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 415us/sample - loss: 2.6475 - acc: 0.4391 - val_loss: 2.5537 - val_acc: 0.4043\n",
      "Epoch 7/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.5227 - acc: 0.4534\n",
      "Epoch 00007: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 413us/sample - loss: 2.5194 - acc: 0.4563 - val_loss: 2.4520 - val_acc: 0.4279\n",
      "Epoch 8/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.3914 - acc: 0.4778\n",
      "Epoch 00008: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 2.3909 - acc: 0.4775 - val_loss: 2.3429 - val_acc: 0.4397\n",
      "Epoch 9/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.2901 - acc: 0.5006\n",
      "Epoch 00009: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 401us/sample - loss: 2.2918 - acc: 0.4988 - val_loss: 2.2774 - val_acc: 0.4326\n",
      "Epoch 10/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.2042 - acc: 0.5106\n",
      "Epoch 00010: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 412us/sample - loss: 2.2049 - acc: 0.5071 - val_loss: 2.1869 - val_acc: 0.4752\n",
      "Epoch 11/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.1376 - acc: 0.5031\n",
      "Epoch 00011: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 409us/sample - loss: 2.1352 - acc: 0.5053 - val_loss: 2.1255 - val_acc: 0.4657\n",
      "Epoch 12/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 2.0450 - acc: 0.5052\n",
      "Epoch 00012: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 2.0416 - acc: 0.5089 - val_loss: 2.0613 - val_acc: 0.4681\n",
      "Epoch 13/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.9759 - acc: 0.5288\n",
      "Epoch 00013: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 1.9754 - acc: 0.5290 - val_loss: 1.9927 - val_acc: 0.4634\n",
      "Epoch 14/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.9251 - acc: 0.5312\n",
      "Epoch 00014: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 420us/sample - loss: 1.9220 - acc: 0.5319 - val_loss: 1.9603 - val_acc: 0.4681\n",
      "Epoch 15/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.8634 - acc: 0.5339\n",
      "Epoch 00015: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 415us/sample - loss: 1.8573 - acc: 0.5343 - val_loss: 1.8476 - val_acc: 0.4988\n",
      "Epoch 16/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.8091 - acc: 0.5319\n",
      "Epoch 00016: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 1.8102 - acc: 0.5278 - val_loss: 1.8849 - val_acc: 0.4610\n",
      "Epoch 17/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.7565 - acc: 0.5450\n",
      "Epoch 00017: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 1.7600 - acc: 0.5437 - val_loss: 1.7470 - val_acc: 0.5390\n",
      "Epoch 18/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.6943 - acc: 0.5612\n",
      "Epoch 00018: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 1.6895 - acc: 0.5638 - val_loss: 1.7441 - val_acc: 0.4988\n",
      "Epoch 19/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.6831 - acc: 0.5553\n",
      "Epoch 00019: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 1.6792 - acc: 0.5544 - val_loss: 1.6944 - val_acc: 0.5201\n",
      "Epoch 20/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.6457 - acc: 0.5550\n",
      "Epoch 00020: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 422us/sample - loss: 1.6358 - acc: 0.5609 - val_loss: 1.6863 - val_acc: 0.5106\n",
      "Epoch 21/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5759 - acc: 0.5778\n",
      "Epoch 00021: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 1.5766 - acc: 0.5768 - val_loss: 1.6750 - val_acc: 0.5106\n",
      "Epoch 22/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.5400 - acc: 0.5778\n",
      "Epoch 00022: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 1.5392 - acc: 0.5792 - val_loss: 1.5805 - val_acc: 0.5414\n",
      "Epoch 23/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.5226 - acc: 0.5853\n",
      "Epoch 00023: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 1.5205 - acc: 0.5904 - val_loss: 1.6564 - val_acc: 0.4870\n",
      "Epoch 24/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4804 - acc: 0.5944\n",
      "Epoch 00024: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 1.4845 - acc: 0.5922 - val_loss: 1.5421 - val_acc: 0.5461\n",
      "Epoch 25/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4597 - acc: 0.5772\n",
      "Epoch 00025: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 422us/sample - loss: 1.4640 - acc: 0.5745 - val_loss: 1.5728 - val_acc: 0.5154\n",
      "Epoch 26/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4527 - acc: 0.5760\n",
      "Epoch 00026: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 1.4548 - acc: 0.5762 - val_loss: 1.4949 - val_acc: 0.5697\n",
      "Epoch 27/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3852 - acc: 0.6125\n",
      "Epoch 00027: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 1.3941 - acc: 0.6076 - val_loss: 1.4902 - val_acc: 0.5745\n",
      "Epoch 28/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3881 - acc: 0.5999\n",
      "Epoch 00028: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 1.3877 - acc: 0.5987 - val_loss: 1.4528 - val_acc: 0.5768\n",
      "Epoch 29/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3645 - acc: 0.6031\n",
      "Epoch 00029: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 1.3714 - acc: 0.6017 - val_loss: 1.5928 - val_acc: 0.5154\n",
      "Epoch 30/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3266 - acc: 0.6288\n",
      "Epoch 00030: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 1.3308 - acc: 0.6247 - val_loss: 1.4635 - val_acc: 0.5532\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3075 - acc: 0.6169\n",
      "Epoch 00031: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 415us/sample - loss: 1.3102 - acc: 0.6123 - val_loss: 1.4757 - val_acc: 0.5485\n",
      "Epoch 32/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.3229 - acc: 0.6159\n",
      "Epoch 00032: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 1.3291 - acc: 0.6099 - val_loss: 1.4731 - val_acc: 0.5343\n",
      "Epoch 33/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2814 - acc: 0.6194\n",
      "Epoch 00033: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 411us/sample - loss: 1.2807 - acc: 0.6200 - val_loss: 1.4878 - val_acc: 0.5343\n",
      "Epoch 34/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2822 - acc: 0.6409\n",
      "Epoch 00034: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 408us/sample - loss: 1.2880 - acc: 0.6359 - val_loss: 1.4430 - val_acc: 0.5556\n",
      "Epoch 35/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2751 - acc: 0.6218\n",
      "Epoch 00035: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 1.2712 - acc: 0.6247 - val_loss: 1.3893 - val_acc: 0.5768\n",
      "Epoch 36/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.2341 - acc: 0.6354\n",
      "Epoch 00036: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 1.2433 - acc: 0.6318 - val_loss: 1.4518 - val_acc: 0.5603\n",
      "Epoch 37/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.2262 - acc: 0.6452\n",
      "Epoch 00037: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 1.2274 - acc: 0.6418 - val_loss: 1.3793 - val_acc: 0.5981\n",
      "Epoch 38/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1961 - acc: 0.6562\n",
      "Epoch 00038: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 1.2032 - acc: 0.6501 - val_loss: 1.4163 - val_acc: 0.5603\n",
      "Epoch 39/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2086 - acc: 0.6327\n",
      "Epoch 00039: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 1.2083 - acc: 0.6359 - val_loss: 1.3697 - val_acc: 0.5792\n",
      "Epoch 40/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2015 - acc: 0.6513\n",
      "Epoch 00040: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 409us/sample - loss: 1.1994 - acc: 0.6501 - val_loss: 1.3559 - val_acc: 0.6028\n",
      "Epoch 41/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2162 - acc: 0.6397\n",
      "Epoch 00041: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 1.2049 - acc: 0.6454 - val_loss: 1.3599 - val_acc: 0.5816\n",
      "Epoch 42/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1668 - acc: 0.6537\n",
      "Epoch 00042: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 410us/sample - loss: 1.1667 - acc: 0.6566 - val_loss: 1.3379 - val_acc: 0.6028\n",
      "Epoch 43/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1680 - acc: 0.6467\n",
      "Epoch 00043: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 1.1628 - acc: 0.6483 - val_loss: 1.4250 - val_acc: 0.5839\n",
      "Epoch 44/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.1772 - acc: 0.6276\n",
      "Epoch 00044: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 1.1688 - acc: 0.6353 - val_loss: 1.3495 - val_acc: 0.5934\n",
      "Epoch 45/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.1391 - acc: 0.6465\n",
      "Epoch 00045: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 1.1439 - acc: 0.6489 - val_loss: 1.3165 - val_acc: 0.6028\n",
      "Epoch 46/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1548 - acc: 0.6662\n",
      "Epoch 00046: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 1.1524 - acc: 0.6631 - val_loss: 1.3158 - val_acc: 0.6028\n",
      "Epoch 47/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1215 - acc: 0.6697\n",
      "Epoch 00047: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 413us/sample - loss: 1.1207 - acc: 0.6678 - val_loss: 1.3420 - val_acc: 0.5934\n",
      "Epoch 48/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1150 - acc: 0.6544\n",
      "Epoch 00048: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 1.1184 - acc: 0.6548 - val_loss: 1.3156 - val_acc: 0.6028\n",
      "Epoch 49/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1040 - acc: 0.6812\n",
      "Epoch 00049: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 420us/sample - loss: 1.1066 - acc: 0.6826 - val_loss: 1.2884 - val_acc: 0.6241\n",
      "Epoch 50/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1097 - acc: 0.6538\n",
      "Epoch 00050: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 422us/sample - loss: 1.1092 - acc: 0.6543 - val_loss: 1.3048 - val_acc: 0.6076\n",
      "Epoch 51/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0705 - acc: 0.6850\n",
      "Epoch 00051: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 1.0733 - acc: 0.6814 - val_loss: 1.2559 - val_acc: 0.6430\n",
      "Epoch 52/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0526 - acc: 0.6938\n",
      "Epoch 00052: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 406us/sample - loss: 1.0536 - acc: 0.6915 - val_loss: 1.4244 - val_acc: 0.5650\n",
      "Epoch 53/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0684 - acc: 0.6783\n",
      "Epoch 00053: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 416us/sample - loss: 1.0667 - acc: 0.6779 - val_loss: 1.2338 - val_acc: 0.6548\n",
      "Epoch 54/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0519 - acc: 0.6887\n",
      "Epoch 00054: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 1.0520 - acc: 0.6874 - val_loss: 1.3393 - val_acc: 0.6028\n",
      "Epoch 55/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0891 - acc: 0.6719\n",
      "Epoch 00055: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 1.0850 - acc: 0.6732 - val_loss: 1.2972 - val_acc: 0.6076\n",
      "Epoch 56/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0190 - acc: 0.7067\n",
      "Epoch 00056: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 1.0191 - acc: 0.7069 - val_loss: 1.3491 - val_acc: 0.6052\n",
      "Epoch 57/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0271 - acc: 0.7106\n",
      "Epoch 00057: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 419us/sample - loss: 1.0205 - acc: 0.7122 - val_loss: 1.2891 - val_acc: 0.6336\n",
      "Epoch 58/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0283 - acc: 0.6918\n",
      "Epoch 00058: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 1.0286 - acc: 0.6921 - val_loss: 1.2592 - val_acc: 0.6619\n",
      "Epoch 59/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0238 - acc: 0.6938\n",
      "Epoch 00059: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 1.0212 - acc: 0.6944 - val_loss: 1.2756 - val_acc: 0.6430\n",
      "Epoch 60/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0069 - acc: 0.6988\n",
      "Epoch 00060: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 1.0070 - acc: 0.6992 - val_loss: 1.3453 - val_acc: 0.6123\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9728 - acc: 0.7194\n",
      "Epoch 00061: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 0.9853 - acc: 0.7134 - val_loss: 1.4119 - val_acc: 0.5934\n",
      "Epoch 62/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.0033 - acc: 0.7018\n",
      "Epoch 00062: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 422us/sample - loss: 0.9955 - acc: 0.7039 - val_loss: 1.3023 - val_acc: 0.6478\n",
      "Epoch 63/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9818 - acc: 0.7212\n",
      "Epoch 00063: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 411us/sample - loss: 0.9840 - acc: 0.7222 - val_loss: 1.2949 - val_acc: 0.6241\n",
      "Epoch 64/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.9778 - acc: 0.7161\n",
      "Epoch 00064: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 0.9785 - acc: 0.7169 - val_loss: 1.2792 - val_acc: 0.6407\n",
      "Epoch 65/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9642 - acc: 0.7073\n",
      "Epoch 00065: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 0.9646 - acc: 0.7080 - val_loss: 1.3358 - val_acc: 0.6194\n",
      "Epoch 66/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9984 - acc: 0.6975\n",
      "Epoch 00066: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 1.0030 - acc: 0.6956 - val_loss: 1.2517 - val_acc: 0.6454\n",
      "Epoch 67/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9559 - acc: 0.7300\n",
      "Epoch 00067: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 415us/sample - loss: 0.9576 - acc: 0.7270 - val_loss: 1.2549 - val_acc: 0.6478\n",
      "Epoch 68/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9749 - acc: 0.7094\n",
      "Epoch 00068: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 0.9860 - acc: 0.7045 - val_loss: 1.2996 - val_acc: 0.6241\n",
      "Epoch 69/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9415 - acc: 0.7335\n",
      "Epoch 00069: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 406us/sample - loss: 0.9416 - acc: 0.7323 - val_loss: 1.3898 - val_acc: 0.6312\n",
      "Epoch 70/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9327 - acc: 0.7366\n",
      "Epoch 00070: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 0.9346 - acc: 0.7370 - val_loss: 1.3409 - val_acc: 0.6454\n",
      "Epoch 71/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9168 - acc: 0.7341\n",
      "Epoch 00071: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 0.9195 - acc: 0.7317 - val_loss: 1.2965 - val_acc: 0.6336\n",
      "Epoch 72/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9472 - acc: 0.7224\n",
      "Epoch 00072: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 419us/sample - loss: 0.9418 - acc: 0.7258 - val_loss: 1.3018 - val_acc: 0.6478\n",
      "Epoch 73/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.9230 - acc: 0.7350\n",
      "Epoch 00073: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 431us/sample - loss: 0.9203 - acc: 0.7346 - val_loss: 1.2884 - val_acc: 0.6430\n",
      "Epoch 74/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9358 - acc: 0.7206\n",
      "Epoch 00074: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 0.9329 - acc: 0.7216 - val_loss: 1.3059 - val_acc: 0.6407\n",
      "Epoch 75/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9225 - acc: 0.7374\n",
      "Epoch 00075: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 0.9202 - acc: 0.7382 - val_loss: 1.3741 - val_acc: 0.6288\n",
      "Epoch 76/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9017 - acc: 0.7360\n",
      "Epoch 00076: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 0.9077 - acc: 0.7340 - val_loss: 1.3715 - val_acc: 0.6359\n",
      "Epoch 77/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9168 - acc: 0.7360\n",
      "Epoch 00077: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 410us/sample - loss: 0.9191 - acc: 0.7335 - val_loss: 1.3343 - val_acc: 0.6336\n",
      "Epoch 78/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8962 - acc: 0.7430\n",
      "Epoch 00078: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 0.8960 - acc: 0.7429 - val_loss: 1.2899 - val_acc: 0.6525\n",
      "Epoch 79/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9036 - acc: 0.7463\n",
      "Epoch 00079: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 0.9047 - acc: 0.7470 - val_loss: 1.3495 - val_acc: 0.6407\n",
      "Epoch 80/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8762 - acc: 0.7420\n",
      "Epoch 00080: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 415us/sample - loss: 0.8783 - acc: 0.7411 - val_loss: 1.2989 - val_acc: 0.6430\n",
      "Epoch 81/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8963 - acc: 0.7412\n",
      "Epoch 00081: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 405us/sample - loss: 0.8959 - acc: 0.7423 - val_loss: 1.3515 - val_acc: 0.6525\n",
      "Epoch 82/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8401 - acc: 0.7669\n",
      "Epoch 00082: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 0.8455 - acc: 0.7618 - val_loss: 1.3874 - val_acc: 0.6359\n",
      "Epoch 83/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8810 - acc: 0.7538\n",
      "Epoch 00083: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 0.8796 - acc: 0.7524 - val_loss: 1.3847 - val_acc: 0.6548\n",
      "Epoch 84/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8378 - acc: 0.7628\n",
      "Epoch 00084: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 0.8446 - acc: 0.7612 - val_loss: 1.3862 - val_acc: 0.6478\n",
      "Epoch 85/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8721 - acc: 0.7570\n",
      "Epoch 00085: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 0.8813 - acc: 0.7577 - val_loss: 1.4072 - val_acc: 0.6501\n",
      "Epoch 86/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8355 - acc: 0.7708\n",
      "Epoch 00086: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 422us/sample - loss: 0.8370 - acc: 0.7701 - val_loss: 1.3285 - val_acc: 0.6525\n",
      "Epoch 87/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8706 - acc: 0.7537\n",
      "Epoch 00087: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 397us/sample - loss: 0.8741 - acc: 0.7530 - val_loss: 1.3607 - val_acc: 0.6430\n",
      "Epoch 88/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8432 - acc: 0.7575\n",
      "Epoch 00088: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 0.8432 - acc: 0.7577 - val_loss: 1.3780 - val_acc: 0.6478\n",
      "Epoch 89/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8456 - acc: 0.7725\n",
      "Epoch 00089: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 407us/sample - loss: 0.8483 - acc: 0.7701 - val_loss: 1.3876 - val_acc: 0.6525\n",
      "Epoch 90/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8326 - acc: 0.7787\n",
      "Epoch 00090: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 409us/sample - loss: 0.8273 - acc: 0.7772 - val_loss: 1.4129 - val_acc: 0.6525\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8143 - acc: 0.7727\n",
      "Epoch 00091: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 0.8165 - acc: 0.7707 - val_loss: 1.4829 - val_acc: 0.6288\n",
      "Epoch 92/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8329 - acc: 0.7684\n",
      "Epoch 00092: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 415us/sample - loss: 0.8351 - acc: 0.7677 - val_loss: 1.4025 - val_acc: 0.6454\n",
      "Epoch 93/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8415 - acc: 0.7721\n",
      "Epoch 00093: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 0.8410 - acc: 0.7725 - val_loss: 1.4176 - val_acc: 0.6383\n",
      "Epoch 94/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8280 - acc: 0.7747\n",
      "Epoch 00094: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 0.8370 - acc: 0.7689 - val_loss: 1.4356 - val_acc: 0.6312\n",
      "Epoch 95/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8263 - acc: 0.7714\n",
      "Epoch 00095: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 407us/sample - loss: 0.8254 - acc: 0.7713 - val_loss: 1.4045 - val_acc: 0.6407\n",
      "Epoch 96/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8322 - acc: 0.7526\n",
      "Epoch 00096: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 0.8263 - acc: 0.7559 - val_loss: 1.4299 - val_acc: 0.6501\n",
      "Epoch 97/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8391 - acc: 0.7669\n",
      "Epoch 00097: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 410us/sample - loss: 0.8324 - acc: 0.7701 - val_loss: 1.3933 - val_acc: 0.6714\n",
      "Epoch 98/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7996 - acc: 0.7838\n",
      "Epoch 00098: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 0.8041 - acc: 0.7813 - val_loss: 1.4592 - val_acc: 0.6501\n",
      "Epoch 99/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8043 - acc: 0.7781\n",
      "Epoch 00099: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 0.7924 - acc: 0.7831 - val_loss: 1.4445 - val_acc: 0.6383\n",
      "Epoch 100/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8145 - acc: 0.7606\n",
      "Epoch 00100: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 413us/sample - loss: 0.8185 - acc: 0.7583 - val_loss: 1.3596 - val_acc: 0.6478\n",
      "443/443 [==============================] - 0s 173us/sample - loss: 1.3472 - acc: 0.6411\n",
      "--- Starting trial: run-4\n",
      "{'TIME_WINDOW': 750, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 128, 'LAST_HIDDEN': 500, 'LAST_DROPOUT': 0.2, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_90 (InputLayer)        [(None, 22, 750)]         0         \n",
      "_________________________________________________________________\n",
      "permute_89 (Permute)         (None, 750, 22)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 747, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_178 (Bat (None, 747, 32)           2988      \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_267 (Dropout)        (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_178 (MaxPoolin (None, 186, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 183, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_179 (Bat (None, 183, 64)           732       \n",
      "_________________________________________________________________\n",
      "activation_179 (Activation)  (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_268 (Dropout)        (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_179 (MaxPoolin (None, 45, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_267 (LSTM)              (None, 45, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_268 (LSTM)              (None, 45, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_269 (LSTM)              (None, 45, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_89 (Flatten)         (None, 5760)              0         \n",
      "_________________________________________________________________\n",
      "dense_178 (Dense)            (None, 500)               2880500   \n",
      "_________________________________________________________________\n",
      "dropout_269 (Dropout)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_179 (Dense)            (None, 4)                 2004      \n",
      "=================================================================\n",
      "Total params: 3,259,312\n",
      "Trainable params: 3,257,452\n",
      "Non-trainable params: 1,860\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.0582 - acc: 0.2946\n",
      "Epoch 00001: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 3.0456 - acc: 0.2931 - val_loss: 2.8007 - val_acc: 0.3853\n",
      "Epoch 2/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.7731 - acc: 0.3520\n",
      "Epoch 00002: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 2.7643 - acc: 0.3558 - val_loss: 2.6105 - val_acc: 0.3830\n",
      "Epoch 3/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.5993 - acc: 0.4099\n",
      "Epoch 00003: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 2.5973 - acc: 0.4078 - val_loss: 2.4707 - val_acc: 0.4161\n",
      "Epoch 4/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.4748 - acc: 0.4105\n",
      "Epoch 00004: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 2.4736 - acc: 0.4131 - val_loss: 2.3598 - val_acc: 0.4397\n",
      "Epoch 5/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 2.3650 - acc: 0.4049\n",
      "Epoch 00005: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 2.3564 - acc: 0.4102 - val_loss: 2.2478 - val_acc: 0.4515\n",
      "Epoch 6/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 2.2447 - acc: 0.4538\n",
      "Epoch 00006: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 2.2392 - acc: 0.4574 - val_loss: 2.1840 - val_acc: 0.4043\n",
      "Epoch 7/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.1590 - acc: 0.4598\n",
      "Epoch 00007: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 2.1546 - acc: 0.4586 - val_loss: 2.0892 - val_acc: 0.4255\n",
      "Epoch 8/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.0677 - acc: 0.4792\n",
      "Epoch 00008: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 2.0641 - acc: 0.4799 - val_loss: 2.0112 - val_acc: 0.4515\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.9810 - acc: 0.5013\n",
      "Epoch 00009: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 1.9828 - acc: 0.4988 - val_loss: 1.9548 - val_acc: 0.4681\n",
      "Epoch 10/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.8998 - acc: 0.5070\n",
      "Epoch 00010: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 1.9052 - acc: 0.5018 - val_loss: 1.9370 - val_acc: 0.4303\n",
      "Epoch 11/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.8348 - acc: 0.5198\n",
      "Epoch 00011: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 1.8319 - acc: 0.5201 - val_loss: 1.8279 - val_acc: 0.4917\n",
      "Epoch 12/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.7845 - acc: 0.5236\n",
      "Epoch 00012: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 1.7825 - acc: 0.5242 - val_loss: 1.7401 - val_acc: 0.5272\n",
      "Epoch 13/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.7298 - acc: 0.5415\n",
      "Epoch 00013: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 1.7274 - acc: 0.5420 - val_loss: 1.7244 - val_acc: 0.4988\n",
      "Epoch 14/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.6831 - acc: 0.5375\n",
      "Epoch 00014: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 1.6840 - acc: 0.5349 - val_loss: 1.6914 - val_acc: 0.5130\n",
      "Epoch 15/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.6167 - acc: 0.5491\n",
      "Epoch 00015: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 1.6303 - acc: 0.5414 - val_loss: 1.7044 - val_acc: 0.4586\n",
      "Epoch 16/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.6067 - acc: 0.5537\n",
      "Epoch 00016: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 411us/sample - loss: 1.6044 - acc: 0.5544 - val_loss: 1.6446 - val_acc: 0.5130\n",
      "Epoch 17/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.5561 - acc: 0.5587\n",
      "Epoch 00017: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 1.5582 - acc: 0.5579 - val_loss: 1.6323 - val_acc: 0.5106\n",
      "Epoch 18/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.5342 - acc: 0.5674\n",
      "Epoch 00018: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 1.5437 - acc: 0.5632 - val_loss: 1.6396 - val_acc: 0.4988\n",
      "Epoch 19/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.4781 - acc: 0.5869\n",
      "Epoch 00019: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 1.4844 - acc: 0.5845 - val_loss: 1.5110 - val_acc: 0.5508\n",
      "Epoch 20/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4631 - acc: 0.5680\n",
      "Epoch 00020: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 1.4672 - acc: 0.5686 - val_loss: 1.4547 - val_acc: 0.5910\n",
      "Epoch 21/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4503 - acc: 0.5746\n",
      "Epoch 00021: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 1.4514 - acc: 0.5762 - val_loss: 1.4614 - val_acc: 0.5792\n",
      "Epoch 22/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4103 - acc: 0.5877\n",
      "Epoch 00022: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 1.4091 - acc: 0.5875 - val_loss: 1.4908 - val_acc: 0.5768\n",
      "Epoch 23/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.4064 - acc: 0.5750\n",
      "Epoch 00023: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 422us/sample - loss: 1.4045 - acc: 0.5768 - val_loss: 1.4442 - val_acc: 0.5508\n",
      "Epoch 24/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.3788 - acc: 0.5840\n",
      "Epoch 00024: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 1.3709 - acc: 0.5869 - val_loss: 1.4316 - val_acc: 0.5650\n",
      "Epoch 25/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.3318 - acc: 0.6074\n",
      "Epoch 00025: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 1.3224 - acc: 0.6135 - val_loss: 1.4127 - val_acc: 0.5674\n",
      "Epoch 26/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3192 - acc: 0.6100\n",
      "Epoch 00026: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 416us/sample - loss: 1.3178 - acc: 0.6111 - val_loss: 1.3816 - val_acc: 0.5721\n",
      "Epoch 27/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3005 - acc: 0.6193\n",
      "Epoch 00027: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 420us/sample - loss: 1.3031 - acc: 0.6123 - val_loss: 1.3942 - val_acc: 0.5957\n",
      "Epoch 28/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2948 - acc: 0.6121\n",
      "Epoch 00028: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 1.2993 - acc: 0.6087 - val_loss: 1.3447 - val_acc: 0.6241\n",
      "Epoch 29/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2892 - acc: 0.6142\n",
      "Epoch 00029: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 422us/sample - loss: 1.2888 - acc: 0.6147 - val_loss: 1.3292 - val_acc: 0.6288\n",
      "Epoch 30/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2793 - acc: 0.6137\n",
      "Epoch 00030: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 1.2770 - acc: 0.6141 - val_loss: 1.3192 - val_acc: 0.6241\n",
      "Epoch 31/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2547 - acc: 0.6129\n",
      "Epoch 00031: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 422us/sample - loss: 1.2538 - acc: 0.6141 - val_loss: 1.3019 - val_acc: 0.5981\n",
      "Epoch 32/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2573 - acc: 0.6225\n",
      "Epoch 00032: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 1.2569 - acc: 0.6206 - val_loss: 1.3132 - val_acc: 0.6147\n",
      "Epoch 33/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2089 - acc: 0.6275\n",
      "Epoch 00033: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 1.2120 - acc: 0.6253 - val_loss: 1.3092 - val_acc: 0.6052\n",
      "Epoch 34/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2155 - acc: 0.6352\n",
      "Epoch 00034: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 435us/sample - loss: 1.2127 - acc: 0.6353 - val_loss: 1.2978 - val_acc: 0.6076\n",
      "Epoch 35/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.1813 - acc: 0.6497\n",
      "Epoch 00035: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 1.1900 - acc: 0.6424 - val_loss: 1.3098 - val_acc: 0.6028\n",
      "Epoch 36/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1765 - acc: 0.6422\n",
      "Epoch 00036: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 435us/sample - loss: 1.1787 - acc: 0.6401 - val_loss: 1.2908 - val_acc: 0.6099\n",
      "Epoch 37/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1605 - acc: 0.6499\n",
      "Epoch 00037: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 1.1632 - acc: 0.6507 - val_loss: 1.2806 - val_acc: 0.6194\n",
      "Epoch 38/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1610 - acc: 0.6550\n",
      "Epoch 00038: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 1.1606 - acc: 0.6543 - val_loss: 1.2679 - val_acc: 0.6076\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1495 - acc: 0.6419\n",
      "Epoch 00039: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 419us/sample - loss: 1.1530 - acc: 0.6383 - val_loss: 1.2756 - val_acc: 0.6194\n",
      "Epoch 40/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1521 - acc: 0.6320\n",
      "Epoch 00040: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 420us/sample - loss: 1.1507 - acc: 0.6348 - val_loss: 1.2679 - val_acc: 0.6217\n",
      "Epoch 41/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1134 - acc: 0.6626\n",
      "Epoch 00041: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 1.1186 - acc: 0.6613 - val_loss: 1.2586 - val_acc: 0.6217\n",
      "Epoch 42/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1271 - acc: 0.6532\n",
      "Epoch 00042: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 1.1292 - acc: 0.6519 - val_loss: 1.2778 - val_acc: 0.6194\n",
      "Epoch 43/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0983 - acc: 0.6709\n",
      "Epoch 00043: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 1.0994 - acc: 0.6661 - val_loss: 1.2502 - val_acc: 0.6312\n",
      "Epoch 44/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0909 - acc: 0.6716\n",
      "Epoch 00044: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 1.0909 - acc: 0.6708 - val_loss: 1.2489 - val_acc: 0.6407\n",
      "Epoch 45/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.0986 - acc: 0.6615\n",
      "Epoch 00045: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 1.0980 - acc: 0.6625 - val_loss: 1.2676 - val_acc: 0.6288\n",
      "Epoch 46/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0571 - acc: 0.6673\n",
      "Epoch 00046: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 422us/sample - loss: 1.0635 - acc: 0.6655 - val_loss: 1.2440 - val_acc: 0.6312\n",
      "Epoch 47/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.0648 - acc: 0.6673\n",
      "Epoch 00047: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 438us/sample - loss: 1.0694 - acc: 0.6673 - val_loss: 1.2849 - val_acc: 0.6265\n",
      "Epoch 48/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0907 - acc: 0.6658\n",
      "Epoch 00048: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 1.0831 - acc: 0.6667 - val_loss: 1.2458 - val_acc: 0.6407\n",
      "Epoch 49/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0639 - acc: 0.6827\n",
      "Epoch 00049: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 431us/sample - loss: 1.0632 - acc: 0.6826 - val_loss: 1.2358 - val_acc: 0.6312\n",
      "Epoch 50/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0616 - acc: 0.6744\n",
      "Epoch 00050: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 436us/sample - loss: 1.0660 - acc: 0.6732 - val_loss: 1.2511 - val_acc: 0.6005\n",
      "Epoch 51/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0581 - acc: 0.6756\n",
      "Epoch 00051: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 1.0499 - acc: 0.6803 - val_loss: 1.2993 - val_acc: 0.6241\n",
      "Epoch 52/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0278 - acc: 0.6869\n",
      "Epoch 00052: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 1.0274 - acc: 0.6868 - val_loss: 1.2995 - val_acc: 0.6147\n",
      "Epoch 53/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0671 - acc: 0.6677\n",
      "Epoch 00053: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 435us/sample - loss: 1.0638 - acc: 0.6678 - val_loss: 1.2544 - val_acc: 0.6241\n",
      "Epoch 54/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0224 - acc: 0.6947\n",
      "Epoch 00054: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 433us/sample - loss: 1.0283 - acc: 0.6921 - val_loss: 1.2516 - val_acc: 0.6099\n",
      "Epoch 55/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.0060 - acc: 0.6927\n",
      "Epoch 00055: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 1.0153 - acc: 0.6891 - val_loss: 1.2648 - val_acc: 0.6076\n",
      "Epoch 56/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0115 - acc: 0.6959\n",
      "Epoch 00056: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 435us/sample - loss: 1.0137 - acc: 0.6950 - val_loss: 1.2801 - val_acc: 0.6430\n",
      "Epoch 57/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0216 - acc: 0.6801\n",
      "Epoch 00057: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 433us/sample - loss: 1.0240 - acc: 0.6785 - val_loss: 1.3099 - val_acc: 0.6217\n",
      "Epoch 58/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9862 - acc: 0.7111\n",
      "Epoch 00058: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 420us/sample - loss: 0.9846 - acc: 0.7116 - val_loss: 1.2793 - val_acc: 0.6265\n",
      "Epoch 59/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9816 - acc: 0.7226\n",
      "Epoch 00059: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 0.9719 - acc: 0.7287 - val_loss: 1.2849 - val_acc: 0.6076\n",
      "Epoch 60/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9910 - acc: 0.7015\n",
      "Epoch 00060: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 0.9955 - acc: 0.7009 - val_loss: 1.2499 - val_acc: 0.6407\n",
      "Epoch 61/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9771 - acc: 0.7151\n",
      "Epoch 00061: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 408us/sample - loss: 0.9737 - acc: 0.7157 - val_loss: 1.2399 - val_acc: 0.6359\n",
      "Epoch 62/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9900 - acc: 0.6863\n",
      "Epoch 00062: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 0.9924 - acc: 0.6850 - val_loss: 1.2974 - val_acc: 0.6241\n",
      "Epoch 63/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.9737 - acc: 0.7135\n",
      "Epoch 00063: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 0.9602 - acc: 0.7199 - val_loss: 1.3305 - val_acc: 0.6312\n",
      "Epoch 64/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.9659 - acc: 0.7148\n",
      "Epoch 00064: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 0.9632 - acc: 0.7145 - val_loss: 1.3073 - val_acc: 0.6217\n",
      "Epoch 65/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9759 - acc: 0.7004\n",
      "Epoch 00065: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 0.9755 - acc: 0.6998 - val_loss: 1.2622 - val_acc: 0.6407\n",
      "Epoch 66/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9487 - acc: 0.7188\n",
      "Epoch 00066: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 0.9411 - acc: 0.7228 - val_loss: 1.2969 - val_acc: 0.6099\n",
      "Epoch 67/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9608 - acc: 0.7124\n",
      "Epoch 00067: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 0.9583 - acc: 0.7139 - val_loss: 1.2414 - val_acc: 0.6430\n",
      "Epoch 68/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9213 - acc: 0.7321\n",
      "Epoch 00068: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 415us/sample - loss: 0.9268 - acc: 0.7293 - val_loss: 1.2989 - val_acc: 0.6288\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9167 - acc: 0.7206\n",
      "Epoch 00069: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 435us/sample - loss: 0.9214 - acc: 0.7234 - val_loss: 1.3738 - val_acc: 0.6217\n",
      "Epoch 70/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9494 - acc: 0.7125\n",
      "Epoch 00070: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 413us/sample - loss: 0.9568 - acc: 0.7104 - val_loss: 1.2683 - val_acc: 0.6454\n",
      "Epoch 71/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9069 - acc: 0.7292\n",
      "Epoch 00071: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 437us/sample - loss: 0.9109 - acc: 0.7270 - val_loss: 1.2974 - val_acc: 0.6407\n",
      "Epoch 72/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9169 - acc: 0.7338\n",
      "Epoch 00072: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 0.9198 - acc: 0.7335 - val_loss: 1.3455 - val_acc: 0.6336\n",
      "Epoch 73/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9041 - acc: 0.7262\n",
      "Epoch 00073: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 431us/sample - loss: 0.9025 - acc: 0.7264 - val_loss: 1.3237 - val_acc: 0.6147\n",
      "Epoch 74/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9097 - acc: 0.7328\n",
      "Epoch 00074: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 0.9091 - acc: 0.7329 - val_loss: 1.3751 - val_acc: 0.6312\n",
      "Epoch 75/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9008 - acc: 0.7372\n",
      "Epoch 00075: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 0.8949 - acc: 0.7405 - val_loss: 1.3407 - val_acc: 0.6194\n",
      "Epoch 76/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8885 - acc: 0.7325\n",
      "Epoch 00076: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 418us/sample - loss: 0.8865 - acc: 0.7346 - val_loss: 1.3668 - val_acc: 0.6265\n",
      "Epoch 77/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8891 - acc: 0.7487\n",
      "Epoch 00077: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 0.8820 - acc: 0.7535 - val_loss: 1.3640 - val_acc: 0.6525\n",
      "Epoch 78/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8914 - acc: 0.7386\n",
      "Epoch 00078: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 0.8892 - acc: 0.7405 - val_loss: 1.3606 - val_acc: 0.6147\n",
      "Epoch 79/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8703 - acc: 0.7565\n",
      "Epoch 00079: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 0.8666 - acc: 0.7571 - val_loss: 1.3845 - val_acc: 0.6383\n",
      "Epoch 80/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8602 - acc: 0.7524\n",
      "Epoch 00080: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 0.8618 - acc: 0.7518 - val_loss: 1.3753 - val_acc: 0.6478\n",
      "Epoch 81/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8775 - acc: 0.7398\n",
      "Epoch 00081: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 0.8761 - acc: 0.7411 - val_loss: 1.4422 - val_acc: 0.6336\n",
      "Epoch 82/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8862 - acc: 0.7408\n",
      "Epoch 00082: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 0.8831 - acc: 0.7423 - val_loss: 1.3223 - val_acc: 0.6312\n",
      "Epoch 83/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8663 - acc: 0.7376\n",
      "Epoch 00083: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 0.8673 - acc: 0.7394 - val_loss: 1.3977 - val_acc: 0.6076\n",
      "Epoch 84/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8532 - acc: 0.7513\n",
      "Epoch 00084: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 433us/sample - loss: 0.8539 - acc: 0.7512 - val_loss: 1.5062 - val_acc: 0.6170\n",
      "Epoch 85/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8722 - acc: 0.7494\n",
      "Epoch 00085: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 436us/sample - loss: 0.8683 - acc: 0.7512 - val_loss: 1.3418 - val_acc: 0.6383\n",
      "Epoch 86/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8289 - acc: 0.7663\n",
      "Epoch 00086: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 0.8263 - acc: 0.7671 - val_loss: 1.3503 - val_acc: 0.6359\n",
      "Epoch 87/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8393 - acc: 0.7623\n",
      "Epoch 00087: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 419us/sample - loss: 0.8364 - acc: 0.7648 - val_loss: 1.3105 - val_acc: 0.6525\n",
      "Epoch 88/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8336 - acc: 0.7591\n",
      "Epoch 00088: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 0.8388 - acc: 0.7600 - val_loss: 1.4388 - val_acc: 0.6147\n",
      "Epoch 89/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8411 - acc: 0.7590\n",
      "Epoch 00089: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 0.8392 - acc: 0.7595 - val_loss: 1.4359 - val_acc: 0.6478\n",
      "Epoch 90/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8295 - acc: 0.7578\n",
      "Epoch 00090: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 435us/sample - loss: 0.8291 - acc: 0.7583 - val_loss: 1.4592 - val_acc: 0.6407\n",
      "Epoch 91/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8316 - acc: 0.7614\n",
      "Epoch 00091: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 431us/sample - loss: 0.8309 - acc: 0.7618 - val_loss: 1.4559 - val_acc: 0.6288\n",
      "Epoch 92/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8269 - acc: 0.7704\n",
      "Epoch 00092: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 0.8257 - acc: 0.7719 - val_loss: 1.5244 - val_acc: 0.6194\n",
      "Epoch 93/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8145 - acc: 0.7744\n",
      "Epoch 00093: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 422us/sample - loss: 0.8157 - acc: 0.7748 - val_loss: 1.3697 - val_acc: 0.6336\n",
      "Epoch 94/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7895 - acc: 0.7812\n",
      "Epoch 00094: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 0.7915 - acc: 0.7801 - val_loss: 1.4122 - val_acc: 0.6430\n",
      "Epoch 95/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8050 - acc: 0.7825\n",
      "Epoch 00095: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 435us/sample - loss: 0.8035 - acc: 0.7819 - val_loss: 1.3566 - val_acc: 0.6454\n",
      "Epoch 96/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8017 - acc: 0.7744\n",
      "Epoch 00096: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 0.7935 - acc: 0.7784 - val_loss: 1.4184 - val_acc: 0.6478\n",
      "Epoch 97/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.7914 - acc: 0.7919\n",
      "Epoch 00097: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 433us/sample - loss: 0.7973 - acc: 0.7890 - val_loss: 1.4741 - val_acc: 0.6241\n",
      "Epoch 98/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.7870 - acc: 0.7819\n",
      "Epoch 00098: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 0.7804 - acc: 0.7855 - val_loss: 1.5238 - val_acc: 0.6407\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.7714 - acc: 0.7862\n",
      "Epoch 00099: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 413us/sample - loss: 0.7671 - acc: 0.7884 - val_loss: 1.6659 - val_acc: 0.6359\n",
      "Epoch 100/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.7972 - acc: 0.7756\n",
      "Epoch 00100: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 0.7926 - acc: 0.7772 - val_loss: 1.5817 - val_acc: 0.6336\n",
      "443/443 [==============================] - 0s 180us/sample - loss: 1.6239 - acc: 0.5824\n",
      "--- Starting trial: run-5\n",
      "{'TIME_WINDOW': 750, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 128, 'LAST_HIDDEN': 500, 'LAST_DROPOUT': 0.3, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_90\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_91 (InputLayer)        [(None, 22, 750)]         0         \n",
      "_________________________________________________________________\n",
      "permute_90 (Permute)         (None, 750, 22)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_180 (Conv1D)          (None, 747, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_180 (Bat (None, 747, 32)           2988      \n",
      "_________________________________________________________________\n",
      "activation_180 (Activation)  (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_270 (Dropout)        (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_180 (MaxPoolin (None, 186, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_181 (Conv1D)          (None, 183, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_181 (Bat (None, 183, 64)           732       \n",
      "_________________________________________________________________\n",
      "activation_181 (Activation)  (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_271 (Dropout)        (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_181 (MaxPoolin (None, 45, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_270 (LSTM)              (None, 45, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_271 (LSTM)              (None, 45, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_272 (LSTM)              (None, 45, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_90 (Flatten)         (None, 5760)              0         \n",
      "_________________________________________________________________\n",
      "dense_180 (Dense)            (None, 500)               2880500   \n",
      "_________________________________________________________________\n",
      "dropout_272 (Dropout)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 4)                 2004      \n",
      "=================================================================\n",
      "Total params: 3,259,312\n",
      "Trainable params: 3,257,452\n",
      "Non-trainable params: 1,860\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.1455 - acc: 0.3048\n",
      "Epoch 00001: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 5s 3ms/sample - loss: 3.1295 - acc: 0.3103 - val_loss: 2.8606 - val_acc: 0.3381\n",
      "Epoch 2/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.8052 - acc: 0.3597\n",
      "Epoch 00002: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 2.8013 - acc: 0.3540 - val_loss: 2.6515 - val_acc: 0.3688\n",
      "Epoch 3/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.6449 - acc: 0.3806\n",
      "Epoch 00003: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 2.6400 - acc: 0.3806 - val_loss: 2.5404 - val_acc: 0.3688\n",
      "Epoch 4/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.5322 - acc: 0.3750\n",
      "Epoch 00004: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 2.5278 - acc: 0.3759 - val_loss: 2.4273 - val_acc: 0.4184\n",
      "Epoch 5/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.4161 - acc: 0.4062\n",
      "Epoch 00005: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 433us/sample - loss: 2.4141 - acc: 0.4066 - val_loss: 2.3300 - val_acc: 0.4303\n",
      "Epoch 6/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.3317 - acc: 0.4255\n",
      "Epoch 00006: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 438us/sample - loss: 2.3295 - acc: 0.4273 - val_loss: 2.2409 - val_acc: 0.4232\n",
      "Epoch 7/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 2.2275 - acc: 0.4323\n",
      "Epoch 00007: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 2.2233 - acc: 0.4320 - val_loss: 2.1498 - val_acc: 0.4421\n",
      "Epoch 8/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.1618 - acc: 0.4338\n",
      "Epoch 00008: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 2.1565 - acc: 0.4368 - val_loss: 2.0799 - val_acc: 0.4539\n",
      "Epoch 9/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.0803 - acc: 0.4512\n",
      "Epoch 00009: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 2.0759 - acc: 0.4521 - val_loss: 1.9928 - val_acc: 0.4586\n",
      "Epoch 10/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.9951 - acc: 0.4783\n",
      "Epoch 00010: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 1.9965 - acc: 0.4746 - val_loss: 1.9318 - val_acc: 0.4775\n",
      "Epoch 11/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.9175 - acc: 0.4944\n",
      "Epoch 00011: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 1.9205 - acc: 0.4911 - val_loss: 1.8618 - val_acc: 0.4941\n",
      "Epoch 12/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.8631 - acc: 0.5084\n",
      "Epoch 00012: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 435us/sample - loss: 1.8626 - acc: 0.5095 - val_loss: 1.7999 - val_acc: 0.5201\n",
      "Epoch 13/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.8131 - acc: 0.5072\n",
      "Epoch 00013: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 1.8155 - acc: 0.5059 - val_loss: 1.7759 - val_acc: 0.5083\n",
      "Epoch 14/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.7672 - acc: 0.5075\n",
      "Epoch 00014: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 1.7637 - acc: 0.5112 - val_loss: 1.7020 - val_acc: 0.5461\n",
      "Epoch 15/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.7197 - acc: 0.5312\n",
      "Epoch 00015: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 1.7205 - acc: 0.5307 - val_loss: 1.6642 - val_acc: 0.5390\n",
      "Epoch 16/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.6774 - acc: 0.5288\n",
      "Epoch 00016: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 1.6765 - acc: 0.5301 - val_loss: 1.6511 - val_acc: 0.5248\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6392 - acc: 0.5300\n",
      "Epoch 00017: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 1.6388 - acc: 0.5325 - val_loss: 1.5959 - val_acc: 0.5485\n",
      "Epoch 18/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.5878 - acc: 0.5469\n",
      "Epoch 00018: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 442us/sample - loss: 1.5863 - acc: 0.5485 - val_loss: 1.5826 - val_acc: 0.5437\n",
      "Epoch 19/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.5515 - acc: 0.5612\n",
      "Epoch 00019: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 1.5524 - acc: 0.5621 - val_loss: 1.5558 - val_acc: 0.5296\n",
      "Epoch 20/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5212 - acc: 0.5587\n",
      "Epoch 00020: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 1.5256 - acc: 0.5550 - val_loss: 1.4957 - val_acc: 0.5650\n",
      "Epoch 21/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.5065 - acc: 0.5581\n",
      "Epoch 00021: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 510us/sample - loss: 1.5071 - acc: 0.5621 - val_loss: 1.4872 - val_acc: 0.5603\n",
      "Epoch 22/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4731 - acc: 0.5876\n",
      "Epoch 00022: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 560us/sample - loss: 1.4721 - acc: 0.5875 - val_loss: 1.4867 - val_acc: 0.5721\n",
      "Epoch 23/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4577 - acc: 0.5733\n",
      "Epoch 00023: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 555us/sample - loss: 1.4566 - acc: 0.5721 - val_loss: 1.4593 - val_acc: 0.5532\n",
      "Epoch 24/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4068 - acc: 0.5833\n",
      "Epoch 00024: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 555us/sample - loss: 1.4079 - acc: 0.5839 - val_loss: 1.4342 - val_acc: 0.5626\n",
      "Epoch 25/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.4021 - acc: 0.5788\n",
      "Epoch 00025: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 1.3994 - acc: 0.5798 - val_loss: 1.4429 - val_acc: 0.5508\n",
      "Epoch 26/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3702 - acc: 0.6011\n",
      "Epoch 00026: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 553us/sample - loss: 1.3691 - acc: 0.6005 - val_loss: 1.3676 - val_acc: 0.6028\n",
      "Epoch 27/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3639 - acc: 0.5975\n",
      "Epoch 00027: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 528us/sample - loss: 1.3626 - acc: 0.5975 - val_loss: 1.3762 - val_acc: 0.5816\n",
      "Epoch 28/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3126 - acc: 0.6115\n",
      "Epoch 00028: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 565us/sample - loss: 1.3166 - acc: 0.6099 - val_loss: 1.3528 - val_acc: 0.6052\n",
      "Epoch 29/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3280 - acc: 0.6048\n",
      "Epoch 00029: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 1.3258 - acc: 0.6076 - val_loss: 1.3408 - val_acc: 0.5981\n",
      "Epoch 30/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3037 - acc: 0.6081\n",
      "Epoch 00030: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 569us/sample - loss: 1.2976 - acc: 0.6117 - val_loss: 1.3020 - val_acc: 0.6123\n",
      "Epoch 31/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2929 - acc: 0.6058\n",
      "Epoch 00031: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 564us/sample - loss: 1.2926 - acc: 0.6058 - val_loss: 1.3304 - val_acc: 0.5910\n",
      "Epoch 32/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2724 - acc: 0.6201\n",
      "Epoch 00032: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 552us/sample - loss: 1.2727 - acc: 0.6212 - val_loss: 1.2867 - val_acc: 0.6099\n",
      "Epoch 33/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2532 - acc: 0.6056\n",
      "Epoch 00033: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 566us/sample - loss: 1.2498 - acc: 0.6076 - val_loss: 1.2969 - val_acc: 0.6217\n",
      "Epoch 34/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2323 - acc: 0.6352\n",
      "Epoch 00034: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 557us/sample - loss: 1.2370 - acc: 0.6336 - val_loss: 1.2862 - val_acc: 0.6052\n",
      "Epoch 35/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2145 - acc: 0.6452\n",
      "Epoch 00035: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 553us/sample - loss: 1.2192 - acc: 0.6424 - val_loss: 1.2996 - val_acc: 0.5981\n",
      "Epoch 36/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2016 - acc: 0.6403\n",
      "Epoch 00036: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 580us/sample - loss: 1.2027 - acc: 0.6371 - val_loss: 1.2685 - val_acc: 0.6076\n",
      "Epoch 37/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.2181 - acc: 0.6237\n",
      "Epoch 00037: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 545us/sample - loss: 1.2097 - acc: 0.6288 - val_loss: 1.2401 - val_acc: 0.6099\n",
      "Epoch 38/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1694 - acc: 0.6431\n",
      "Epoch 00038: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 500us/sample - loss: 1.1727 - acc: 0.6413 - val_loss: 1.3326 - val_acc: 0.5910\n",
      "Epoch 39/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1682 - acc: 0.6365\n",
      "Epoch 00039: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 540us/sample - loss: 1.1770 - acc: 0.6342 - val_loss: 1.2257 - val_acc: 0.6265\n",
      "Epoch 40/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1490 - acc: 0.6639\n",
      "Epoch 00040: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 521us/sample - loss: 1.1548 - acc: 0.6613 - val_loss: 1.2395 - val_acc: 0.6265\n",
      "Epoch 41/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1534 - acc: 0.6544\n",
      "Epoch 00041: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 549us/sample - loss: 1.1523 - acc: 0.6554 - val_loss: 1.2627 - val_acc: 0.6170\n",
      "Epoch 42/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1438 - acc: 0.6460\n",
      "Epoch 00042: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 564us/sample - loss: 1.1441 - acc: 0.6472 - val_loss: 1.2259 - val_acc: 0.6288\n",
      "Epoch 43/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1439 - acc: 0.6538\n",
      "Epoch 00043: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 1.1385 - acc: 0.6566 - val_loss: 1.2134 - val_acc: 0.6501\n",
      "Epoch 44/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1288 - acc: 0.6587\n",
      "Epoch 00044: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 563us/sample - loss: 1.1267 - acc: 0.6602 - val_loss: 1.2875 - val_acc: 0.6241\n",
      "Epoch 45/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1067 - acc: 0.6639\n",
      "Epoch 00045: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 560us/sample - loss: 1.1030 - acc: 0.6678 - val_loss: 1.2834 - val_acc: 0.6099\n",
      "Epoch 46/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0970 - acc: 0.6653\n",
      "Epoch 00046: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 554us/sample - loss: 1.0990 - acc: 0.6637 - val_loss: 1.2834 - val_acc: 0.6005\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0864 - acc: 0.6750\n",
      "Epoch 00047: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 1.0888 - acc: 0.6738 - val_loss: 1.2420 - val_acc: 0.6525\n",
      "Epoch 48/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.0552 - acc: 0.6855\n",
      "Epoch 00048: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 483us/sample - loss: 1.0607 - acc: 0.6838 - val_loss: 1.2212 - val_acc: 0.6312\n",
      "Epoch 49/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0603 - acc: 0.6857\n",
      "Epoch 00049: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 439us/sample - loss: 1.0620 - acc: 0.6856 - val_loss: 1.2280 - val_acc: 0.6478\n",
      "Epoch 50/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.0546 - acc: 0.6908\n",
      "Epoch 00050: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 431us/sample - loss: 1.0617 - acc: 0.6862 - val_loss: 1.2101 - val_acc: 0.6619\n",
      "Epoch 51/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0649 - acc: 0.6812\n",
      "Epoch 00051: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 1.0608 - acc: 0.6838 - val_loss: 1.1961 - val_acc: 0.6714\n",
      "Epoch 52/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0704 - acc: 0.6777\n",
      "Epoch 00052: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 440us/sample - loss: 1.0742 - acc: 0.6773 - val_loss: 1.2261 - val_acc: 0.6525\n",
      "Epoch 53/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0138 - acc: 0.7044\n",
      "Epoch 00053: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 444us/sample - loss: 1.0189 - acc: 0.7009 - val_loss: 1.2447 - val_acc: 0.6241\n",
      "Epoch 54/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.0276 - acc: 0.7018\n",
      "Epoch 00054: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 440us/sample - loss: 1.0279 - acc: 0.7027 - val_loss: 1.2624 - val_acc: 0.6312\n",
      "Epoch 55/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0339 - acc: 0.6716\n",
      "Epoch 00055: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 417us/sample - loss: 1.0308 - acc: 0.6749 - val_loss: 1.2069 - val_acc: 0.6454\n",
      "Epoch 56/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0069 - acc: 0.7124\n",
      "Epoch 00056: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 432us/sample - loss: 1.0089 - acc: 0.7098 - val_loss: 1.2043 - val_acc: 0.6407\n",
      "Epoch 57/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0287 - acc: 0.7034\n",
      "Epoch 00057: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 423us/sample - loss: 1.0274 - acc: 0.7033 - val_loss: 1.1783 - val_acc: 0.6525\n",
      "Epoch 58/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9954 - acc: 0.7105\n",
      "Epoch 00058: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 1.0025 - acc: 0.7051 - val_loss: 1.2983 - val_acc: 0.6217\n",
      "Epoch 59/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.9991 - acc: 0.7012\n",
      "Epoch 00059: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 0.9921 - acc: 0.7039 - val_loss: 1.2314 - val_acc: 0.6596\n",
      "Epoch 60/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9899 - acc: 0.6932\n",
      "Epoch 00060: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 0.9820 - acc: 0.6974 - val_loss: 1.2068 - val_acc: 0.6548\n",
      "Epoch 61/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.9635 - acc: 0.7207\n",
      "Epoch 00061: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 0.9710 - acc: 0.7181 - val_loss: 1.3563 - val_acc: 0.5863\n",
      "Epoch 62/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9826 - acc: 0.7085\n",
      "Epoch 00062: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 0.9979 - acc: 0.7033 - val_loss: 1.2035 - val_acc: 0.6619\n",
      "Epoch 63/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9564 - acc: 0.7194\n",
      "Epoch 00063: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 0.9582 - acc: 0.7204 - val_loss: 1.2488 - val_acc: 0.6572\n",
      "Epoch 64/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9468 - acc: 0.7175\n",
      "Epoch 00064: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 434us/sample - loss: 0.9458 - acc: 0.7187 - val_loss: 1.1836 - val_acc: 0.6738\n",
      "Epoch 65/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.9603 - acc: 0.7214\n",
      "Epoch 00065: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 0.9643 - acc: 0.7187 - val_loss: 1.2134 - val_acc: 0.6525\n",
      "Epoch 66/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9551 - acc: 0.7206\n",
      "Epoch 00066: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 0.9568 - acc: 0.7204 - val_loss: 1.2267 - val_acc: 0.6738\n",
      "Epoch 67/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9489 - acc: 0.7224\n",
      "Epoch 00067: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 431us/sample - loss: 0.9447 - acc: 0.7234 - val_loss: 1.2274 - val_acc: 0.6667\n",
      "Epoch 68/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9278 - acc: 0.7386\n",
      "Epoch 00068: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 440us/sample - loss: 0.9287 - acc: 0.7376 - val_loss: 1.2656 - val_acc: 0.6596\n",
      "Epoch 69/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.9270 - acc: 0.7266\n",
      "Epoch 00069: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 436us/sample - loss: 0.9257 - acc: 0.7246 - val_loss: 1.2957 - val_acc: 0.6454\n",
      "Epoch 70/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9308 - acc: 0.7356\n",
      "Epoch 00070: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 0.9232 - acc: 0.7394 - val_loss: 1.2018 - val_acc: 0.6501\n",
      "Epoch 71/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9358 - acc: 0.7207\n",
      "Epoch 00071: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 0.9351 - acc: 0.7222 - val_loss: 1.2429 - val_acc: 0.6690\n",
      "Epoch 72/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9368 - acc: 0.7224\n",
      "Epoch 00072: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 448us/sample - loss: 0.9398 - acc: 0.7193 - val_loss: 1.2626 - val_acc: 0.6407\n",
      "Epoch 73/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.9211 - acc: 0.7344\n",
      "Epoch 00073: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 0.9164 - acc: 0.7376 - val_loss: 1.2734 - val_acc: 0.6383\n",
      "Epoch 74/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8863 - acc: 0.7474\n",
      "Epoch 00074: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 0.8807 - acc: 0.7476 - val_loss: 1.2306 - val_acc: 0.6809\n",
      "Epoch 75/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8782 - acc: 0.7396\n",
      "Epoch 00075: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 425us/sample - loss: 0.8819 - acc: 0.7388 - val_loss: 1.2509 - val_acc: 0.6761\n",
      "Epoch 76/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8785 - acc: 0.7455\n",
      "Epoch 00076: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 414us/sample - loss: 0.8792 - acc: 0.7459 - val_loss: 1.2617 - val_acc: 0.6714\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8917 - acc: 0.7398\n",
      "Epoch 00077: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 421us/sample - loss: 0.8892 - acc: 0.7400 - val_loss: 1.3553 - val_acc: 0.6407\n",
      "Epoch 78/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8818 - acc: 0.7487\n",
      "Epoch 00078: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 0.8838 - acc: 0.7518 - val_loss: 1.3088 - val_acc: 0.6525\n",
      "Epoch 79/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8797 - acc: 0.7604\n",
      "Epoch 00079: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 435us/sample - loss: 0.8834 - acc: 0.7577 - val_loss: 1.2616 - val_acc: 0.6548\n",
      "Epoch 80/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8676 - acc: 0.7520\n",
      "Epoch 00080: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 424us/sample - loss: 0.8609 - acc: 0.7553 - val_loss: 1.2477 - val_acc: 0.6667\n",
      "Epoch 81/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8662 - acc: 0.7455\n",
      "Epoch 00081: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 0.8662 - acc: 0.7465 - val_loss: 1.3458 - val_acc: 0.6525\n",
      "Epoch 82/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8657 - acc: 0.7544\n",
      "Epoch 00082: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 429us/sample - loss: 0.8640 - acc: 0.7547 - val_loss: 1.2629 - val_acc: 0.6454\n",
      "Epoch 83/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8313 - acc: 0.7650\n",
      "Epoch 00083: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 0.8292 - acc: 0.7654 - val_loss: 1.2582 - val_acc: 0.6738\n",
      "Epoch 84/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8451 - acc: 0.7578\n",
      "Epoch 00084: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 427us/sample - loss: 0.8444 - acc: 0.7547 - val_loss: 1.2574 - val_acc: 0.6761\n",
      "Epoch 85/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8556 - acc: 0.7400\n",
      "Epoch 00085: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 435us/sample - loss: 0.8537 - acc: 0.7405 - val_loss: 1.2586 - val_acc: 0.6761\n",
      "Epoch 86/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8424 - acc: 0.7566\n",
      "Epoch 00086: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 428us/sample - loss: 0.8415 - acc: 0.7559 - val_loss: 1.2650 - val_acc: 0.6667\n",
      "Epoch 87/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8143 - acc: 0.7694\n",
      "Epoch 00087: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 444us/sample - loss: 0.8252 - acc: 0.7689 - val_loss: 1.3179 - val_acc: 0.6667\n",
      "Epoch 88/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8224 - acc: 0.7689\n",
      "Epoch 00088: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 439us/sample - loss: 0.8347 - acc: 0.7624 - val_loss: 1.2811 - val_acc: 0.6643\n",
      "Epoch 89/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.7973 - acc: 0.7770\n",
      "Epoch 00089: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 445us/sample - loss: 0.7944 - acc: 0.7784 - val_loss: 1.2559 - val_acc: 0.6832\n",
      "Epoch 90/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.7990 - acc: 0.7812\n",
      "Epoch 00090: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 436us/sample - loss: 0.7996 - acc: 0.7807 - val_loss: 1.3104 - val_acc: 0.6714\n",
      "Epoch 91/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8110 - acc: 0.7692\n",
      "Epoch 00091: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 435us/sample - loss: 0.8078 - acc: 0.7713 - val_loss: 1.2960 - val_acc: 0.6619\n",
      "Epoch 92/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8091 - acc: 0.7740\n",
      "Epoch 00092: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 433us/sample - loss: 0.8107 - acc: 0.7725 - val_loss: 1.2763 - val_acc: 0.6714\n",
      "Epoch 93/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7883 - acc: 0.7895\n",
      "Epoch 00093: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 435us/sample - loss: 0.7892 - acc: 0.7926 - val_loss: 1.3456 - val_acc: 0.6525\n",
      "Epoch 94/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8105 - acc: 0.7728\n",
      "Epoch 00094: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 442us/sample - loss: 0.8098 - acc: 0.7730 - val_loss: 1.3449 - val_acc: 0.6572\n",
      "Epoch 95/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.7766 - acc: 0.7923\n",
      "Epoch 00095: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 436us/sample - loss: 0.7708 - acc: 0.7949 - val_loss: 1.3189 - val_acc: 0.6738\n",
      "Epoch 96/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7746 - acc: 0.7953\n",
      "Epoch 00096: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 430us/sample - loss: 0.7951 - acc: 0.7866 - val_loss: 1.3408 - val_acc: 0.6667\n",
      "Epoch 97/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.7616 - acc: 0.7944\n",
      "Epoch 00097: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 435us/sample - loss: 0.7601 - acc: 0.7949 - val_loss: 1.3130 - val_acc: 0.6667\n",
      "Epoch 98/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7818 - acc: 0.7851\n",
      "Epoch 00098: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 0.7806 - acc: 0.7866 - val_loss: 1.3389 - val_acc: 0.6738\n",
      "Epoch 99/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7905 - acc: 0.7761\n",
      "Epoch 00099: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 0.7922 - acc: 0.7730 - val_loss: 1.4393 - val_acc: 0.6430\n",
      "Epoch 100/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.7789 - acc: 0.7780\n",
      "Epoch 00100: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 426us/sample - loss: 0.7744 - acc: 0.7801 - val_loss: 1.3777 - val_acc: 0.6809\n",
      "443/443 [==============================] - 0s 184us/sample - loss: 1.4186 - acc: 0.6185\n",
      "--- Starting trial: run-6\n",
      "{'TIME_WINDOW': 750, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 128, 'LAST_HIDDEN': 1000, 'LAST_DROPOUT': 0.2, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_92 (InputLayer)        [(None, 22, 750)]         0         \n",
      "_________________________________________________________________\n",
      "permute_91 (Permute)         (None, 750, 22)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_182 (Conv1D)          (None, 747, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_182 (Bat (None, 747, 32)           2988      \n",
      "_________________________________________________________________\n",
      "activation_182 (Activation)  (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_273 (Dropout)        (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_182 (MaxPoolin (None, 186, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_183 (Conv1D)          (None, 183, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_183 (Bat (None, 183, 64)           732       \n",
      "_________________________________________________________________\n",
      "activation_183 (Activation)  (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_274 (Dropout)        (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_183 (MaxPoolin (None, 45, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_273 (LSTM)              (None, 45, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_274 (LSTM)              (None, 45, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_275 (LSTM)              (None, 45, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_91 (Flatten)         (None, 5760)              0         \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (None, 1000)              5761000   \n",
      "_________________________________________________________________\n",
      "dropout_275 (Dropout)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 4)                 4004      \n",
      "=================================================================\n",
      "Total params: 6,141,812\n",
      "Trainable params: 6,139,952\n",
      "Non-trainable params: 1,860\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664/1692 [============================>.] - ETA: 0s - loss: 3.8384 - acc: 0.3131\n",
      "Epoch 00001: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 3.8343 - acc: 0.3150 - val_loss: 3.5183 - val_acc: 0.3688\n",
      "Epoch 2/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 3.4413 - acc: 0.3552\n",
      "Epoch 00002: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 450us/sample - loss: 3.4392 - acc: 0.3552 - val_loss: 3.2441 - val_acc: 0.4279\n",
      "Epoch 3/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 3.2014 - acc: 0.3830\n",
      "Epoch 00003: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 454us/sample - loss: 3.1966 - acc: 0.3836 - val_loss: 3.0312 - val_acc: 0.4043\n",
      "Epoch 4/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.9971 - acc: 0.4038\n",
      "Epoch 00004: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 455us/sample - loss: 2.9907 - acc: 0.4060 - val_loss: 2.8530 - val_acc: 0.4492\n",
      "Epoch 5/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.8282 - acc: 0.4213\n",
      "Epoch 00005: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 460us/sample - loss: 2.8264 - acc: 0.4226 - val_loss: 2.6982 - val_acc: 0.4326\n",
      "Epoch 6/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.6509 - acc: 0.4381\n",
      "Epoch 00006: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 449us/sample - loss: 2.6456 - acc: 0.4391 - val_loss: 2.5210 - val_acc: 0.4775\n",
      "Epoch 7/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.5043 - acc: 0.4627\n",
      "Epoch 00007: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 455us/sample - loss: 2.5063 - acc: 0.4610 - val_loss: 2.4263 - val_acc: 0.4397\n",
      "Epoch 8/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.3691 - acc: 0.4736\n",
      "Epoch 00008: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 447us/sample - loss: 2.3666 - acc: 0.4752 - val_loss: 2.2834 - val_acc: 0.5059\n",
      "Epoch 9/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.2563 - acc: 0.4783\n",
      "Epoch 00009: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 452us/sample - loss: 2.2518 - acc: 0.4770 - val_loss: 2.1705 - val_acc: 0.4823\n",
      "Epoch 10/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.1597 - acc: 0.4913\n",
      "Epoch 00010: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 459us/sample - loss: 2.1608 - acc: 0.4888 - val_loss: 2.1001 - val_acc: 0.4775\n",
      "Epoch 11/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.0663 - acc: 0.4798\n",
      "Epoch 00011: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 460us/sample - loss: 2.0668 - acc: 0.4787 - val_loss: 2.0436 - val_acc: 0.4634\n",
      "Epoch 12/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.9756 - acc: 0.4969\n",
      "Epoch 00012: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 457us/sample - loss: 1.9808 - acc: 0.4894 - val_loss: 1.9215 - val_acc: 0.5201\n",
      "Epoch 13/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.9117 - acc: 0.5019\n",
      "Epoch 00013: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 460us/sample - loss: 1.9031 - acc: 0.5095 - val_loss: 1.8872 - val_acc: 0.5035\n",
      "Epoch 14/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.8411 - acc: 0.5198\n",
      "Epoch 00014: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 463us/sample - loss: 1.8353 - acc: 0.5207 - val_loss: 1.7767 - val_acc: 0.5248\n",
      "Epoch 15/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.7823 - acc: 0.5213\n",
      "Epoch 00015: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 469us/sample - loss: 1.7824 - acc: 0.5183 - val_loss: 1.7424 - val_acc: 0.5248\n",
      "Epoch 16/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.7248 - acc: 0.5282\n",
      "Epoch 00016: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 450us/sample - loss: 1.7270 - acc: 0.5272 - val_loss: 1.6845 - val_acc: 0.5414\n",
      "Epoch 17/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6812 - acc: 0.5312\n",
      "Epoch 00017: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 441us/sample - loss: 1.6803 - acc: 0.5319 - val_loss: 1.6358 - val_acc: 0.5485\n",
      "Epoch 18/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.6148 - acc: 0.5306\n",
      "Epoch 00018: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 461us/sample - loss: 1.6031 - acc: 0.5378 - val_loss: 1.6546 - val_acc: 0.5130\n",
      "Epoch 19/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5849 - acc: 0.5478\n",
      "Epoch 00019: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 458us/sample - loss: 1.5901 - acc: 0.5426 - val_loss: 1.6430 - val_acc: 0.5106\n",
      "Epoch 20/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.5464 - acc: 0.5519\n",
      "Epoch 00020: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 440us/sample - loss: 1.5472 - acc: 0.5485 - val_loss: 1.5264 - val_acc: 0.5437\n",
      "Epoch 21/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4993 - acc: 0.5643\n",
      "Epoch 00021: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 449us/sample - loss: 1.4984 - acc: 0.5626 - val_loss: 1.5670 - val_acc: 0.5390\n",
      "Epoch 22/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4811 - acc: 0.5509\n",
      "Epoch 00022: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 461us/sample - loss: 1.4805 - acc: 0.5508 - val_loss: 1.4729 - val_acc: 0.5603\n",
      "Epoch 23/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4351 - acc: 0.5691\n",
      "Epoch 00023: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 448us/sample - loss: 1.4369 - acc: 0.5680 - val_loss: 1.4758 - val_acc: 0.5626\n",
      "Epoch 24/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.4131 - acc: 0.5744\n",
      "Epoch 00024: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 459us/sample - loss: 1.4109 - acc: 0.5751 - val_loss: 1.4509 - val_acc: 0.5697\n",
      "Epoch 25/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.4204 - acc: 0.5625\n",
      "Epoch 00025: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 445us/sample - loss: 1.4143 - acc: 0.5632 - val_loss: 1.4346 - val_acc: 0.5556\n",
      "Epoch 26/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3752 - acc: 0.5805\n",
      "Epoch 00026: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 448us/sample - loss: 1.3712 - acc: 0.5816 - val_loss: 1.4123 - val_acc: 0.5768\n",
      "Epoch 27/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3522 - acc: 0.5805\n",
      "Epoch 00027: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 452us/sample - loss: 1.3545 - acc: 0.5792 - val_loss: 1.4256 - val_acc: 0.5626\n",
      "Epoch 28/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3496 - acc: 0.5763\n",
      "Epoch 00028: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 464us/sample - loss: 1.3457 - acc: 0.5780 - val_loss: 1.3698 - val_acc: 0.5792\n",
      "Epoch 29/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3000 - acc: 0.5925\n",
      "Epoch 00029: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 467us/sample - loss: 1.3117 - acc: 0.5869 - val_loss: 1.3627 - val_acc: 0.5792\n",
      "Epoch 30/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3014 - acc: 0.5804\n",
      "Epoch 00030: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 449us/sample - loss: 1.3037 - acc: 0.5827 - val_loss: 1.3413 - val_acc: 0.5863\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2749 - acc: 0.5992\n",
      "Epoch 00031: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 448us/sample - loss: 1.2735 - acc: 0.6011 - val_loss: 1.3970 - val_acc: 0.5626\n",
      "Epoch 32/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2697 - acc: 0.5870\n",
      "Epoch 00032: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 454us/sample - loss: 1.2739 - acc: 0.5863 - val_loss: 1.3243 - val_acc: 0.6028\n",
      "Epoch 33/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2537 - acc: 0.6027\n",
      "Epoch 00033: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 449us/sample - loss: 1.2514 - acc: 0.6058 - val_loss: 1.3263 - val_acc: 0.5721\n",
      "Epoch 34/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2435 - acc: 0.6072\n",
      "Epoch 00034: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 457us/sample - loss: 1.2404 - acc: 0.6082 - val_loss: 1.3767 - val_acc: 0.5934\n",
      "Epoch 35/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2406 - acc: 0.6027\n",
      "Epoch 00035: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 455us/sample - loss: 1.2422 - acc: 0.6034 - val_loss: 1.3434 - val_acc: 0.5887\n",
      "Epoch 36/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2219 - acc: 0.6213\n",
      "Epoch 00036: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 447us/sample - loss: 1.2227 - acc: 0.6212 - val_loss: 1.3078 - val_acc: 0.5910\n",
      "Epoch 37/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2012 - acc: 0.6293\n",
      "Epoch 00037: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 451us/sample - loss: 1.2041 - acc: 0.6277 - val_loss: 1.3342 - val_acc: 0.5745\n",
      "Epoch 38/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1959 - acc: 0.6275\n",
      "Epoch 00038: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 457us/sample - loss: 1.2070 - acc: 0.6217 - val_loss: 1.3432 - val_acc: 0.5768\n",
      "Epoch 39/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1952 - acc: 0.6219\n",
      "Epoch 00039: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 447us/sample - loss: 1.1881 - acc: 0.6259 - val_loss: 1.2943 - val_acc: 0.6005\n",
      "Epoch 40/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1811 - acc: 0.6352\n",
      "Epoch 00040: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 454us/sample - loss: 1.1853 - acc: 0.6318 - val_loss: 1.3080 - val_acc: 0.5863\n",
      "Epoch 41/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1694 - acc: 0.6274\n",
      "Epoch 00041: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 458us/sample - loss: 1.1714 - acc: 0.6265 - val_loss: 1.3137 - val_acc: 0.5792\n",
      "Epoch 42/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1703 - acc: 0.6354\n",
      "Epoch 00042: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 456us/sample - loss: 1.1666 - acc: 0.6383 - val_loss: 1.2850 - val_acc: 0.6099\n",
      "Epoch 43/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1452 - acc: 0.6431\n",
      "Epoch 00043: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 464us/sample - loss: 1.1462 - acc: 0.6413 - val_loss: 1.3089 - val_acc: 0.5839\n",
      "Epoch 44/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1438 - acc: 0.6370\n",
      "Epoch 00044: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 461us/sample - loss: 1.1412 - acc: 0.6371 - val_loss: 1.2370 - val_acc: 0.6147\n",
      "Epoch 45/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1181 - acc: 0.6490\n",
      "Epoch 00045: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 458us/sample - loss: 1.1189 - acc: 0.6489 - val_loss: 1.2997 - val_acc: 0.6170\n",
      "Epoch 46/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1145 - acc: 0.6507\n",
      "Epoch 00046: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 454us/sample - loss: 1.1102 - acc: 0.6519 - val_loss: 1.2403 - val_acc: 0.6265\n",
      "Epoch 47/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1239 - acc: 0.6400\n",
      "Epoch 00047: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 448us/sample - loss: 1.1205 - acc: 0.6418 - val_loss: 1.2909 - val_acc: 0.6147\n",
      "Epoch 48/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0898 - acc: 0.6606\n",
      "Epoch 00048: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 459us/sample - loss: 1.0928 - acc: 0.6602 - val_loss: 1.2404 - val_acc: 0.6265\n",
      "Epoch 49/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1158 - acc: 0.6531\n",
      "Epoch 00049: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 457us/sample - loss: 1.1114 - acc: 0.6554 - val_loss: 1.2381 - val_acc: 0.6430\n",
      "Epoch 50/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1100 - acc: 0.6600\n",
      "Epoch 00050: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 507us/sample - loss: 1.1211 - acc: 0.6543 - val_loss: 1.2505 - val_acc: 0.6170\n",
      "Epoch 51/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0842 - acc: 0.6599\n",
      "Epoch 00051: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 550us/sample - loss: 1.0907 - acc: 0.6566 - val_loss: 1.3003 - val_acc: 0.5957\n",
      "Epoch 52/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0717 - acc: 0.6709\n",
      "Epoch 00052: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 529us/sample - loss: 1.0762 - acc: 0.6667 - val_loss: 1.2404 - val_acc: 0.6383\n",
      "Epoch 53/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0638 - acc: 0.6856\n",
      "Epoch 00053: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 549us/sample - loss: 1.0679 - acc: 0.6826 - val_loss: 1.3118 - val_acc: 0.6099\n",
      "Epoch 54/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0782 - acc: 0.6508\n",
      "Epoch 00054: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 547us/sample - loss: 1.0746 - acc: 0.6519 - val_loss: 1.2861 - val_acc: 0.6194\n",
      "Epoch 55/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0550 - acc: 0.6837\n",
      "Epoch 00055: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 568us/sample - loss: 1.0595 - acc: 0.6832 - val_loss: 1.3192 - val_acc: 0.6076\n",
      "Epoch 56/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0447 - acc: 0.6737\n",
      "Epoch 00056: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 540us/sample - loss: 1.0423 - acc: 0.6749 - val_loss: 1.2415 - val_acc: 0.6407\n",
      "Epoch 57/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0329 - acc: 0.6838\n",
      "Epoch 00057: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 566us/sample - loss: 1.0329 - acc: 0.6838 - val_loss: 1.2903 - val_acc: 0.6407\n",
      "Epoch 58/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0380 - acc: 0.6971\n",
      "Epoch 00058: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 555us/sample - loss: 1.0362 - acc: 0.6968 - val_loss: 1.2861 - val_acc: 0.6241\n",
      "Epoch 59/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0178 - acc: 0.6956\n",
      "Epoch 00059: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 558us/sample - loss: 1.0264 - acc: 0.6933 - val_loss: 1.2578 - val_acc: 0.6288\n",
      "Epoch 60/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0115 - acc: 0.6888\n",
      "Epoch 00060: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 564us/sample - loss: 1.0136 - acc: 0.6897 - val_loss: 1.2904 - val_acc: 0.6312\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0204 - acc: 0.6792\n",
      "Epoch 00061: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 578us/sample - loss: 1.0365 - acc: 0.6708 - val_loss: 1.2141 - val_acc: 0.6336\n",
      "Epoch 62/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0265 - acc: 0.6875\n",
      "Epoch 00062: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 575us/sample - loss: 1.0213 - acc: 0.6891 - val_loss: 1.1847 - val_acc: 0.6407\n",
      "Epoch 63/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0046 - acc: 0.7071\n",
      "Epoch 00063: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 569us/sample - loss: 0.9987 - acc: 0.7098 - val_loss: 1.2135 - val_acc: 0.6501\n",
      "Epoch 64/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0033 - acc: 0.6938\n",
      "Epoch 00064: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 564us/sample - loss: 1.0010 - acc: 0.6944 - val_loss: 1.2903 - val_acc: 0.6454\n",
      "Epoch 65/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9852 - acc: 0.7022\n",
      "Epoch 00065: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 0.9857 - acc: 0.7051 - val_loss: 1.2784 - val_acc: 0.6359\n",
      "Epoch 66/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9849 - acc: 0.6990\n",
      "Epoch 00066: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 574us/sample - loss: 0.9834 - acc: 0.7033 - val_loss: 1.2726 - val_acc: 0.6336\n",
      "Epoch 67/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9890 - acc: 0.6965\n",
      "Epoch 00067: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 556us/sample - loss: 0.9854 - acc: 0.6974 - val_loss: 1.3251 - val_acc: 0.6359\n",
      "Epoch 68/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9793 - acc: 0.7075\n",
      "Epoch 00068: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 573us/sample - loss: 0.9719 - acc: 0.7116 - val_loss: 1.2933 - val_acc: 0.6336\n",
      "Epoch 69/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9672 - acc: 0.7151\n",
      "Epoch 00069: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 0.9671 - acc: 0.7169 - val_loss: 1.2369 - val_acc: 0.6501\n",
      "Epoch 70/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9621 - acc: 0.7133\n",
      "Epoch 00070: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 572us/sample - loss: 0.9616 - acc: 0.7122 - val_loss: 1.2856 - val_acc: 0.6288\n",
      "Epoch 71/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9526 - acc: 0.7245\n",
      "Epoch 00071: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 562us/sample - loss: 0.9517 - acc: 0.7246 - val_loss: 1.2536 - val_acc: 0.6454\n",
      "Epoch 72/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9466 - acc: 0.7231\n",
      "Epoch 00072: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 538us/sample - loss: 0.9524 - acc: 0.7216 - val_loss: 1.3361 - val_acc: 0.6241\n",
      "Epoch 73/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9664 - acc: 0.7130\n",
      "Epoch 00073: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 459us/sample - loss: 0.9643 - acc: 0.7110 - val_loss: 1.2788 - val_acc: 0.6407\n",
      "Epoch 74/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9263 - acc: 0.7446\n",
      "Epoch 00074: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 455us/sample - loss: 0.9249 - acc: 0.7459 - val_loss: 1.2783 - val_acc: 0.6430\n",
      "Epoch 75/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9706 - acc: 0.7054\n",
      "Epoch 00075: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 463us/sample - loss: 0.9639 - acc: 0.7063 - val_loss: 1.2784 - val_acc: 0.6548\n",
      "Epoch 76/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9183 - acc: 0.7372\n",
      "Epoch 00076: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 467us/sample - loss: 0.9115 - acc: 0.7394 - val_loss: 1.2855 - val_acc: 0.6596\n",
      "Epoch 77/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9167 - acc: 0.7275\n",
      "Epoch 00077: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 548us/sample - loss: 0.9275 - acc: 0.7216 - val_loss: 1.2771 - val_acc: 0.6336\n",
      "Epoch 78/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9374 - acc: 0.7232\n",
      "Epoch 00078: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 566us/sample - loss: 0.9303 - acc: 0.7264 - val_loss: 1.3671 - val_acc: 0.6359\n",
      "Epoch 79/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9124 - acc: 0.7433\n",
      "Epoch 00079: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 0.9145 - acc: 0.7429 - val_loss: 1.3370 - val_acc: 0.6336\n",
      "Epoch 80/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9238 - acc: 0.7275\n",
      "Epoch 00080: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 552us/sample - loss: 0.9255 - acc: 0.7264 - val_loss: 1.2919 - val_acc: 0.6548\n",
      "Epoch 81/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8791 - acc: 0.7356\n",
      "Epoch 00081: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 563us/sample - loss: 0.8899 - acc: 0.7311 - val_loss: 1.3600 - val_acc: 0.6501\n",
      "Epoch 82/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9041 - acc: 0.7451\n",
      "Epoch 00082: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 555us/sample - loss: 0.9070 - acc: 0.7435 - val_loss: 1.2681 - val_acc: 0.6572\n",
      "Epoch 83/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9057 - acc: 0.7428\n",
      "Epoch 00083: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 562us/sample - loss: 0.9046 - acc: 0.7429 - val_loss: 1.3356 - val_acc: 0.6359\n",
      "Epoch 84/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8971 - acc: 0.7457\n",
      "Epoch 00084: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 530us/sample - loss: 0.8949 - acc: 0.7470 - val_loss: 1.3488 - val_acc: 0.6430\n",
      "Epoch 85/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9025 - acc: 0.7457\n",
      "Epoch 00085: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 570us/sample - loss: 0.9055 - acc: 0.7417 - val_loss: 1.2830 - val_acc: 0.6430\n",
      "Epoch 86/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9146 - acc: 0.7206\n",
      "Epoch 00086: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 542us/sample - loss: 0.9095 - acc: 0.7210 - val_loss: 1.3479 - val_acc: 0.6430\n",
      "Epoch 87/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8841 - acc: 0.7596\n",
      "Epoch 00087: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 551us/sample - loss: 0.8814 - acc: 0.7600 - val_loss: 1.3007 - val_acc: 0.6596\n",
      "Epoch 88/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9190 - acc: 0.7304\n",
      "Epoch 00088: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 0.9126 - acc: 0.7340 - val_loss: 1.2774 - val_acc: 0.6525\n",
      "Epoch 89/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8711 - acc: 0.7462\n",
      "Epoch 00089: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 457us/sample - loss: 0.8736 - acc: 0.7441 - val_loss: 1.3557 - val_acc: 0.6478\n",
      "Epoch 90/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8616 - acc: 0.7494\n",
      "Epoch 00090: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 448us/sample - loss: 0.8645 - acc: 0.7470 - val_loss: 1.3681 - val_acc: 0.6643\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8770 - acc: 0.7426\n",
      "Epoch 00091: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 453us/sample - loss: 0.8747 - acc: 0.7435 - val_loss: 1.4255 - val_acc: 0.6288\n",
      "Epoch 92/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8554 - acc: 0.7506\n",
      "Epoch 00092: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 449us/sample - loss: 0.8612 - acc: 0.7488 - val_loss: 1.3391 - val_acc: 0.6572\n",
      "Epoch 93/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8863 - acc: 0.7457\n",
      "Epoch 00093: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 455us/sample - loss: 0.8834 - acc: 0.7476 - val_loss: 1.4200 - val_acc: 0.6454\n",
      "Epoch 94/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8645 - acc: 0.7583\n",
      "Epoch 00094: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 456us/sample - loss: 0.8626 - acc: 0.7577 - val_loss: 1.4286 - val_acc: 0.6359\n",
      "Epoch 95/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8338 - acc: 0.7770\n",
      "Epoch 00095: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 459us/sample - loss: 0.8385 - acc: 0.7742 - val_loss: 1.3394 - val_acc: 0.6667\n",
      "Epoch 96/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8649 - acc: 0.7431\n",
      "Epoch 00096: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 477us/sample - loss: 0.8589 - acc: 0.7447 - val_loss: 1.3858 - val_acc: 0.6430\n",
      "Epoch 97/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8681 - acc: 0.7417\n",
      "Epoch 00097: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 562us/sample - loss: 0.8669 - acc: 0.7441 - val_loss: 1.3762 - val_acc: 0.6572\n",
      "Epoch 98/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8299 - acc: 0.7698\n",
      "Epoch 00098: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 537us/sample - loss: 0.8303 - acc: 0.7707 - val_loss: 1.3882 - val_acc: 0.6407\n",
      "Epoch 99/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8460 - acc: 0.7675\n",
      "Epoch 00099: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 563us/sample - loss: 0.8495 - acc: 0.7660 - val_loss: 1.4236 - val_acc: 0.6359\n",
      "Epoch 100/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8337 - acc: 0.7555\n",
      "Epoch 00100: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 546us/sample - loss: 0.8403 - acc: 0.7541 - val_loss: 1.4369 - val_acc: 0.6548\n",
      "443/443 [==============================] - 0s 169us/sample - loss: 1.4392 - acc: 0.6027\n",
      "--- Starting trial: run-7\n",
      "{'TIME_WINDOW': 750, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 128, 'LAST_HIDDEN': 1000, 'LAST_DROPOUT': 0.3, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_93 (InputLayer)        [(None, 22, 750)]         0         \n",
      "_________________________________________________________________\n",
      "permute_92 (Permute)         (None, 750, 22)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_184 (Conv1D)          (None, 747, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_184 (Bat (None, 747, 32)           2988      \n",
      "_________________________________________________________________\n",
      "activation_184 (Activation)  (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_276 (Dropout)        (None, 747, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_184 (MaxPoolin (None, 186, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_185 (Conv1D)          (None, 183, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_185 (Bat (None, 183, 64)           732       \n",
      "_________________________________________________________________\n",
      "activation_185 (Activation)  (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_277 (Dropout)        (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_185 (MaxPoolin (None, 45, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_276 (LSTM)              (None, 45, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_277 (LSTM)              (None, 45, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_278 (LSTM)              (None, 45, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_92 (Flatten)         (None, 5760)              0         \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 1000)              5761000   \n",
      "_________________________________________________________________\n",
      "dropout_278 (Dropout)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 4)                 4004      \n",
      "=================================================================\n",
      "Total params: 6,141,812\n",
      "Trainable params: 6,139,952\n",
      "Non-trainable params: 1,860\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 3.8330 - acc: 0.3167\n",
      "Epoch 00001: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 3.8291 - acc: 0.3186 - val_loss: 3.5104 - val_acc: 0.3759\n",
      "Epoch 2/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 3.4410 - acc: 0.3918\n",
      "Epoch 00002: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 456us/sample - loss: 3.4400 - acc: 0.3907 - val_loss: 3.2566 - val_acc: 0.3712\n",
      "Epoch 3/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 3.1911 - acc: 0.4291\n",
      "Epoch 00003: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 450us/sample - loss: 3.1927 - acc: 0.4267 - val_loss: 3.0441 - val_acc: 0.4161\n",
      "Epoch 4/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.9943 - acc: 0.4315\n",
      "Epoch 00004: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 454us/sample - loss: 2.9944 - acc: 0.4320 - val_loss: 2.8646 - val_acc: 0.4208\n",
      "Epoch 5/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 2.8093 - acc: 0.4694\n",
      "Epoch 00005: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 454us/sample - loss: 2.8126 - acc: 0.4604 - val_loss: 2.7105 - val_acc: 0.4563\n",
      "Epoch 6/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.6593 - acc: 0.4633\n",
      "Epoch 00006: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 453us/sample - loss: 2.6594 - acc: 0.4604 - val_loss: 2.5879 - val_acc: 0.4326\n",
      "Epoch 7/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.5068 - acc: 0.4812\n",
      "Epoch 00007: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 457us/sample - loss: 2.5036 - acc: 0.4817 - val_loss: 2.4359 - val_acc: 0.4634\n",
      "Epoch 8/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.3856 - acc: 0.4890\n",
      "Epoch 00008: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 454us/sample - loss: 2.3808 - acc: 0.4923 - val_loss: 2.3395 - val_acc: 0.4681\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.2645 - acc: 0.5048\n",
      "Epoch 00009: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 455us/sample - loss: 2.2634 - acc: 0.5035 - val_loss: 2.1897 - val_acc: 0.4965\n",
      "Epoch 10/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.1770 - acc: 0.5043\n",
      "Epoch 00010: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 452us/sample - loss: 2.1713 - acc: 0.5065 - val_loss: 2.1229 - val_acc: 0.4894\n",
      "Epoch 11/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.0584 - acc: 0.5362\n",
      "Epoch 00011: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 450us/sample - loss: 2.0622 - acc: 0.5331 - val_loss: 2.0154 - val_acc: 0.5130\n",
      "Epoch 12/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.9833 - acc: 0.5192\n",
      "Epoch 00012: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 450us/sample - loss: 1.9807 - acc: 0.5213 - val_loss: 1.9701 - val_acc: 0.4728\n",
      "Epoch 13/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.9126 - acc: 0.5252\n",
      "Epoch 00013: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 453us/sample - loss: 1.9098 - acc: 0.5278 - val_loss: 1.8659 - val_acc: 0.5248\n",
      "Epoch 14/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.8533 - acc: 0.5276\n",
      "Epoch 00014: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 441us/sample - loss: 1.8530 - acc: 0.5284 - val_loss: 1.8115 - val_acc: 0.5343\n",
      "Epoch 15/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.7816 - acc: 0.5472\n",
      "Epoch 00015: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 464us/sample - loss: 1.7827 - acc: 0.5467 - val_loss: 1.8425 - val_acc: 0.4870\n",
      "Epoch 16/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.7334 - acc: 0.5412\n",
      "Epoch 00016: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 446us/sample - loss: 1.7399 - acc: 0.5366 - val_loss: 1.7347 - val_acc: 0.5272\n",
      "Epoch 17/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6691 - acc: 0.5539\n",
      "Epoch 00017: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 462us/sample - loss: 1.6696 - acc: 0.5520 - val_loss: 1.6830 - val_acc: 0.5177\n",
      "Epoch 18/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.6375 - acc: 0.5535\n",
      "Epoch 00018: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 453us/sample - loss: 1.6379 - acc: 0.5532 - val_loss: 1.6271 - val_acc: 0.5556\n",
      "Epoch 19/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6000 - acc: 0.5509\n",
      "Epoch 00019: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 451us/sample - loss: 1.5979 - acc: 0.5491 - val_loss: 1.6045 - val_acc: 0.5650\n",
      "Epoch 20/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5549 - acc: 0.5571\n",
      "Epoch 00020: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 446us/sample - loss: 1.5555 - acc: 0.5561 - val_loss: 1.5789 - val_acc: 0.5461\n",
      "Epoch 21/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.5174 - acc: 0.5853\n",
      "Epoch 00021: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 447us/sample - loss: 1.5096 - acc: 0.5869 - val_loss: 1.5652 - val_acc: 0.5272\n",
      "Epoch 22/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4906 - acc: 0.5697\n",
      "Epoch 00022: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 460us/sample - loss: 1.4912 - acc: 0.5674 - val_loss: 1.6056 - val_acc: 0.5106\n",
      "Epoch 23/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4617 - acc: 0.5702\n",
      "Epoch 00023: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 452us/sample - loss: 1.4583 - acc: 0.5686 - val_loss: 1.4785 - val_acc: 0.5556\n",
      "Epoch 24/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.4103 - acc: 0.5919\n",
      "Epoch 00024: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 456us/sample - loss: 1.4178 - acc: 0.5863 - val_loss: 1.5086 - val_acc: 0.5343\n",
      "Epoch 25/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4255 - acc: 0.5680\n",
      "Epoch 00025: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 444us/sample - loss: 1.4238 - acc: 0.5686 - val_loss: 1.4238 - val_acc: 0.5887\n",
      "Epoch 26/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3812 - acc: 0.5950\n",
      "Epoch 00026: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 460us/sample - loss: 1.3813 - acc: 0.5934 - val_loss: 1.4214 - val_acc: 0.5934\n",
      "Epoch 27/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3484 - acc: 0.6016\n",
      "Epoch 00027: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 452us/sample - loss: 1.3501 - acc: 0.5993 - val_loss: 1.4113 - val_acc: 0.5934\n",
      "Epoch 28/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3309 - acc: 0.5944\n",
      "Epoch 00028: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 472us/sample - loss: 1.3373 - acc: 0.5922 - val_loss: 1.4099 - val_acc: 0.5626\n",
      "Epoch 29/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3373 - acc: 0.5950\n",
      "Epoch 00029: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 457us/sample - loss: 1.3278 - acc: 0.5999 - val_loss: 1.3965 - val_acc: 0.5556\n",
      "Epoch 30/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.2981 - acc: 0.6152\n",
      "Epoch 00030: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 450us/sample - loss: 1.2963 - acc: 0.6129 - val_loss: 1.3923 - val_acc: 0.5745\n",
      "Epoch 31/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2722 - acc: 0.6069\n",
      "Epoch 00031: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 450us/sample - loss: 1.2794 - acc: 0.6064 - val_loss: 1.3680 - val_acc: 0.5697\n",
      "Epoch 32/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2688 - acc: 0.5950\n",
      "Epoch 00032: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 457us/sample - loss: 1.2661 - acc: 0.5969 - val_loss: 1.3942 - val_acc: 0.5532\n",
      "Epoch 33/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2502 - acc: 0.6213\n",
      "Epoch 00033: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 464us/sample - loss: 1.2498 - acc: 0.6217 - val_loss: 1.3765 - val_acc: 0.5603\n",
      "Epoch 34/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2351 - acc: 0.6135\n",
      "Epoch 00034: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 455us/sample - loss: 1.2375 - acc: 0.6123 - val_loss: 1.3473 - val_acc: 0.5579\n",
      "Epoch 35/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2275 - acc: 0.6250\n",
      "Epoch 00035: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 448us/sample - loss: 1.2257 - acc: 0.6259 - val_loss: 1.3002 - val_acc: 0.6099\n",
      "Epoch 36/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2543 - acc: 0.6040\n",
      "Epoch 00036: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 453us/sample - loss: 1.2471 - acc: 0.6087 - val_loss: 1.2885 - val_acc: 0.6099\n",
      "Epoch 37/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1992 - acc: 0.6354\n",
      "Epoch 00037: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 449us/sample - loss: 1.1960 - acc: 0.6342 - val_loss: 1.2997 - val_acc: 0.6005\n",
      "Epoch 38/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1895 - acc: 0.6298\n",
      "Epoch 00038: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 451us/sample - loss: 1.1894 - acc: 0.6312 - val_loss: 1.2950 - val_acc: 0.6170\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1752 - acc: 0.6450\n",
      "Epoch 00039: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 451us/sample - loss: 1.1781 - acc: 0.6413 - val_loss: 1.2773 - val_acc: 0.6147\n",
      "Epoch 40/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1524 - acc: 0.6496\n",
      "Epoch 00040: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 459us/sample - loss: 1.1505 - acc: 0.6501 - val_loss: 1.2477 - val_acc: 0.6194\n",
      "Epoch 41/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1724 - acc: 0.6294\n",
      "Epoch 00041: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 463us/sample - loss: 1.1679 - acc: 0.6300 - val_loss: 1.2551 - val_acc: 0.6265\n",
      "Epoch 42/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1570 - acc: 0.6460\n",
      "Epoch 00042: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 460us/sample - loss: 1.1545 - acc: 0.6483 - val_loss: 1.2544 - val_acc: 0.6076\n",
      "Epoch 43/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1382 - acc: 0.6472\n",
      "Epoch 00043: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 447us/sample - loss: 1.1344 - acc: 0.6489 - val_loss: 1.2917 - val_acc: 0.5887\n",
      "Epoch 44/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1170 - acc: 0.6524\n",
      "Epoch 00044: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 455us/sample - loss: 1.1199 - acc: 0.6501 - val_loss: 1.2838 - val_acc: 0.5839\n",
      "Epoch 45/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1281 - acc: 0.6490\n",
      "Epoch 00045: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 452us/sample - loss: 1.1235 - acc: 0.6513 - val_loss: 1.3164 - val_acc: 0.5721\n",
      "Epoch 46/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1259 - acc: 0.6508\n",
      "Epoch 00046: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 450us/sample - loss: 1.1237 - acc: 0.6513 - val_loss: 1.2998 - val_acc: 0.5745\n",
      "Epoch 47/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0899 - acc: 0.6737\n",
      "Epoch 00047: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 462us/sample - loss: 1.0997 - acc: 0.6726 - val_loss: 1.3749 - val_acc: 0.5697\n",
      "Epoch 48/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1103 - acc: 0.6456\n",
      "Epoch 00048: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 464us/sample - loss: 1.1077 - acc: 0.6478 - val_loss: 1.2955 - val_acc: 0.5768\n",
      "Epoch 49/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0631 - acc: 0.6856\n",
      "Epoch 00049: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 461us/sample - loss: 1.0645 - acc: 0.6862 - val_loss: 1.2328 - val_acc: 0.6525\n",
      "Epoch 50/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0837 - acc: 0.6735\n",
      "Epoch 00050: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 455us/sample - loss: 1.0787 - acc: 0.6761 - val_loss: 1.2701 - val_acc: 0.5745\n",
      "Epoch 51/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0792 - acc: 0.6741\n",
      "Epoch 00051: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 453us/sample - loss: 1.0783 - acc: 0.6743 - val_loss: 1.2296 - val_acc: 0.6241\n",
      "Epoch 52/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0793 - acc: 0.6728\n",
      "Epoch 00052: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 455us/sample - loss: 1.0765 - acc: 0.6761 - val_loss: 1.2670 - val_acc: 0.6052\n",
      "Epoch 53/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0372 - acc: 0.6787\n",
      "Epoch 00053: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 448us/sample - loss: 1.0416 - acc: 0.6779 - val_loss: 1.2753 - val_acc: 0.6076\n",
      "Epoch 54/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0643 - acc: 0.6728\n",
      "Epoch 00054: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 448us/sample - loss: 1.0568 - acc: 0.6755 - val_loss: 1.2303 - val_acc: 0.6123\n",
      "Epoch 55/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0476 - acc: 0.6839\n",
      "Epoch 00055: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 457us/sample - loss: 1.0473 - acc: 0.6862 - val_loss: 1.2877 - val_acc: 0.6123\n",
      "Epoch 56/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0364 - acc: 0.6850\n",
      "Epoch 00056: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 447us/sample - loss: 1.0454 - acc: 0.6814 - val_loss: 1.2112 - val_acc: 0.6430\n",
      "Epoch 57/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.0325 - acc: 0.6953\n",
      "Epoch 00057: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 451us/sample - loss: 1.0429 - acc: 0.6874 - val_loss: 1.2334 - val_acc: 0.6359\n",
      "Epoch 58/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0211 - acc: 0.6894\n",
      "Epoch 00058: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 480us/sample - loss: 1.0206 - acc: 0.6933 - val_loss: 1.2367 - val_acc: 0.6194\n",
      "Epoch 59/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0102 - acc: 0.6936\n",
      "Epoch 00059: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 524us/sample - loss: 1.0107 - acc: 0.6933 - val_loss: 1.3251 - val_acc: 0.5887\n",
      "Epoch 60/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0226 - acc: 0.6801\n",
      "Epoch 00060: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 539us/sample - loss: 1.0272 - acc: 0.6767 - val_loss: 1.2564 - val_acc: 0.6147\n",
      "Epoch 61/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0170 - acc: 0.6832\n",
      "Epoch 00061: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 568us/sample - loss: 1.0180 - acc: 0.6820 - val_loss: 1.1960 - val_acc: 0.6572\n",
      "Epoch 62/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0190 - acc: 0.6862\n",
      "Epoch 00062: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 560us/sample - loss: 1.0092 - acc: 0.6915 - val_loss: 1.2261 - val_acc: 0.6454\n",
      "Epoch 63/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9905 - acc: 0.7114\n",
      "Epoch 00063: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 0.9893 - acc: 0.7104 - val_loss: 1.2709 - val_acc: 0.6005\n",
      "Epoch 64/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9685 - acc: 0.7126\n",
      "Epoch 00064: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 556us/sample - loss: 0.9699 - acc: 0.7122 - val_loss: 1.2566 - val_acc: 0.6288\n",
      "Epoch 65/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0075 - acc: 0.6856\n",
      "Epoch 00065: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 567us/sample - loss: 1.0048 - acc: 0.6868 - val_loss: 1.2800 - val_acc: 0.6099\n",
      "Epoch 66/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9807 - acc: 0.6939\n",
      "Epoch 00066: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 564us/sample - loss: 0.9850 - acc: 0.6962 - val_loss: 1.2540 - val_acc: 0.6170\n",
      "Epoch 67/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9660 - acc: 0.7181\n",
      "Epoch 00067: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 579us/sample - loss: 0.9596 - acc: 0.7210 - val_loss: 1.2320 - val_acc: 0.6336\n",
      "Epoch 68/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9853 - acc: 0.7121\n",
      "Epoch 00068: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 547us/sample - loss: 0.9829 - acc: 0.7122 - val_loss: 1.2632 - val_acc: 0.6336\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9794 - acc: 0.7119\n",
      "Epoch 00069: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 483us/sample - loss: 0.9867 - acc: 0.7057 - val_loss: 1.2579 - val_acc: 0.6265\n",
      "Epoch 70/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9593 - acc: 0.7113\n",
      "Epoch 00070: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 458us/sample - loss: 0.9632 - acc: 0.7098 - val_loss: 1.2537 - val_acc: 0.6123\n",
      "Epoch 71/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9631 - acc: 0.7226\n",
      "Epoch 00071: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 460us/sample - loss: 0.9636 - acc: 0.7187 - val_loss: 1.2180 - val_acc: 0.6265\n",
      "Epoch 72/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9631 - acc: 0.7091\n",
      "Epoch 00072: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 460us/sample - loss: 0.9623 - acc: 0.7098 - val_loss: 1.2051 - val_acc: 0.6265\n",
      "Epoch 73/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9383 - acc: 0.7200\n",
      "Epoch 00073: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 450us/sample - loss: 0.9387 - acc: 0.7199 - val_loss: 1.2661 - val_acc: 0.6312\n",
      "Epoch 74/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9302 - acc: 0.7328\n",
      "Epoch 00074: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 461us/sample - loss: 0.9250 - acc: 0.7352 - val_loss: 1.2451 - val_acc: 0.6359\n",
      "Epoch 75/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9450 - acc: 0.7213\n",
      "Epoch 00075: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 454us/sample - loss: 0.9415 - acc: 0.7216 - val_loss: 1.2508 - val_acc: 0.6288\n",
      "Epoch 76/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9182 - acc: 0.7326\n",
      "Epoch 00076: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 450us/sample - loss: 0.9161 - acc: 0.7340 - val_loss: 1.2711 - val_acc: 0.6407\n",
      "Epoch 77/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9472 - acc: 0.7144\n",
      "Epoch 00077: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 458us/sample - loss: 0.9515 - acc: 0.7116 - val_loss: 1.1953 - val_acc: 0.6501\n",
      "Epoch 78/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9306 - acc: 0.7213\n",
      "Epoch 00078: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 462us/sample - loss: 0.9243 - acc: 0.7264 - val_loss: 1.2849 - val_acc: 0.6147\n",
      "Epoch 79/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9390 - acc: 0.7194\n",
      "Epoch 00079: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 462us/sample - loss: 0.9392 - acc: 0.7204 - val_loss: 1.3024 - val_acc: 0.6147\n",
      "Epoch 80/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9240 - acc: 0.7243\n",
      "Epoch 00080: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 449us/sample - loss: 0.9274 - acc: 0.7228 - val_loss: 1.2577 - val_acc: 0.6312\n",
      "Epoch 81/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8829 - acc: 0.7469\n",
      "Epoch 00081: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 443us/sample - loss: 0.8861 - acc: 0.7453 - val_loss: 1.2891 - val_acc: 0.6478\n",
      "Epoch 82/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9132 - acc: 0.7279\n",
      "Epoch 00082: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 458us/sample - loss: 0.9109 - acc: 0.7287 - val_loss: 1.2479 - val_acc: 0.6478\n",
      "Epoch 83/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9119 - acc: 0.7353\n",
      "Epoch 00083: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 460us/sample - loss: 0.9073 - acc: 0.7382 - val_loss: 1.2307 - val_acc: 0.6288\n",
      "Epoch 84/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8766 - acc: 0.7387\n",
      "Epoch 00084: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 468us/sample - loss: 0.8779 - acc: 0.7400 - val_loss: 1.2844 - val_acc: 0.6312\n",
      "Epoch 85/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8787 - acc: 0.7560\n",
      "Epoch 00085: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 455us/sample - loss: 0.8795 - acc: 0.7553 - val_loss: 1.2826 - val_acc: 0.6359\n",
      "Epoch 86/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9253 - acc: 0.7145\n",
      "Epoch 00086: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 455us/sample - loss: 0.9256 - acc: 0.7145 - val_loss: 1.2529 - val_acc: 0.6430\n",
      "Epoch 87/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8772 - acc: 0.7394\n",
      "Epoch 00087: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 458us/sample - loss: 0.8791 - acc: 0.7400 - val_loss: 1.3270 - val_acc: 0.6312\n",
      "Epoch 88/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9000 - acc: 0.7450\n",
      "Epoch 00088: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 456us/sample - loss: 0.8904 - acc: 0.7476 - val_loss: 1.2667 - val_acc: 0.6407\n",
      "Epoch 89/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8464 - acc: 0.7531\n",
      "Epoch 00089: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 457us/sample - loss: 0.8433 - acc: 0.7565 - val_loss: 1.3139 - val_acc: 0.6383\n",
      "Epoch 90/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8715 - acc: 0.7518\n",
      "Epoch 00090: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 464us/sample - loss: 0.8765 - acc: 0.7500 - val_loss: 1.3647 - val_acc: 0.6265\n",
      "Epoch 91/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8948 - acc: 0.7200\n",
      "Epoch 00091: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 456us/sample - loss: 0.8968 - acc: 0.7193 - val_loss: 1.2584 - val_acc: 0.6548\n",
      "Epoch 92/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8728 - acc: 0.7539\n",
      "Epoch 00092: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 450us/sample - loss: 0.8619 - acc: 0.7589 - val_loss: 1.2990 - val_acc: 0.6430\n",
      "Epoch 93/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8631 - acc: 0.7494\n",
      "Epoch 00093: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 458us/sample - loss: 0.8667 - acc: 0.7482 - val_loss: 1.3021 - val_acc: 0.6501\n",
      "Epoch 94/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8841 - acc: 0.7356\n",
      "Epoch 00094: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 461us/sample - loss: 0.8835 - acc: 0.7388 - val_loss: 1.3048 - val_acc: 0.6336\n",
      "Epoch 95/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8679 - acc: 0.7380\n",
      "Epoch 00095: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 450us/sample - loss: 0.8651 - acc: 0.7388 - val_loss: 1.1996 - val_acc: 0.6596\n",
      "Epoch 96/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8747 - acc: 0.7455\n",
      "Epoch 00096: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 456us/sample - loss: 0.8688 - acc: 0.7506 - val_loss: 1.2751 - val_acc: 0.6761\n",
      "Epoch 97/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 0.8590 - acc: 0.7539\n",
      "Epoch 00097: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 447us/sample - loss: 0.8496 - acc: 0.7595 - val_loss: 1.2876 - val_acc: 0.6454\n",
      "Epoch 98/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8325 - acc: 0.7694\n",
      "Epoch 00098: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 450us/sample - loss: 0.8397 - acc: 0.7660 - val_loss: 1.3151 - val_acc: 0.6761\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8293 - acc: 0.7594\n",
      "Epoch 00099: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 456us/sample - loss: 0.8249 - acc: 0.7600 - val_loss: 1.3150 - val_acc: 0.6454\n",
      "Epoch 100/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8308 - acc: 0.7727\n",
      "Epoch 00100: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 464us/sample - loss: 0.8337 - acc: 0.7713 - val_loss: 1.2694 - val_acc: 0.6525\n",
      "443/443 [==============================] - 0s 177us/sample - loss: 1.3424 - acc: 0.6230\n",
      "Training data shape with slices: (1692, 22, 1000)\n",
      "Training label shape with slice: (1692,)\n",
      "Validation data shape with slices: (423, 22, 1000)\n",
      "Validation label shape with slice: (423,)\n",
      "Testing data shape with slices: (443, 22, 1000)\n",
      "Testing label shape with slice: (443,)\n",
      "--- Starting trial: run-8\n",
      "{'TIME_WINDOW': 1000, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 64, 'LAST_HIDDEN': 500, 'LAST_DROPOUT': 0.2, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_94 (InputLayer)        [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute_93 (Permute)         (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_186 (Conv1D)          (None, 997, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_186 (Bat (None, 997, 32)           3988      \n",
      "_________________________________________________________________\n",
      "activation_186 (Activation)  (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_279 (Dropout)        (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_186 (MaxPoolin (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_187 (Conv1D)          (None, 246, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_187 (Bat (None, 246, 64)           984       \n",
      "_________________________________________________________________\n",
      "activation_187 (Activation)  (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_280 (Dropout)        (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_187 (MaxPoolin (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_279 (LSTM)              (None, 61, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_280 (LSTM)              (None, 61, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_281 (LSTM)              (None, 61, 64)            49408     \n",
      "_________________________________________________________________\n",
      "flatten_93 (Flatten)         (None, 3904)              0         \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 500)               1952500   \n",
      "_________________________________________________________________\n",
      "dropout_281 (Dropout)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 4)                 2004      \n",
      "=================================================================\n",
      "Total params: 2,250,388\n",
      "Trainable params: 2,247,902\n",
      "Non-trainable params: 2,486\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 3.1695 - acc: 0.2722\n",
      "Epoch 00001: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 4s 3ms/sample - loss: 3.1645 - acc: 0.2754 - val_loss: 2.8528 - val_acc: 0.3664\n",
      "Epoch 2/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.8131 - acc: 0.3269\n",
      "Epoch 00002: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 531us/sample - loss: 2.8121 - acc: 0.3268 - val_loss: 2.6364 - val_acc: 0.3901\n",
      "Epoch 3/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.6569 - acc: 0.3756\n",
      "Epoch 00003: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 615us/sample - loss: 2.6561 - acc: 0.3741 - val_loss: 2.5375 - val_acc: 0.3972\n",
      "Epoch 4/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.5400 - acc: 0.3811\n",
      "Epoch 00004: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 616us/sample - loss: 2.5368 - acc: 0.3830 - val_loss: 2.4204 - val_acc: 0.4161\n",
      "Epoch 5/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.4312 - acc: 0.4150\n",
      "Epoch 00005: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 611us/sample - loss: 2.4336 - acc: 0.4149 - val_loss: 2.3228 - val_acc: 0.4586\n",
      "Epoch 6/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.3493 - acc: 0.4339\n",
      "Epoch 00006: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 623us/sample - loss: 2.3517 - acc: 0.4309 - val_loss: 2.2319 - val_acc: 0.4657\n",
      "Epoch 7/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.2720 - acc: 0.4350\n",
      "Epoch 00007: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 613us/sample - loss: 2.2680 - acc: 0.4403 - val_loss: 2.1684 - val_acc: 0.4421\n",
      "Epoch 8/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.1860 - acc: 0.4471\n",
      "Epoch 00008: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 604us/sample - loss: 2.1809 - acc: 0.4509 - val_loss: 2.1096 - val_acc: 0.4610\n",
      "Epoch 9/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.1104 - acc: 0.4718\n",
      "Epoch 00009: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 623us/sample - loss: 2.1120 - acc: 0.4722 - val_loss: 2.0150 - val_acc: 0.5012\n",
      "Epoch 10/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.0433 - acc: 0.4850\n",
      "Epoch 00010: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 595us/sample - loss: 2.0410 - acc: 0.4858 - val_loss: 1.9727 - val_acc: 0.4917\n",
      "Epoch 11/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.9766 - acc: 0.4914\n",
      "Epoch 00011: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 616us/sample - loss: 1.9803 - acc: 0.4900 - val_loss: 1.9230 - val_acc: 0.4965\n",
      "Epoch 12/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.9393 - acc: 0.4792\n",
      "Epoch 00012: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 1.9410 - acc: 0.4787 - val_loss: 1.8842 - val_acc: 0.4704\n",
      "Epoch 13/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.8700 - acc: 0.5108\n",
      "Epoch 00013: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 461us/sample - loss: 1.8684 - acc: 0.5100 - val_loss: 1.8386 - val_acc: 0.4799\n",
      "Epoch 14/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.8195 - acc: 0.5153\n",
      "Epoch 00014: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 474us/sample - loss: 1.8182 - acc: 0.5171 - val_loss: 1.8332 - val_acc: 0.4728\n",
      "Epoch 15/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.7818 - acc: 0.5245\n",
      "Epoch 00015: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 468us/sample - loss: 1.7852 - acc: 0.5225 - val_loss: 1.7621 - val_acc: 0.5083\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7219 - acc: 0.5505\n",
      "Epoch 00016: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 463us/sample - loss: 1.7242 - acc: 0.5491 - val_loss: 1.7214 - val_acc: 0.5106\n",
      "Epoch 17/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.6920 - acc: 0.5349\n",
      "Epoch 00017: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 456us/sample - loss: 1.6935 - acc: 0.5343 - val_loss: 1.6809 - val_acc: 0.5177\n",
      "Epoch 18/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.6621 - acc: 0.5362\n",
      "Epoch 00018: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 465us/sample - loss: 1.6560 - acc: 0.5420 - val_loss: 1.6543 - val_acc: 0.5296\n",
      "Epoch 19/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.6240 - acc: 0.5463\n",
      "Epoch 00019: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 471us/sample - loss: 1.6214 - acc: 0.5491 - val_loss: 1.6350 - val_acc: 0.5508\n",
      "Epoch 20/100\n",
      "1536/1692 [==========================>...] - ETA: 0s - loss: 1.6079 - acc: 0.5475\n",
      "Epoch 00020: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 470us/sample - loss: 1.6068 - acc: 0.5467 - val_loss: 1.6185 - val_acc: 0.5225\n",
      "Epoch 21/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.5327 - acc: 0.5906\n",
      "Epoch 00021: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 460us/sample - loss: 1.5344 - acc: 0.5875 - val_loss: 1.5802 - val_acc: 0.5414\n",
      "Epoch 22/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.5203 - acc: 0.5754\n",
      "Epoch 00022: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 469us/sample - loss: 1.5218 - acc: 0.5733 - val_loss: 1.6146 - val_acc: 0.5154\n",
      "Epoch 23/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.4823 - acc: 0.5869\n",
      "Epoch 00023: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 474us/sample - loss: 1.4860 - acc: 0.5851 - val_loss: 1.5550 - val_acc: 0.5556\n",
      "Epoch 24/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4851 - acc: 0.5682\n",
      "Epoch 00024: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 448us/sample - loss: 1.4739 - acc: 0.5768 - val_loss: 1.5934 - val_acc: 0.5154\n",
      "Epoch 25/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4536 - acc: 0.5827\n",
      "Epoch 00025: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 474us/sample - loss: 1.4530 - acc: 0.5810 - val_loss: 1.5200 - val_acc: 0.5792\n",
      "Epoch 26/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.4258 - acc: 0.5925\n",
      "Epoch 00026: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 477us/sample - loss: 1.4241 - acc: 0.5922 - val_loss: 1.5027 - val_acc: 0.5485\n",
      "Epoch 27/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4135 - acc: 0.5829\n",
      "Epoch 00027: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 476us/sample - loss: 1.4142 - acc: 0.5822 - val_loss: 1.4991 - val_acc: 0.5414\n",
      "Epoch 28/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3920 - acc: 0.5989\n",
      "Epoch 00028: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 464us/sample - loss: 1.3951 - acc: 0.5981 - val_loss: 1.4577 - val_acc: 0.5626\n",
      "Epoch 29/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3723 - acc: 0.5969\n",
      "Epoch 00029: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 476us/sample - loss: 1.3695 - acc: 0.6011 - val_loss: 1.4806 - val_acc: 0.5603\n",
      "Epoch 30/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3479 - acc: 0.5980\n",
      "Epoch 00030: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 470us/sample - loss: 1.3444 - acc: 0.5993 - val_loss: 1.3897 - val_acc: 0.5745\n",
      "Epoch 31/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3350 - acc: 0.6094\n",
      "Epoch 00031: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 484us/sample - loss: 1.3369 - acc: 0.6064 - val_loss: 1.4866 - val_acc: 0.5390\n",
      "Epoch 32/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3235 - acc: 0.6158\n",
      "Epoch 00032: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 467us/sample - loss: 1.3222 - acc: 0.6188 - val_loss: 1.4317 - val_acc: 0.5532\n",
      "Epoch 33/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3056 - acc: 0.6014\n",
      "Epoch 00033: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 457us/sample - loss: 1.3030 - acc: 0.6028 - val_loss: 1.3917 - val_acc: 0.5768\n",
      "Epoch 34/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2647 - acc: 0.6369\n",
      "Epoch 00034: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 478us/sample - loss: 1.2651 - acc: 0.6353 - val_loss: 1.4063 - val_acc: 0.5816\n",
      "Epoch 35/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2789 - acc: 0.6112\n",
      "Epoch 00035: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 464us/sample - loss: 1.2764 - acc: 0.6129 - val_loss: 1.3562 - val_acc: 0.6099\n",
      "Epoch 36/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2602 - acc: 0.6300\n",
      "Epoch 00036: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 474us/sample - loss: 1.2583 - acc: 0.6336 - val_loss: 1.4581 - val_acc: 0.5603\n",
      "Epoch 37/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2369 - acc: 0.6346\n",
      "Epoch 00037: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 481us/sample - loss: 1.2371 - acc: 0.6318 - val_loss: 1.3635 - val_acc: 0.6028\n",
      "Epoch 38/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2290 - acc: 0.6369\n",
      "Epoch 00038: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 473us/sample - loss: 1.2249 - acc: 0.6418 - val_loss: 1.3523 - val_acc: 0.5910\n",
      "Epoch 39/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2051 - acc: 0.6520\n",
      "Epoch 00039: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 463us/sample - loss: 1.2043 - acc: 0.6531 - val_loss: 1.3632 - val_acc: 0.5745\n",
      "Epoch 40/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1962 - acc: 0.6400\n",
      "Epoch 00040: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 471us/sample - loss: 1.1955 - acc: 0.6418 - val_loss: 1.3581 - val_acc: 0.5934\n",
      "Epoch 41/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1723 - acc: 0.6665\n",
      "Epoch 00041: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 474us/sample - loss: 1.1778 - acc: 0.6637 - val_loss: 1.3540 - val_acc: 0.6028\n",
      "Epoch 42/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1653 - acc: 0.6486\n",
      "Epoch 00042: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 468us/sample - loss: 1.1675 - acc: 0.6454 - val_loss: 1.2961 - val_acc: 0.6217\n",
      "Epoch 43/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1615 - acc: 0.6456\n",
      "Epoch 00043: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 478us/sample - loss: 1.1621 - acc: 0.6454 - val_loss: 1.4001 - val_acc: 0.5745\n",
      "Epoch 44/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1398 - acc: 0.6617\n",
      "Epoch 00044: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 519us/sample - loss: 1.1454 - acc: 0.6596 - val_loss: 1.2711 - val_acc: 0.6288\n",
      "Epoch 45/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1435 - acc: 0.6594\n",
      "Epoch 00045: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 609us/sample - loss: 1.1400 - acc: 0.6619 - val_loss: 1.3692 - val_acc: 0.5934\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1270 - acc: 0.6601\n",
      "Epoch 00046: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 609us/sample - loss: 1.1269 - acc: 0.6637 - val_loss: 1.2847 - val_acc: 0.6147\n",
      "Epoch 47/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1099 - acc: 0.6728\n",
      "Epoch 00047: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 615us/sample - loss: 1.1136 - acc: 0.6702 - val_loss: 1.2719 - val_acc: 0.6147\n",
      "Epoch 48/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1166 - acc: 0.6814\n",
      "Epoch 00048: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 608us/sample - loss: 1.1114 - acc: 0.6850 - val_loss: 1.2805 - val_acc: 0.6241\n",
      "Epoch 49/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0830 - acc: 0.6779\n",
      "Epoch 00049: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 596us/sample - loss: 1.0862 - acc: 0.6785 - val_loss: 1.3381 - val_acc: 0.6005\n",
      "Epoch 50/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0937 - acc: 0.6665\n",
      "Epoch 00050: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 607us/sample - loss: 1.0934 - acc: 0.6678 - val_loss: 1.3204 - val_acc: 0.5957\n",
      "Epoch 51/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0784 - acc: 0.6737\n",
      "Epoch 00051: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 626us/sample - loss: 1.0739 - acc: 0.6773 - val_loss: 1.2908 - val_acc: 0.6336\n",
      "Epoch 52/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0743 - acc: 0.6917\n",
      "Epoch 00052: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 606us/sample - loss: 1.0747 - acc: 0.6897 - val_loss: 1.2975 - val_acc: 0.6288\n",
      "Epoch 53/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0464 - acc: 0.7007\n",
      "Epoch 00053: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 596us/sample - loss: 1.0474 - acc: 0.7004 - val_loss: 1.3333 - val_acc: 0.6147\n",
      "Epoch 54/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0574 - acc: 0.6977\n",
      "Epoch 00054: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 580us/sample - loss: 1.0547 - acc: 0.6939 - val_loss: 1.3180 - val_acc: 0.6170\n",
      "Epoch 55/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0435 - acc: 0.6863\n",
      "Epoch 00055: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 633us/sample - loss: 1.0398 - acc: 0.6874 - val_loss: 1.2612 - val_acc: 0.6383\n",
      "Epoch 56/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0229 - acc: 0.7019\n",
      "Epoch 00056: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 576us/sample - loss: 1.0232 - acc: 0.6998 - val_loss: 1.2807 - val_acc: 0.6501\n",
      "Epoch 57/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0299 - acc: 0.6941\n",
      "Epoch 00057: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 604us/sample - loss: 1.0274 - acc: 0.6950 - val_loss: 1.2905 - val_acc: 0.6454\n",
      "Epoch 58/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0356 - acc: 0.6862\n",
      "Epoch 00058: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 617us/sample - loss: 1.0342 - acc: 0.6868 - val_loss: 1.2773 - val_acc: 0.6454\n",
      "Epoch 59/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0181 - acc: 0.6924\n",
      "Epoch 00059: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 567us/sample - loss: 1.0197 - acc: 0.6915 - val_loss: 1.3195 - val_acc: 0.6359\n",
      "Epoch 60/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0221 - acc: 0.6906\n",
      "Epoch 00060: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 601us/sample - loss: 1.0218 - acc: 0.6903 - val_loss: 1.3588 - val_acc: 0.6099\n",
      "Epoch 61/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0071 - acc: 0.7073\n",
      "Epoch 00061: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 620us/sample - loss: 1.0053 - acc: 0.7074 - val_loss: 1.2577 - val_acc: 0.6501\n",
      "Epoch 62/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9726 - acc: 0.7270\n",
      "Epoch 00062: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 566us/sample - loss: 0.9811 - acc: 0.7258 - val_loss: 1.3427 - val_acc: 0.6170\n",
      "Epoch 63/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0048 - acc: 0.7096\n",
      "Epoch 00063: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 466us/sample - loss: 1.0070 - acc: 0.7074 - val_loss: 1.3188 - val_acc: 0.6217\n",
      "Epoch 64/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9905 - acc: 0.7053\n",
      "Epoch 00064: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 469us/sample - loss: 0.9854 - acc: 0.7074 - val_loss: 1.3689 - val_acc: 0.6005\n",
      "Epoch 65/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9712 - acc: 0.7181\n",
      "Epoch 00065: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 476us/sample - loss: 0.9805 - acc: 0.7145 - val_loss: 1.3293 - val_acc: 0.6336\n",
      "Epoch 66/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9606 - acc: 0.7250\n",
      "Epoch 00066: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 479us/sample - loss: 0.9607 - acc: 0.7252 - val_loss: 1.3302 - val_acc: 0.6217\n",
      "Epoch 67/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9778 - acc: 0.7212\n",
      "Epoch 00067: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 472us/sample - loss: 0.9791 - acc: 0.7216 - val_loss: 1.2978 - val_acc: 0.6359\n",
      "Epoch 68/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9480 - acc: 0.7319\n",
      "Epoch 00068: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 475us/sample - loss: 0.9531 - acc: 0.7275 - val_loss: 1.2856 - val_acc: 0.6288\n",
      "Epoch 69/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9564 - acc: 0.7194\n",
      "Epoch 00069: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 476us/sample - loss: 0.9614 - acc: 0.7187 - val_loss: 1.2705 - val_acc: 0.6407\n",
      "Epoch 70/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9362 - acc: 0.7163\n",
      "Epoch 00070: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 471us/sample - loss: 0.9365 - acc: 0.7187 - val_loss: 1.2752 - val_acc: 0.6359\n",
      "Epoch 71/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9280 - acc: 0.7381\n",
      "Epoch 00071: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 471us/sample - loss: 0.9344 - acc: 0.7370 - val_loss: 1.2952 - val_acc: 0.6359\n",
      "Epoch 72/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9379 - acc: 0.7325\n",
      "Epoch 00072: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 477us/sample - loss: 0.9412 - acc: 0.7317 - val_loss: 1.2487 - val_acc: 0.6525\n",
      "Epoch 73/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9233 - acc: 0.7321\n",
      "Epoch 00073: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 476us/sample - loss: 0.9211 - acc: 0.7329 - val_loss: 1.3322 - val_acc: 0.6336\n",
      "Epoch 74/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9102 - acc: 0.7449\n",
      "Epoch 00074: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 483us/sample - loss: 0.9180 - acc: 0.7400 - val_loss: 1.3538 - val_acc: 0.6383\n",
      "Epoch 75/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9304 - acc: 0.7312\n",
      "Epoch 00075: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 478us/sample - loss: 0.9346 - acc: 0.7305 - val_loss: 1.2685 - val_acc: 0.6738\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9180 - acc: 0.7481\n",
      "Epoch 00076: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 452us/sample - loss: 0.9223 - acc: 0.7447 - val_loss: 1.2957 - val_acc: 0.6312\n",
      "Epoch 77/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9064 - acc: 0.7437\n",
      "Epoch 00077: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 469us/sample - loss: 0.9092 - acc: 0.7423 - val_loss: 1.3231 - val_acc: 0.6383\n",
      "Epoch 78/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9159 - acc: 0.7328\n",
      "Epoch 00078: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 466us/sample - loss: 0.9139 - acc: 0.7346 - val_loss: 1.2495 - val_acc: 0.6501\n",
      "Epoch 79/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8981 - acc: 0.7396\n",
      "Epoch 00079: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 463us/sample - loss: 0.8970 - acc: 0.7405 - val_loss: 1.3019 - val_acc: 0.6407\n",
      "Epoch 80/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8660 - acc: 0.7548\n",
      "Epoch 00080: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 472us/sample - loss: 0.8726 - acc: 0.7518 - val_loss: 1.2880 - val_acc: 0.6430\n",
      "Epoch 81/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8772 - acc: 0.7532\n",
      "Epoch 00081: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 470us/sample - loss: 0.8863 - acc: 0.7482 - val_loss: 1.3295 - val_acc: 0.6548\n",
      "Epoch 82/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8816 - acc: 0.7500\n",
      "Epoch 00082: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 462us/sample - loss: 0.8916 - acc: 0.7453 - val_loss: 1.2961 - val_acc: 0.6430\n",
      "Epoch 83/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8647 - acc: 0.7656\n",
      "Epoch 00083: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 461us/sample - loss: 0.8677 - acc: 0.7660 - val_loss: 1.3519 - val_acc: 0.6596\n",
      "Epoch 84/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8640 - acc: 0.7626\n",
      "Epoch 00084: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 462us/sample - loss: 0.8667 - acc: 0.7618 - val_loss: 1.3026 - val_acc: 0.6690\n",
      "Epoch 85/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8582 - acc: 0.7488\n",
      "Epoch 00085: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 472us/sample - loss: 0.8554 - acc: 0.7524 - val_loss: 1.3209 - val_acc: 0.6525\n",
      "Epoch 86/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8670 - acc: 0.7564\n",
      "Epoch 00086: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 478us/sample - loss: 0.8772 - acc: 0.7500 - val_loss: 1.3184 - val_acc: 0.6643\n",
      "Epoch 87/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8504 - acc: 0.7666\n",
      "Epoch 00087: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 573us/sample - loss: 0.8500 - acc: 0.7719 - val_loss: 1.3528 - val_acc: 0.6407\n",
      "Epoch 88/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8728 - acc: 0.7538\n",
      "Epoch 00088: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 597us/sample - loss: 0.8690 - acc: 0.7559 - val_loss: 1.3428 - val_acc: 0.6525\n",
      "Epoch 89/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8489 - acc: 0.7580\n",
      "Epoch 00089: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 587us/sample - loss: 0.8497 - acc: 0.7577 - val_loss: 1.3096 - val_acc: 0.6454\n",
      "Epoch 90/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8338 - acc: 0.7733\n",
      "Epoch 00090: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 603us/sample - loss: 0.8357 - acc: 0.7713 - val_loss: 1.3627 - val_acc: 0.6478\n",
      "Epoch 91/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8241 - acc: 0.7812\n",
      "Epoch 00091: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 584us/sample - loss: 0.8264 - acc: 0.7819 - val_loss: 1.3292 - val_acc: 0.6714\n",
      "Epoch 92/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8307 - acc: 0.7728\n",
      "Epoch 00092: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 613us/sample - loss: 0.8322 - acc: 0.7725 - val_loss: 1.3101 - val_acc: 0.6643\n",
      "Epoch 93/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8043 - acc: 0.7837\n",
      "Epoch 00093: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 606us/sample - loss: 0.8064 - acc: 0.7813 - val_loss: 1.3473 - val_acc: 0.6619\n",
      "Epoch 94/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8107 - acc: 0.7756\n",
      "Epoch 00094: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 631us/sample - loss: 0.8074 - acc: 0.7790 - val_loss: 1.3377 - val_acc: 0.6596\n",
      "Epoch 95/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8261 - acc: 0.7721\n",
      "Epoch 00095: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 614us/sample - loss: 0.8233 - acc: 0.7725 - val_loss: 1.3362 - val_acc: 0.6619\n",
      "Epoch 96/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7968 - acc: 0.7879\n",
      "Epoch 00096: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 608us/sample - loss: 0.8013 - acc: 0.7861 - val_loss: 1.3580 - val_acc: 0.6525\n",
      "Epoch 97/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8123 - acc: 0.7855\n",
      "Epoch 00097: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 614us/sample - loss: 0.8148 - acc: 0.7849 - val_loss: 1.3390 - val_acc: 0.6572\n",
      "Epoch 98/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.7941 - acc: 0.7862\n",
      "Epoch 00098: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 605us/sample - loss: 0.7934 - acc: 0.7849 - val_loss: 1.3722 - val_acc: 0.6738\n",
      "Epoch 99/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.7688 - acc: 0.8045\n",
      "Epoch 00099: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 613us/sample - loss: 0.7688 - acc: 0.8038 - val_loss: 1.3635 - val_acc: 0.6643\n",
      "Epoch 100/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.7898 - acc: 0.7812\n",
      "Epoch 00100: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 610us/sample - loss: 0.7915 - acc: 0.7807 - val_loss: 1.4114 - val_acc: 0.6478\n",
      "443/443 [==============================] - 0s 450us/sample - loss: 1.3634 - acc: 0.6456\n",
      "--- Starting trial: run-9\n",
      "{'TIME_WINDOW': 1000, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 64, 'LAST_HIDDEN': 500, 'LAST_DROPOUT': 0.3, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_95 (InputLayer)        [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute_94 (Permute)         (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_188 (Conv1D)          (None, 997, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_188 (Bat (None, 997, 32)           3988      \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_282 (Dropout)        (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_188 (MaxPoolin (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_189 (Conv1D)          (None, 246, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_189 (Bat (None, 246, 64)           984       \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_283 (Dropout)        (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_189 (MaxPoolin (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_282 (LSTM)              (None, 61, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_283 (LSTM)              (None, 61, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_284 (LSTM)              (None, 61, 64)            49408     \n",
      "_________________________________________________________________\n",
      "flatten_94 (Flatten)         (None, 3904)              0         \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (None, 500)               1952500   \n",
      "_________________________________________________________________\n",
      "dropout_284 (Dropout)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_189 (Dense)            (None, 4)                 2004      \n",
      "=================================================================\n",
      "Total params: 2,250,388\n",
      "Trainable params: 2,247,902\n",
      "Non-trainable params: 2,486\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664/1692 [============================>.] - ETA: 0s - loss: 3.0637 - acc: 0.2939\n",
      "Epoch 00001: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 3.0610 - acc: 0.2920 - val_loss: 2.7838 - val_acc: 0.3830\n",
      "Epoch 2/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.7622 - acc: 0.3618\n",
      "Epoch 00002: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 594us/sample - loss: 2.7611 - acc: 0.3605 - val_loss: 2.6137 - val_acc: 0.4161\n",
      "Epoch 3/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.6088 - acc: 0.3958\n",
      "Epoch 00003: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 601us/sample - loss: 2.6101 - acc: 0.3954 - val_loss: 2.5092 - val_acc: 0.3924\n",
      "Epoch 4/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.4998 - acc: 0.4062\n",
      "Epoch 00004: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 600us/sample - loss: 2.4982 - acc: 0.4066 - val_loss: 2.4047 - val_acc: 0.4374\n",
      "Epoch 5/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.3833 - acc: 0.4393\n",
      "Epoch 00005: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 589us/sample - loss: 2.3815 - acc: 0.4421 - val_loss: 2.3292 - val_acc: 0.4326\n",
      "Epoch 6/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.2816 - acc: 0.4663\n",
      "Epoch 00006: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 619us/sample - loss: 2.2799 - acc: 0.4675 - val_loss: 2.2530 - val_acc: 0.4444\n",
      "Epoch 7/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.1998 - acc: 0.4737\n",
      "Epoch 00007: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 610us/sample - loss: 2.2039 - acc: 0.4722 - val_loss: 2.1727 - val_acc: 0.4610\n",
      "Epoch 8/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.1505 - acc: 0.4606\n",
      "Epoch 00008: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 566us/sample - loss: 2.1475 - acc: 0.4628 - val_loss: 2.1029 - val_acc: 0.4634\n",
      "Epoch 9/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.0717 - acc: 0.4881\n",
      "Epoch 00009: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 594us/sample - loss: 2.0673 - acc: 0.4929 - val_loss: 2.0425 - val_acc: 0.4823\n",
      "Epoch 10/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.9981 - acc: 0.5108\n",
      "Epoch 00010: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 603us/sample - loss: 1.9953 - acc: 0.5124 - val_loss: 1.9889 - val_acc: 0.4988\n",
      "Epoch 11/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.9564 - acc: 0.5081\n",
      "Epoch 00011: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 599us/sample - loss: 1.9533 - acc: 0.5112 - val_loss: 1.9243 - val_acc: 0.4965\n",
      "Epoch 12/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.8870 - acc: 0.5108\n",
      "Epoch 00012: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 616us/sample - loss: 1.8873 - acc: 0.5106 - val_loss: 1.9342 - val_acc: 0.4752\n",
      "Epoch 13/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.8389 - acc: 0.5325\n",
      "Epoch 00013: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 565us/sample - loss: 1.8340 - acc: 0.5325 - val_loss: 1.8570 - val_acc: 0.5248\n",
      "Epoch 14/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7904 - acc: 0.5343\n",
      "Epoch 00014: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 569us/sample - loss: 1.7904 - acc: 0.5343 - val_loss: 1.8618 - val_acc: 0.4799\n",
      "Epoch 15/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7485 - acc: 0.5403\n",
      "Epoch 00015: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 571us/sample - loss: 1.7453 - acc: 0.5420 - val_loss: 1.7626 - val_acc: 0.5296\n",
      "Epoch 16/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7294 - acc: 0.5258\n",
      "Epoch 00016: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 560us/sample - loss: 1.7281 - acc: 0.5248 - val_loss: 1.7595 - val_acc: 0.4846\n",
      "Epoch 17/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.6627 - acc: 0.5487\n",
      "Epoch 00017: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 599us/sample - loss: 1.6656 - acc: 0.5455 - val_loss: 1.7390 - val_acc: 0.5248\n",
      "Epoch 18/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.6369 - acc: 0.5497\n",
      "Epoch 00018: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 586us/sample - loss: 1.6405 - acc: 0.5479 - val_loss: 1.7198 - val_acc: 0.5012\n",
      "Epoch 19/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6019 - acc: 0.5539\n",
      "Epoch 00019: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 599us/sample - loss: 1.6059 - acc: 0.5520 - val_loss: 1.6571 - val_acc: 0.5177\n",
      "Epoch 20/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5733 - acc: 0.5697\n",
      "Epoch 00020: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 584us/sample - loss: 1.5750 - acc: 0.5680 - val_loss: 1.6484 - val_acc: 0.5154\n",
      "Epoch 21/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.5446 - acc: 0.5631\n",
      "Epoch 00021: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 614us/sample - loss: 1.5453 - acc: 0.5650 - val_loss: 1.6011 - val_acc: 0.5296\n",
      "Epoch 22/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.5249 - acc: 0.5656\n",
      "Epoch 00022: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 612us/sample - loss: 1.5208 - acc: 0.5691 - val_loss: 1.5815 - val_acc: 0.5154\n",
      "Epoch 23/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5018 - acc: 0.5661\n",
      "Epoch 00023: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 614us/sample - loss: 1.5025 - acc: 0.5680 - val_loss: 1.5838 - val_acc: 0.5414\n",
      "Epoch 24/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4472 - acc: 0.5956\n",
      "Epoch 00024: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 623us/sample - loss: 1.4463 - acc: 0.5957 - val_loss: 1.5505 - val_acc: 0.5366\n",
      "Epoch 25/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.4408 - acc: 0.5950\n",
      "Epoch 00025: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 604us/sample - loss: 1.4473 - acc: 0.5916 - val_loss: 1.5707 - val_acc: 0.5177\n",
      "Epoch 26/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4207 - acc: 0.6016\n",
      "Epoch 00026: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 609us/sample - loss: 1.4205 - acc: 0.6011 - val_loss: 1.5218 - val_acc: 0.5461\n",
      "Epoch 27/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3864 - acc: 0.6028\n",
      "Epoch 00027: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 617us/sample - loss: 1.3896 - acc: 0.6011 - val_loss: 1.4767 - val_acc: 0.5934\n",
      "Epoch 28/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3838 - acc: 0.5870\n",
      "Epoch 00028: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 614us/sample - loss: 1.3822 - acc: 0.5892 - val_loss: 1.4933 - val_acc: 0.5390\n",
      "Epoch 29/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3700 - acc: 0.6062\n",
      "Epoch 00029: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 604us/sample - loss: 1.3708 - acc: 0.6028 - val_loss: 1.4904 - val_acc: 0.5437\n",
      "Epoch 30/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3399 - acc: 0.6042\n",
      "Epoch 00030: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 598us/sample - loss: 1.3402 - acc: 0.6046 - val_loss: 1.4341 - val_acc: 0.5863\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3283 - acc: 0.6112\n",
      "Epoch 00031: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 601us/sample - loss: 1.3300 - acc: 0.6093 - val_loss: 1.4336 - val_acc: 0.5887\n",
      "Epoch 32/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3127 - acc: 0.6184\n",
      "Epoch 00032: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 605us/sample - loss: 1.3108 - acc: 0.6194 - val_loss: 1.4600 - val_acc: 0.5532\n",
      "Epoch 33/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2821 - acc: 0.6161\n",
      "Epoch 00033: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 610us/sample - loss: 1.2886 - acc: 0.6141 - val_loss: 1.4078 - val_acc: 0.5816\n",
      "Epoch 34/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2775 - acc: 0.6334\n",
      "Epoch 00034: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 627us/sample - loss: 1.2777 - acc: 0.6330 - val_loss: 1.4564 - val_acc: 0.5626\n",
      "Epoch 35/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2703 - acc: 0.6292\n",
      "Epoch 00035: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 601us/sample - loss: 1.2718 - acc: 0.6300 - val_loss: 1.3893 - val_acc: 0.5768\n",
      "Epoch 36/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2451 - acc: 0.6381\n",
      "Epoch 00036: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 625us/sample - loss: 1.2513 - acc: 0.6353 - val_loss: 1.3917 - val_acc: 0.5792\n",
      "Epoch 37/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2254 - acc: 0.6363\n",
      "Epoch 00037: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 631us/sample - loss: 1.2292 - acc: 0.6336 - val_loss: 1.4385 - val_acc: 0.5556\n",
      "Epoch 38/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2414 - acc: 0.6232\n",
      "Epoch 00038: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 594us/sample - loss: 1.2417 - acc: 0.6235 - val_loss: 1.3585 - val_acc: 0.5910\n",
      "Epoch 39/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1937 - acc: 0.6473\n",
      "Epoch 00039: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 615us/sample - loss: 1.2025 - acc: 0.6424 - val_loss: 1.4044 - val_acc: 0.5745\n",
      "Epoch 40/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1992 - acc: 0.6382\n",
      "Epoch 00040: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 626us/sample - loss: 1.1986 - acc: 0.6389 - val_loss: 1.3643 - val_acc: 0.5650\n",
      "Epoch 41/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1890 - acc: 0.6369\n",
      "Epoch 00041: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 607us/sample - loss: 1.1868 - acc: 0.6383 - val_loss: 1.3888 - val_acc: 0.5816\n",
      "Epoch 42/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1554 - acc: 0.6719\n",
      "Epoch 00042: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 611us/sample - loss: 1.1538 - acc: 0.6720 - val_loss: 1.3956 - val_acc: 0.5721\n",
      "Epoch 43/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1547 - acc: 0.6507\n",
      "Epoch 00043: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 578us/sample - loss: 1.1577 - acc: 0.6495 - val_loss: 1.3929 - val_acc: 0.5863\n",
      "Epoch 44/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1414 - acc: 0.6614\n",
      "Epoch 00044: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 581us/sample - loss: 1.1470 - acc: 0.6578 - val_loss: 1.4147 - val_acc: 0.5745\n",
      "Epoch 45/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1494 - acc: 0.6645\n",
      "Epoch 00045: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 586us/sample - loss: 1.1545 - acc: 0.6596 - val_loss: 1.3354 - val_acc: 0.5792\n",
      "Epoch 46/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1346 - acc: 0.6532\n",
      "Epoch 00046: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 577us/sample - loss: 1.1303 - acc: 0.6566 - val_loss: 1.2622 - val_acc: 0.6241\n",
      "Epoch 47/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1216 - acc: 0.6661\n",
      "Epoch 00047: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 611us/sample - loss: 1.1250 - acc: 0.6649 - val_loss: 1.3882 - val_acc: 0.5650\n",
      "Epoch 48/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1166 - acc: 0.6689\n",
      "Epoch 00048: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 611us/sample - loss: 1.1165 - acc: 0.6696 - val_loss: 1.2814 - val_acc: 0.6123\n",
      "Epoch 49/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1039 - acc: 0.6759\n",
      "Epoch 00049: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 604us/sample - loss: 1.0971 - acc: 0.6773 - val_loss: 1.3555 - val_acc: 0.5887\n",
      "Epoch 50/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0978 - acc: 0.6785\n",
      "Epoch 00050: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 614us/sample - loss: 1.0939 - acc: 0.6797 - val_loss: 1.3672 - val_acc: 0.5697\n",
      "Epoch 51/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1119 - acc: 0.6709\n",
      "Epoch 00051: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 611us/sample - loss: 1.1131 - acc: 0.6690 - val_loss: 1.2983 - val_acc: 0.6170\n",
      "Epoch 52/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0924 - acc: 0.6831\n",
      "Epoch 00052: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 603us/sample - loss: 1.0885 - acc: 0.6826 - val_loss: 1.3830 - val_acc: 0.5626\n",
      "Epoch 53/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0459 - acc: 0.6926\n",
      "Epoch 00053: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 579us/sample - loss: 1.0480 - acc: 0.6915 - val_loss: 1.3546 - val_acc: 0.5887\n",
      "Epoch 54/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0700 - acc: 0.6785\n",
      "Epoch 00054: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 605us/sample - loss: 1.0701 - acc: 0.6785 - val_loss: 1.3673 - val_acc: 0.5768\n",
      "Epoch 55/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0622 - acc: 0.6938\n",
      "Epoch 00055: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 612us/sample - loss: 1.0605 - acc: 0.6944 - val_loss: 1.2621 - val_acc: 0.6052\n",
      "Epoch 56/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0407 - acc: 0.6918\n",
      "Epoch 00056: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 592us/sample - loss: 1.0343 - acc: 0.6950 - val_loss: 1.3196 - val_acc: 0.6076\n",
      "Epoch 57/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0383 - acc: 0.6949\n",
      "Epoch 00057: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 589us/sample - loss: 1.0379 - acc: 0.6933 - val_loss: 1.3141 - val_acc: 0.5934\n",
      "Epoch 58/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0256 - acc: 0.7003\n",
      "Epoch 00058: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 548us/sample - loss: 1.0273 - acc: 0.6968 - val_loss: 1.3344 - val_acc: 0.6076\n",
      "Epoch 59/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0030 - acc: 0.6977\n",
      "Epoch 00059: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 579us/sample - loss: 1.0021 - acc: 0.6980 - val_loss: 1.2927 - val_acc: 0.6099\n",
      "Epoch 60/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0122 - acc: 0.7013\n",
      "Epoch 00060: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 592us/sample - loss: 1.0245 - acc: 0.6944 - val_loss: 1.3203 - val_acc: 0.6147\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0140 - acc: 0.6911\n",
      "Epoch 00061: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 619us/sample - loss: 1.0154 - acc: 0.6909 - val_loss: 1.2707 - val_acc: 0.6265\n",
      "Epoch 62/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0131 - acc: 0.6958\n",
      "Epoch 00062: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 615us/sample - loss: 1.0120 - acc: 0.6950 - val_loss: 1.2803 - val_acc: 0.6194\n",
      "Epoch 63/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0185 - acc: 0.7028\n",
      "Epoch 00063: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 600us/sample - loss: 1.0143 - acc: 0.7039 - val_loss: 1.3846 - val_acc: 0.5816\n",
      "Epoch 64/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0121 - acc: 0.6998\n",
      "Epoch 00064: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 527us/sample - loss: 1.0095 - acc: 0.7021 - val_loss: 1.3218 - val_acc: 0.6028\n",
      "Epoch 65/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9655 - acc: 0.7188\n",
      "Epoch 00065: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 570us/sample - loss: 0.9669 - acc: 0.7175 - val_loss: 1.3687 - val_acc: 0.5981\n",
      "Epoch 66/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9601 - acc: 0.7326\n",
      "Epoch 00066: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 528us/sample - loss: 0.9647 - acc: 0.7293 - val_loss: 1.3052 - val_acc: 0.6005\n",
      "Epoch 67/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9654 - acc: 0.7212\n",
      "Epoch 00067: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 556us/sample - loss: 0.9624 - acc: 0.7240 - val_loss: 1.2749 - val_acc: 0.6099\n",
      "Epoch 68/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9510 - acc: 0.7292\n",
      "Epoch 00068: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 525us/sample - loss: 0.9510 - acc: 0.7293 - val_loss: 1.2911 - val_acc: 0.6241\n",
      "Epoch 69/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9546 - acc: 0.7169\n",
      "Epoch 00069: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 609us/sample - loss: 0.9578 - acc: 0.7151 - val_loss: 1.2839 - val_acc: 0.6265\n",
      "Epoch 70/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9444 - acc: 0.7337\n",
      "Epoch 00070: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 603us/sample - loss: 0.9443 - acc: 0.7317 - val_loss: 1.3705 - val_acc: 0.6052\n",
      "Epoch 71/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9560 - acc: 0.7200\n",
      "Epoch 00071: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 601us/sample - loss: 0.9520 - acc: 0.7216 - val_loss: 1.3103 - val_acc: 0.6028\n",
      "Epoch 72/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9403 - acc: 0.7212\n",
      "Epoch 00072: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 507us/sample - loss: 0.9328 - acc: 0.7246 - val_loss: 1.2964 - val_acc: 0.6170\n",
      "Epoch 73/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9547 - acc: 0.7260\n",
      "Epoch 00073: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 510us/sample - loss: 0.9564 - acc: 0.7252 - val_loss: 1.2741 - val_acc: 0.6194\n",
      "Epoch 74/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9185 - acc: 0.7206\n",
      "Epoch 00074: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 491us/sample - loss: 0.9156 - acc: 0.7222 - val_loss: 1.3426 - val_acc: 0.6076\n",
      "Epoch 75/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9216 - acc: 0.7400\n",
      "Epoch 00075: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 543us/sample - loss: 0.9184 - acc: 0.7417 - val_loss: 1.3098 - val_acc: 0.6170\n",
      "Epoch 76/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9094 - acc: 0.7422\n",
      "Epoch 00076: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 454us/sample - loss: 0.9128 - acc: 0.7405 - val_loss: 1.2873 - val_acc: 0.6241\n",
      "Epoch 77/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9199 - acc: 0.7236\n",
      "Epoch 00077: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 568us/sample - loss: 0.9187 - acc: 0.7240 - val_loss: 1.2485 - val_acc: 0.6123\n",
      "Epoch 78/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9011 - acc: 0.7380\n",
      "Epoch 00078: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 566us/sample - loss: 0.9003 - acc: 0.7376 - val_loss: 1.3534 - val_acc: 0.6052\n",
      "Epoch 79/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9168 - acc: 0.7374\n",
      "Epoch 00079: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 517us/sample - loss: 0.9181 - acc: 0.7346 - val_loss: 1.3773 - val_acc: 0.6052\n",
      "Epoch 80/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8895 - acc: 0.7468\n",
      "Epoch 00080: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 558us/sample - loss: 0.8897 - acc: 0.7476 - val_loss: 1.3067 - val_acc: 0.6478\n",
      "Epoch 81/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9035 - acc: 0.7398\n",
      "Epoch 00081: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 547us/sample - loss: 0.9038 - acc: 0.7400 - val_loss: 1.3502 - val_acc: 0.6052\n",
      "Epoch 82/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8940 - acc: 0.7462\n",
      "Epoch 00082: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 606us/sample - loss: 0.8938 - acc: 0.7465 - val_loss: 1.3438 - val_acc: 0.6028\n",
      "Epoch 83/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8698 - acc: 0.7616\n",
      "Epoch 00083: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 621us/sample - loss: 0.8636 - acc: 0.7630 - val_loss: 1.3546 - val_acc: 0.5934\n",
      "Epoch 84/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9039 - acc: 0.7396\n",
      "Epoch 00084: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 605us/sample - loss: 0.8994 - acc: 0.7423 - val_loss: 1.3099 - val_acc: 0.6336\n",
      "Epoch 85/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8511 - acc: 0.7584\n",
      "Epoch 00085: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 578us/sample - loss: 0.8514 - acc: 0.7589 - val_loss: 1.3468 - val_acc: 0.6099\n",
      "Epoch 86/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8846 - acc: 0.7476\n",
      "Epoch 00086: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 584us/sample - loss: 0.8868 - acc: 0.7470 - val_loss: 1.3478 - val_acc: 0.6217\n",
      "Epoch 87/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8827 - acc: 0.7613\n",
      "Epoch 00087: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 616us/sample - loss: 0.8817 - acc: 0.7612 - val_loss: 1.3048 - val_acc: 0.6217\n",
      "Epoch 88/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8470 - acc: 0.7650\n",
      "Epoch 00088: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 591us/sample - loss: 0.8464 - acc: 0.7648 - val_loss: 1.4295 - val_acc: 0.6147\n",
      "Epoch 89/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8516 - acc: 0.7653\n",
      "Epoch 00089: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 601us/sample - loss: 0.8548 - acc: 0.7648 - val_loss: 1.3262 - val_acc: 0.6241\n",
      "Epoch 90/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8308 - acc: 0.7616\n",
      "Epoch 00090: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 614us/sample - loss: 0.8266 - acc: 0.7636 - val_loss: 1.2911 - val_acc: 0.6359\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8453 - acc: 0.7574\n",
      "Epoch 00091: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 615us/sample - loss: 0.8492 - acc: 0.7565 - val_loss: 1.3341 - val_acc: 0.6383\n",
      "Epoch 92/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8453 - acc: 0.7675\n",
      "Epoch 00092: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 600us/sample - loss: 0.8419 - acc: 0.7677 - val_loss: 1.3701 - val_acc: 0.6430\n",
      "Epoch 93/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8397 - acc: 0.7604\n",
      "Epoch 00093: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 590us/sample - loss: 0.8392 - acc: 0.7606 - val_loss: 1.3156 - val_acc: 0.6407\n",
      "Epoch 94/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8020 - acc: 0.7881\n",
      "Epoch 00094: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 590us/sample - loss: 0.8127 - acc: 0.7843 - val_loss: 1.3680 - val_acc: 0.6194\n",
      "Epoch 95/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8293 - acc: 0.7706\n",
      "Epoch 00095: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 627us/sample - loss: 0.8290 - acc: 0.7695 - val_loss: 1.3241 - val_acc: 0.6241\n",
      "Epoch 96/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8084 - acc: 0.7794\n",
      "Epoch 00096: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 605us/sample - loss: 0.8154 - acc: 0.7754 - val_loss: 1.3563 - val_acc: 0.6383\n",
      "Epoch 97/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8108 - acc: 0.7764\n",
      "Epoch 00097: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 617us/sample - loss: 0.8131 - acc: 0.7748 - val_loss: 1.4394 - val_acc: 0.6123\n",
      "Epoch 98/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8469 - acc: 0.7638\n",
      "Epoch 00098: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 604us/sample - loss: 0.8433 - acc: 0.7665 - val_loss: 1.3203 - val_acc: 0.6383\n",
      "Epoch 99/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8104 - acc: 0.7806\n",
      "Epoch 00099: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 609us/sample - loss: 0.8187 - acc: 0.7760 - val_loss: 1.3680 - val_acc: 0.6383\n",
      "Epoch 100/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8002 - acc: 0.7781\n",
      "Epoch 00100: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 609us/sample - loss: 0.8075 - acc: 0.7772 - val_loss: 1.3419 - val_acc: 0.6336\n",
      "443/443 [==============================] - 0s 263us/sample - loss: 1.3836 - acc: 0.6095\n",
      "--- Starting trial: run-10\n",
      "{'TIME_WINDOW': 1000, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 64, 'LAST_HIDDEN': 1000, 'LAST_DROPOUT': 0.2, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_95\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_96 (InputLayer)        [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute_95 (Permute)         (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 997, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_190 (Bat (None, 997, 32)           3988      \n",
      "_________________________________________________________________\n",
      "activation_190 (Activation)  (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_285 (Dropout)        (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_190 (MaxPoolin (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_191 (Conv1D)          (None, 246, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_191 (Bat (None, 246, 64)           984       \n",
      "_________________________________________________________________\n",
      "activation_191 (Activation)  (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_286 (Dropout)        (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_191 (MaxPoolin (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_285 (LSTM)              (None, 61, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_286 (LSTM)              (None, 61, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_287 (LSTM)              (None, 61, 64)            49408     \n",
      "_________________________________________________________________\n",
      "flatten_95 (Flatten)         (None, 3904)              0         \n",
      "_________________________________________________________________\n",
      "dense_190 (Dense)            (None, 1000)              3905000   \n",
      "_________________________________________________________________\n",
      "dropout_287 (Dropout)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_191 (Dense)            (None, 4)                 4004      \n",
      "=================================================================\n",
      "Total params: 4,204,888\n",
      "Trainable params: 4,202,402\n",
      "Non-trainable params: 2,486\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 3.8266 - acc: 0.2887\n",
      "Epoch 00001: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 3.8113 - acc: 0.2943 - val_loss: 3.4778 - val_acc: 0.3499\n",
      "Epoch 2/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.4172 - acc: 0.3603\n",
      "Epoch 00002: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 579us/sample - loss: 3.4097 - acc: 0.3623 - val_loss: 3.2142 - val_acc: 0.3877\n",
      "Epoch 3/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 3.2096 - acc: 0.3732\n",
      "Epoch 00003: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 572us/sample - loss: 3.2072 - acc: 0.3741 - val_loss: 3.0573 - val_acc: 0.3948\n",
      "Epoch 4/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 3.0371 - acc: 0.3958\n",
      "Epoch 00004: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 583us/sample - loss: 3.0369 - acc: 0.3936 - val_loss: 2.9029 - val_acc: 0.4161\n",
      "Epoch 5/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.8738 - acc: 0.4201\n",
      "Epoch 00005: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 620us/sample - loss: 2.8739 - acc: 0.4208 - val_loss: 2.7450 - val_acc: 0.4303\n",
      "Epoch 6/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.7450 - acc: 0.4238\n",
      "Epoch 00006: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 605us/sample - loss: 2.7390 - acc: 0.4273 - val_loss: 2.6434 - val_acc: 0.4066\n",
      "Epoch 7/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.5953 - acc: 0.4573\n",
      "Epoch 00007: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 613us/sample - loss: 2.5977 - acc: 0.4515 - val_loss: 2.5234 - val_acc: 0.4208\n",
      "Epoch 8/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.4881 - acc: 0.4675\n",
      "Epoch 00008: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 619us/sample - loss: 2.4869 - acc: 0.4675 - val_loss: 2.3883 - val_acc: 0.4563\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.3530 - acc: 0.4775\n",
      "Epoch 00009: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 607us/sample - loss: 2.3575 - acc: 0.4770 - val_loss: 2.3019 - val_acc: 0.4752\n",
      "Epoch 10/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.2638 - acc: 0.4869\n",
      "Epoch 00010: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 611us/sample - loss: 2.2657 - acc: 0.4823 - val_loss: 2.1965 - val_acc: 0.4870\n",
      "Epoch 11/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.1748 - acc: 0.5043\n",
      "Epoch 00011: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 616us/sample - loss: 2.1708 - acc: 0.5047 - val_loss: 2.1105 - val_acc: 0.4988\n",
      "Epoch 12/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.0843 - acc: 0.5102\n",
      "Epoch 00012: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 614us/sample - loss: 2.0828 - acc: 0.5112 - val_loss: 2.0504 - val_acc: 0.4965\n",
      "Epoch 13/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.0037 - acc: 0.5214\n",
      "Epoch 00013: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 620us/sample - loss: 2.0025 - acc: 0.5219 - val_loss: 1.9430 - val_acc: 0.5059\n",
      "Epoch 14/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.9433 - acc: 0.5319\n",
      "Epoch 00014: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 614us/sample - loss: 1.9421 - acc: 0.5284 - val_loss: 1.8861 - val_acc: 0.5343\n",
      "Epoch 15/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.8716 - acc: 0.5256\n",
      "Epoch 00015: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 618us/sample - loss: 1.8670 - acc: 0.5248 - val_loss: 1.8244 - val_acc: 0.5390\n",
      "Epoch 16/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.8202 - acc: 0.5374\n",
      "Epoch 00016: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 618us/sample - loss: 1.8208 - acc: 0.5366 - val_loss: 1.7496 - val_acc: 0.5437\n",
      "Epoch 17/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7461 - acc: 0.5571\n",
      "Epoch 00017: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 609us/sample - loss: 1.7442 - acc: 0.5579 - val_loss: 1.8225 - val_acc: 0.4917\n",
      "Epoch 18/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.6834 - acc: 0.5719\n",
      "Epoch 00018: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 611us/sample - loss: 1.6864 - acc: 0.5727 - val_loss: 1.7623 - val_acc: 0.4846\n",
      "Epoch 19/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.6510 - acc: 0.5577\n",
      "Epoch 00019: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 616us/sample - loss: 1.6504 - acc: 0.5567 - val_loss: 1.6600 - val_acc: 0.5154\n",
      "Epoch 20/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.6094 - acc: 0.5571\n",
      "Epoch 00020: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 604us/sample - loss: 1.6136 - acc: 0.5538 - val_loss: 1.6359 - val_acc: 0.5319\n",
      "Epoch 21/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.5869 - acc: 0.5625\n",
      "Epoch 00021: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 620us/sample - loss: 1.5841 - acc: 0.5650 - val_loss: 1.5740 - val_acc: 0.5556\n",
      "Epoch 22/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.5576 - acc: 0.5556\n",
      "Epoch 00022: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 600us/sample - loss: 1.5523 - acc: 0.5597 - val_loss: 1.5273 - val_acc: 0.5768\n",
      "Epoch 23/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.5000 - acc: 0.5788\n",
      "Epoch 00023: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 620us/sample - loss: 1.5054 - acc: 0.5739 - val_loss: 1.5348 - val_acc: 0.5437\n",
      "Epoch 24/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4939 - acc: 0.5692\n",
      "Epoch 00024: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 628us/sample - loss: 1.4902 - acc: 0.5709 - val_loss: 1.5017 - val_acc: 0.5579\n",
      "Epoch 25/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4537 - acc: 0.5865\n",
      "Epoch 00025: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 609us/sample - loss: 1.4514 - acc: 0.5881 - val_loss: 1.4839 - val_acc: 0.5650\n",
      "Epoch 26/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4352 - acc: 0.5895\n",
      "Epoch 00026: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 608us/sample - loss: 1.4340 - acc: 0.5910 - val_loss: 1.4968 - val_acc: 0.5556\n",
      "Epoch 27/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3901 - acc: 0.6112\n",
      "Epoch 00027: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 595us/sample - loss: 1.3868 - acc: 0.6135 - val_loss: 1.4913 - val_acc: 0.5414\n",
      "Epoch 28/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3970 - acc: 0.5864\n",
      "Epoch 00028: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 599us/sample - loss: 1.3962 - acc: 0.5851 - val_loss: 1.4542 - val_acc: 0.5390\n",
      "Epoch 29/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3387 - acc: 0.6118\n",
      "Epoch 00029: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 597us/sample - loss: 1.3380 - acc: 0.6117 - val_loss: 1.4559 - val_acc: 0.5626\n",
      "Epoch 30/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3296 - acc: 0.6125\n",
      "Epoch 00030: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 582us/sample - loss: 1.3190 - acc: 0.6164 - val_loss: 1.4158 - val_acc: 0.5697\n",
      "Epoch 31/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2888 - acc: 0.6356\n",
      "Epoch 00031: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 619us/sample - loss: 1.2913 - acc: 0.6342 - val_loss: 1.4022 - val_acc: 0.5768\n",
      "Epoch 32/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3124 - acc: 0.6085\n",
      "Epoch 00032: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 575us/sample - loss: 1.3021 - acc: 0.6147 - val_loss: 1.4459 - val_acc: 0.5461\n",
      "Epoch 33/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2648 - acc: 0.6213\n",
      "Epoch 00033: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 626us/sample - loss: 1.2672 - acc: 0.6194 - val_loss: 1.4831 - val_acc: 0.5272\n",
      "Epoch 34/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2305 - acc: 0.6344\n",
      "Epoch 00034: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 581us/sample - loss: 1.2304 - acc: 0.6348 - val_loss: 1.4121 - val_acc: 0.5532\n",
      "Epoch 35/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2405 - acc: 0.6325\n",
      "Epoch 00035: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 614us/sample - loss: 1.2357 - acc: 0.6306 - val_loss: 1.6055 - val_acc: 0.4870\n",
      "Epoch 36/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2406 - acc: 0.6109\n",
      "Epoch 00036: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 589us/sample - loss: 1.2401 - acc: 0.6123 - val_loss: 1.3494 - val_acc: 0.5863\n",
      "Epoch 37/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2081 - acc: 0.6338\n",
      "Epoch 00037: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 600us/sample - loss: 1.2108 - acc: 0.6330 - val_loss: 1.3792 - val_acc: 0.5650\n",
      "Epoch 38/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2130 - acc: 0.6446\n",
      "Epoch 00038: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 588us/sample - loss: 1.2179 - acc: 0.6442 - val_loss: 1.3549 - val_acc: 0.5721\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2020 - acc: 0.6346\n",
      "Epoch 00039: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 606us/sample - loss: 1.2019 - acc: 0.6353 - val_loss: 1.3557 - val_acc: 0.5768\n",
      "Epoch 40/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1778 - acc: 0.6413\n",
      "Epoch 00040: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 586us/sample - loss: 1.1852 - acc: 0.6365 - val_loss: 1.3912 - val_acc: 0.5579\n",
      "Epoch 41/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1740 - acc: 0.6505\n",
      "Epoch 00041: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 611us/sample - loss: 1.1660 - acc: 0.6560 - val_loss: 1.3070 - val_acc: 0.5934\n",
      "Epoch 42/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1559 - acc: 0.6544\n",
      "Epoch 00042: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 612us/sample - loss: 1.1531 - acc: 0.6531 - val_loss: 1.3484 - val_acc: 0.5934\n",
      "Epoch 43/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1406 - acc: 0.6562\n",
      "Epoch 00043: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 613us/sample - loss: 1.1462 - acc: 0.6566 - val_loss: 1.3673 - val_acc: 0.5579\n",
      "Epoch 44/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1357 - acc: 0.6500\n",
      "Epoch 00044: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 579us/sample - loss: 1.1434 - acc: 0.6478 - val_loss: 1.2913 - val_acc: 0.6005\n",
      "Epoch 45/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1199 - acc: 0.6743\n",
      "Epoch 00045: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 573us/sample - loss: 1.1204 - acc: 0.6720 - val_loss: 1.3113 - val_acc: 0.5910\n",
      "Epoch 46/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1099 - acc: 0.6659\n",
      "Epoch 00046: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 584us/sample - loss: 1.1153 - acc: 0.6631 - val_loss: 1.3059 - val_acc: 0.5981\n",
      "Epoch 47/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0978 - acc: 0.6710\n",
      "Epoch 00047: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 622us/sample - loss: 1.1022 - acc: 0.6667 - val_loss: 1.3467 - val_acc: 0.5839\n",
      "Epoch 48/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0922 - acc: 0.6710\n",
      "Epoch 00048: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 602us/sample - loss: 1.0865 - acc: 0.6732 - val_loss: 1.3078 - val_acc: 0.6099\n",
      "Epoch 49/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0849 - acc: 0.6821\n",
      "Epoch 00049: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 610us/sample - loss: 1.0854 - acc: 0.6803 - val_loss: 1.2921 - val_acc: 0.5863\n",
      "Epoch 50/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0670 - acc: 0.6893\n",
      "Epoch 00050: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 605us/sample - loss: 1.0713 - acc: 0.6885 - val_loss: 1.2717 - val_acc: 0.6217\n",
      "Epoch 51/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0615 - acc: 0.6844\n",
      "Epoch 00051: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 590us/sample - loss: 1.0610 - acc: 0.6856 - val_loss: 1.2963 - val_acc: 0.6076\n",
      "Epoch 52/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0515 - acc: 0.6938\n",
      "Epoch 00052: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 610us/sample - loss: 1.0540 - acc: 0.6909 - val_loss: 1.3187 - val_acc: 0.5887\n",
      "Epoch 53/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0386 - acc: 0.6971\n",
      "Epoch 00053: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 615us/sample - loss: 1.0372 - acc: 0.6980 - val_loss: 1.3288 - val_acc: 0.5816\n",
      "Epoch 54/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0371 - acc: 0.6955\n",
      "Epoch 00054: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 613us/sample - loss: 1.0447 - acc: 0.6927 - val_loss: 1.3100 - val_acc: 0.5981\n",
      "Epoch 55/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0287 - acc: 0.6888\n",
      "Epoch 00055: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 610us/sample - loss: 1.0286 - acc: 0.6897 - val_loss: 1.3020 - val_acc: 0.6123\n",
      "Epoch 56/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0453 - acc: 0.6850\n",
      "Epoch 00056: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 594us/sample - loss: 1.0445 - acc: 0.6856 - val_loss: 1.3153 - val_acc: 0.6005\n",
      "Epoch 57/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0532 - acc: 0.6725\n",
      "Epoch 00057: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 562us/sample - loss: 1.0555 - acc: 0.6726 - val_loss: 1.2780 - val_acc: 0.6076\n",
      "Epoch 58/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9936 - acc: 0.7151\n",
      "Epoch 00058: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 568us/sample - loss: 0.9893 - acc: 0.7193 - val_loss: 1.3544 - val_acc: 0.5957\n",
      "Epoch 59/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0354 - acc: 0.6832\n",
      "Epoch 00059: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 567us/sample - loss: 1.0326 - acc: 0.6850 - val_loss: 1.2873 - val_acc: 0.6147\n",
      "Epoch 60/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9996 - acc: 0.7022\n",
      "Epoch 00060: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 483us/sample - loss: 0.9987 - acc: 0.7027 - val_loss: 1.3015 - val_acc: 0.6170\n",
      "Epoch 61/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0231 - acc: 0.6983\n",
      "Epoch 00061: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 481us/sample - loss: 1.0167 - acc: 0.7009 - val_loss: 1.3665 - val_acc: 0.6005\n",
      "Epoch 62/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9841 - acc: 0.6945\n",
      "Epoch 00062: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 482us/sample - loss: 0.9902 - acc: 0.6915 - val_loss: 1.3237 - val_acc: 0.6005\n",
      "Epoch 63/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9613 - acc: 0.7219\n",
      "Epoch 00063: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 493us/sample - loss: 0.9771 - acc: 0.7128 - val_loss: 1.3149 - val_acc: 0.6076\n",
      "Epoch 64/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9888 - acc: 0.6996\n",
      "Epoch 00064: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 484us/sample - loss: 0.9918 - acc: 0.6998 - val_loss: 1.3214 - val_acc: 0.6147\n",
      "Epoch 65/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9484 - acc: 0.7275\n",
      "Epoch 00065: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 482us/sample - loss: 0.9497 - acc: 0.7246 - val_loss: 1.3552 - val_acc: 0.6170\n",
      "Epoch 66/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9846 - acc: 0.6958\n",
      "Epoch 00066: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 495us/sample - loss: 0.9837 - acc: 0.6962 - val_loss: 1.3076 - val_acc: 0.6194\n",
      "Epoch 67/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9637 - acc: 0.7117\n",
      "Epoch 00067: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 494us/sample - loss: 0.9582 - acc: 0.7128 - val_loss: 1.3245 - val_acc: 0.6336\n",
      "Epoch 68/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9722 - acc: 0.7066\n",
      "Epoch 00068: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 487us/sample - loss: 0.9704 - acc: 0.7063 - val_loss: 1.3858 - val_acc: 0.5981\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9444 - acc: 0.7341\n",
      "Epoch 00069: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 493us/sample - loss: 0.9498 - acc: 0.7293 - val_loss: 1.3126 - val_acc: 0.6170\n",
      "Epoch 70/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9368 - acc: 0.7277\n",
      "Epoch 00070: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 497us/sample - loss: 0.9395 - acc: 0.7275 - val_loss: 1.3285 - val_acc: 0.6265\n",
      "Epoch 71/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9432 - acc: 0.7315\n",
      "Epoch 00071: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 496us/sample - loss: 0.9427 - acc: 0.7323 - val_loss: 1.3279 - val_acc: 0.6359\n",
      "Epoch 72/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9152 - acc: 0.7423\n",
      "Epoch 00072: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 497us/sample - loss: 0.9280 - acc: 0.7358 - val_loss: 1.3791 - val_acc: 0.6099\n",
      "Epoch 73/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9095 - acc: 0.7462\n",
      "Epoch 00073: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 482us/sample - loss: 0.9142 - acc: 0.7423 - val_loss: 1.4351 - val_acc: 0.5887\n",
      "Epoch 74/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8926 - acc: 0.7474\n",
      "Epoch 00074: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 494us/sample - loss: 0.8989 - acc: 0.7441 - val_loss: 1.3594 - val_acc: 0.6099\n",
      "Epoch 75/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9311 - acc: 0.7404\n",
      "Epoch 00075: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 0.9439 - acc: 0.7358 - val_loss: 1.4212 - val_acc: 0.5839\n",
      "Epoch 76/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9012 - acc: 0.7430\n",
      "Epoch 00076: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 488us/sample - loss: 0.9081 - acc: 0.7405 - val_loss: 1.3942 - val_acc: 0.6028\n",
      "Epoch 77/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9047 - acc: 0.7481\n",
      "Epoch 00077: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 484us/sample - loss: 0.9014 - acc: 0.7494 - val_loss: 1.3390 - val_acc: 0.6241\n",
      "Epoch 78/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9091 - acc: 0.7302\n",
      "Epoch 00078: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 484us/sample - loss: 0.9132 - acc: 0.7299 - val_loss: 1.3023 - val_acc: 0.6288\n",
      "Epoch 79/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9073 - acc: 0.7404\n",
      "Epoch 00079: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 488us/sample - loss: 0.9126 - acc: 0.7382 - val_loss: 1.3738 - val_acc: 0.5910\n",
      "Epoch 80/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8997 - acc: 0.7423\n",
      "Epoch 00080: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 493us/sample - loss: 0.9029 - acc: 0.7435 - val_loss: 1.3504 - val_acc: 0.6194\n",
      "Epoch 81/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8793 - acc: 0.7564\n",
      "Epoch 00081: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 488us/sample - loss: 0.8872 - acc: 0.7559 - val_loss: 1.3989 - val_acc: 0.6123\n",
      "Epoch 82/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9064 - acc: 0.7462\n",
      "Epoch 00082: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 482us/sample - loss: 0.9080 - acc: 0.7447 - val_loss: 1.3167 - val_acc: 0.6359\n",
      "Epoch 83/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8720 - acc: 0.7494\n",
      "Epoch 00083: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 492us/sample - loss: 0.8727 - acc: 0.7506 - val_loss: 1.3731 - val_acc: 0.6170\n",
      "Epoch 84/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8502 - acc: 0.7506\n",
      "Epoch 00084: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 0.8489 - acc: 0.7506 - val_loss: 1.4429 - val_acc: 0.6194\n",
      "Epoch 85/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8711 - acc: 0.7596\n",
      "Epoch 00085: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 486us/sample - loss: 0.8689 - acc: 0.7624 - val_loss: 1.4053 - val_acc: 0.6194\n",
      "Epoch 86/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8437 - acc: 0.7787\n",
      "Epoch 00086: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 490us/sample - loss: 0.8437 - acc: 0.7766 - val_loss: 1.5056 - val_acc: 0.6076\n",
      "Epoch 87/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8545 - acc: 0.7615\n",
      "Epoch 00087: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 497us/sample - loss: 0.8491 - acc: 0.7624 - val_loss: 1.3668 - val_acc: 0.6265\n",
      "Epoch 88/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8626 - acc: 0.7474\n",
      "Epoch 00088: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 496us/sample - loss: 0.8623 - acc: 0.7482 - val_loss: 1.4961 - val_acc: 0.6147\n",
      "Epoch 89/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8140 - acc: 0.7768\n",
      "Epoch 00089: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 500us/sample - loss: 0.8297 - acc: 0.7701 - val_loss: 1.4156 - val_acc: 0.6194\n",
      "Epoch 90/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8513 - acc: 0.7481\n",
      "Epoch 00090: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 0.8479 - acc: 0.7524 - val_loss: 1.4398 - val_acc: 0.6194\n",
      "Epoch 91/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8297 - acc: 0.7679\n",
      "Epoch 00091: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 496us/sample - loss: 0.8290 - acc: 0.7683 - val_loss: 1.5110 - val_acc: 0.6123\n",
      "Epoch 92/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8314 - acc: 0.7749\n",
      "Epoch 00092: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 498us/sample - loss: 0.8342 - acc: 0.7719 - val_loss: 1.5187 - val_acc: 0.6217\n",
      "Epoch 93/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8173 - acc: 0.7806\n",
      "Epoch 00093: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 485us/sample - loss: 0.8158 - acc: 0.7831 - val_loss: 1.5469 - val_acc: 0.6170\n",
      "Epoch 94/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8221 - acc: 0.7672\n",
      "Epoch 00094: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 489us/sample - loss: 0.8226 - acc: 0.7689 - val_loss: 1.5820 - val_acc: 0.5957\n",
      "Epoch 95/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7958 - acc: 0.7902\n",
      "Epoch 00095: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 490us/sample - loss: 0.7987 - acc: 0.7896 - val_loss: 1.5587 - val_acc: 0.5981\n",
      "Epoch 96/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7865 - acc: 0.7908\n",
      "Epoch 00096: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 489us/sample - loss: 0.7917 - acc: 0.7896 - val_loss: 1.5053 - val_acc: 0.6217\n",
      "Epoch 97/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8223 - acc: 0.7723\n",
      "Epoch 00097: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 486us/sample - loss: 0.8236 - acc: 0.7730 - val_loss: 1.5250 - val_acc: 0.6170\n",
      "Epoch 98/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7958 - acc: 0.7838\n",
      "Epoch 00098: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 486us/sample - loss: 0.8037 - acc: 0.7801 - val_loss: 1.4922 - val_acc: 0.6217\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8019 - acc: 0.7736\n",
      "Epoch 00099: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 493us/sample - loss: 0.8061 - acc: 0.7730 - val_loss: 1.5084 - val_acc: 0.6147\n",
      "Epoch 100/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8009 - acc: 0.7832\n",
      "Epoch 00100: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 477us/sample - loss: 0.7996 - acc: 0.7796 - val_loss: 1.5894 - val_acc: 0.6005\n",
      "443/443 [==============================] - 0s 219us/sample - loss: 1.6557 - acc: 0.5688\n",
      "--- Starting trial: run-11\n",
      "{'TIME_WINDOW': 1000, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 64, 'LAST_HIDDEN': 1000, 'LAST_DROPOUT': 0.3, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_96\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_97 (InputLayer)        [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute_96 (Permute)         (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_192 (Conv1D)          (None, 997, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_192 (Bat (None, 997, 32)           3988      \n",
      "_________________________________________________________________\n",
      "activation_192 (Activation)  (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_288 (Dropout)        (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_192 (MaxPoolin (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_193 (Conv1D)          (None, 246, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_193 (Bat (None, 246, 64)           984       \n",
      "_________________________________________________________________\n",
      "activation_193 (Activation)  (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_289 (Dropout)        (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_193 (MaxPoolin (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_288 (LSTM)              (None, 61, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_289 (LSTM)              (None, 61, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_290 (LSTM)              (None, 61, 64)            49408     \n",
      "_________________________________________________________________\n",
      "flatten_96 (Flatten)         (None, 3904)              0         \n",
      "_________________________________________________________________\n",
      "dense_192 (Dense)            (None, 1000)              3905000   \n",
      "_________________________________________________________________\n",
      "dropout_290 (Dropout)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 4)                 4004      \n",
      "=================================================================\n",
      "Total params: 4,204,888\n",
      "Trainable params: 4,202,402\n",
      "Non-trainable params: 2,486\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.8447 - acc: 0.2876\n",
      "Epoch 00001: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 6s 3ms/sample - loss: 3.8262 - acc: 0.2872 - val_loss: 3.4857 - val_acc: 0.3853\n",
      "Epoch 2/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.4303 - acc: 0.3540\n",
      "Epoch 00002: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 493us/sample - loss: 3.4236 - acc: 0.3582 - val_loss: 3.2275 - val_acc: 0.4137\n",
      "Epoch 3/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.2229 - acc: 0.3858\n",
      "Epoch 00003: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 3.2190 - acc: 0.3818 - val_loss: 3.0804 - val_acc: 0.3783\n",
      "Epoch 4/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.0589 - acc: 0.4094\n",
      "Epoch 00004: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 3.0534 - acc: 0.4096 - val_loss: 2.9342 - val_acc: 0.4137\n",
      "Epoch 5/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.9021 - acc: 0.4267\n",
      "Epoch 00005: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 488us/sample - loss: 2.9017 - acc: 0.4249 - val_loss: 2.8325 - val_acc: 0.3901\n",
      "Epoch 6/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.7562 - acc: 0.4349\n",
      "Epoch 00006: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 492us/sample - loss: 2.7545 - acc: 0.4332 - val_loss: 2.6713 - val_acc: 0.4374\n",
      "Epoch 7/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.6409 - acc: 0.4630\n",
      "Epoch 00007: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 490us/sample - loss: 2.6373 - acc: 0.4610 - val_loss: 2.5617 - val_acc: 0.4326\n",
      "Epoch 8/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.5213 - acc: 0.4662\n",
      "Epoch 00008: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 490us/sample - loss: 2.5139 - acc: 0.4657 - val_loss: 2.4788 - val_acc: 0.4161\n",
      "Epoch 9/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.3995 - acc: 0.4751\n",
      "Epoch 00009: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 492us/sample - loss: 2.3955 - acc: 0.4775 - val_loss: 2.3916 - val_acc: 0.4043\n",
      "Epoch 10/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.3076 - acc: 0.4847\n",
      "Epoch 00010: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 491us/sample - loss: 2.3061 - acc: 0.4852 - val_loss: 2.2724 - val_acc: 0.4397\n",
      "Epoch 11/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.2132 - acc: 0.4943\n",
      "Epoch 00011: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 493us/sample - loss: 2.2160 - acc: 0.4923 - val_loss: 2.2307 - val_acc: 0.4066\n",
      "Epoch 12/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.1350 - acc: 0.5159\n",
      "Epoch 00012: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 498us/sample - loss: 2.1274 - acc: 0.5183 - val_loss: 2.1473 - val_acc: 0.4374\n",
      "Epoch 13/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.0454 - acc: 0.5172\n",
      "Epoch 00013: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 495us/sample - loss: 2.0617 - acc: 0.5095 - val_loss: 2.0364 - val_acc: 0.4752\n",
      "Epoch 14/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.9873 - acc: 0.5013\n",
      "Epoch 00014: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 1.9822 - acc: 0.4994 - val_loss: 1.9643 - val_acc: 0.4752\n",
      "Epoch 15/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.9117 - acc: 0.5478\n",
      "Epoch 00015: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 497us/sample - loss: 1.9120 - acc: 0.5479 - val_loss: 1.9012 - val_acc: 0.5059\n",
      "Epoch 16/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.8481 - acc: 0.5395\n",
      "Epoch 00016: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 497us/sample - loss: 1.8447 - acc: 0.5408 - val_loss: 1.8142 - val_acc: 0.5390\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.8257 - acc: 0.5440\n",
      "Epoch 00017: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 493us/sample - loss: 1.8172 - acc: 0.5496 - val_loss: 1.7918 - val_acc: 0.5106\n",
      "Epoch 18/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.7338 - acc: 0.5753\n",
      "Epoch 00018: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 1.7354 - acc: 0.5768 - val_loss: 1.7342 - val_acc: 0.5508\n",
      "Epoch 19/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.6788 - acc: 0.5663\n",
      "Epoch 00019: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 1.6792 - acc: 0.5668 - val_loss: 1.6696 - val_acc: 0.5697\n",
      "Epoch 20/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.6546 - acc: 0.5663\n",
      "Epoch 00020: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 496us/sample - loss: 1.6525 - acc: 0.5656 - val_loss: 1.6681 - val_acc: 0.5366\n",
      "Epoch 21/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.6174 - acc: 0.5733\n",
      "Epoch 00021: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 1.6155 - acc: 0.5739 - val_loss: 1.6728 - val_acc: 0.5225\n",
      "Epoch 22/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5569 - acc: 0.5957\n",
      "Epoch 00022: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 495us/sample - loss: 1.5537 - acc: 0.5946 - val_loss: 1.6369 - val_acc: 0.5225\n",
      "Epoch 23/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5520 - acc: 0.5753\n",
      "Epoch 00023: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 492us/sample - loss: 1.5550 - acc: 0.5727 - val_loss: 1.5817 - val_acc: 0.5390\n",
      "Epoch 24/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.5373 - acc: 0.5694\n",
      "Epoch 00024: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 481us/sample - loss: 1.5341 - acc: 0.5697 - val_loss: 1.5545 - val_acc: 0.5461\n",
      "Epoch 25/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.4760 - acc: 0.5831\n",
      "Epoch 00025: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 479us/sample - loss: 1.4701 - acc: 0.5839 - val_loss: 1.5252 - val_acc: 0.5508\n",
      "Epoch 26/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4403 - acc: 0.6046\n",
      "Epoch 00026: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 482us/sample - loss: 1.4386 - acc: 0.6064 - val_loss: 1.4971 - val_acc: 0.5768\n",
      "Epoch 27/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4120 - acc: 0.6014\n",
      "Epoch 00027: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 482us/sample - loss: 1.4109 - acc: 0.6046 - val_loss: 1.4328 - val_acc: 0.6052\n",
      "Epoch 28/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3966 - acc: 0.6065\n",
      "Epoch 00028: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 490us/sample - loss: 1.4029 - acc: 0.5981 - val_loss: 1.4346 - val_acc: 0.5863\n",
      "Epoch 29/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3411 - acc: 0.6237\n",
      "Epoch 00029: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 494us/sample - loss: 1.3432 - acc: 0.6182 - val_loss: 1.4314 - val_acc: 0.5792\n",
      "Epoch 30/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3667 - acc: 0.6014\n",
      "Epoch 00030: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 494us/sample - loss: 1.3665 - acc: 0.5987 - val_loss: 1.4006 - val_acc: 0.5957\n",
      "Epoch 31/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3359 - acc: 0.6154\n",
      "Epoch 00031: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 1.3341 - acc: 0.6141 - val_loss: 1.5022 - val_acc: 0.5626\n",
      "Epoch 32/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3409 - acc: 0.5989\n",
      "Epoch 00032: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 489us/sample - loss: 1.3369 - acc: 0.6011 - val_loss: 1.3848 - val_acc: 0.5957\n",
      "Epoch 33/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2932 - acc: 0.6186\n",
      "Epoch 00033: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 491us/sample - loss: 1.2978 - acc: 0.6147 - val_loss: 1.3632 - val_acc: 0.5957\n",
      "Epoch 34/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2744 - acc: 0.6429\n",
      "Epoch 00034: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 485us/sample - loss: 1.2750 - acc: 0.6430 - val_loss: 1.3776 - val_acc: 0.6194\n",
      "Epoch 35/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2453 - acc: 0.6448\n",
      "Epoch 00035: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 490us/sample - loss: 1.2492 - acc: 0.6413 - val_loss: 1.3541 - val_acc: 0.6076\n",
      "Epoch 36/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2209 - acc: 0.6531\n",
      "Epoch 00036: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 490us/sample - loss: 1.2237 - acc: 0.6513 - val_loss: 1.3399 - val_acc: 0.6123\n",
      "Epoch 37/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2156 - acc: 0.6556\n",
      "Epoch 00037: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 493us/sample - loss: 1.2208 - acc: 0.6560 - val_loss: 1.3121 - val_acc: 0.6099\n",
      "Epoch 38/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1946 - acc: 0.6614\n",
      "Epoch 00038: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 500us/sample - loss: 1.1968 - acc: 0.6596 - val_loss: 1.3029 - val_acc: 0.6052\n",
      "Epoch 39/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1767 - acc: 0.6569\n",
      "Epoch 00039: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 1.1720 - acc: 0.6566 - val_loss: 1.3155 - val_acc: 0.6052\n",
      "Epoch 40/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1827 - acc: 0.6460\n",
      "Epoch 00040: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 495us/sample - loss: 1.1750 - acc: 0.6472 - val_loss: 1.2812 - val_acc: 0.6241\n",
      "Epoch 41/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1854 - acc: 0.6505\n",
      "Epoch 00041: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 492us/sample - loss: 1.1961 - acc: 0.6472 - val_loss: 1.3409 - val_acc: 0.5816\n",
      "Epoch 42/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1488 - acc: 0.6607\n",
      "Epoch 00042: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 1.1439 - acc: 0.6625 - val_loss: 1.2739 - val_acc: 0.6099\n",
      "Epoch 43/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1424 - acc: 0.6556\n",
      "Epoch 00043: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 494us/sample - loss: 1.1420 - acc: 0.6554 - val_loss: 1.2991 - val_acc: 0.6170\n",
      "Epoch 44/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1314 - acc: 0.6620\n",
      "Epoch 00044: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 482us/sample - loss: 1.1342 - acc: 0.6619 - val_loss: 1.2919 - val_acc: 0.6123\n",
      "Epoch 45/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1230 - acc: 0.6658\n",
      "Epoch 00045: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 497us/sample - loss: 1.1269 - acc: 0.6649 - val_loss: 1.2957 - val_acc: 0.6076\n",
      "Epoch 46/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1205 - acc: 0.6773\n",
      "Epoch 00046: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 486us/sample - loss: 1.1179 - acc: 0.6761 - val_loss: 1.2975 - val_acc: 0.6005\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0848 - acc: 0.6798\n",
      "Epoch 00047: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 492us/sample - loss: 1.0796 - acc: 0.6838 - val_loss: 1.2913 - val_acc: 0.6194\n",
      "Epoch 48/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0641 - acc: 0.6773\n",
      "Epoch 00048: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 487us/sample - loss: 1.0765 - acc: 0.6684 - val_loss: 1.2599 - val_acc: 0.6383\n",
      "Epoch 49/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0712 - acc: 0.6862\n",
      "Epoch 00049: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 495us/sample - loss: 1.0805 - acc: 0.6814 - val_loss: 1.2125 - val_acc: 0.6454\n",
      "Epoch 50/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0576 - acc: 0.6786\n",
      "Epoch 00050: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 484us/sample - loss: 1.0634 - acc: 0.6791 - val_loss: 1.3087 - val_acc: 0.5910\n",
      "Epoch 51/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0724 - acc: 0.6907\n",
      "Epoch 00051: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 491us/sample - loss: 1.0688 - acc: 0.6897 - val_loss: 1.2698 - val_acc: 0.6099\n",
      "Epoch 52/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0641 - acc: 0.6862\n",
      "Epoch 00052: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 483us/sample - loss: 1.0702 - acc: 0.6850 - val_loss: 1.2153 - val_acc: 0.6478\n",
      "Epoch 53/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0419 - acc: 0.6983\n",
      "Epoch 00053: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 500us/sample - loss: 1.0512 - acc: 0.6950 - val_loss: 1.2687 - val_acc: 0.6147\n",
      "Epoch 54/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0445 - acc: 0.6990\n",
      "Epoch 00054: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 489us/sample - loss: 1.0493 - acc: 0.6921 - val_loss: 1.2229 - val_acc: 0.6336\n",
      "Epoch 55/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0135 - acc: 0.7009\n",
      "Epoch 00055: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 491us/sample - loss: 1.0153 - acc: 0.6992 - val_loss: 1.2281 - val_acc: 0.6525\n",
      "Epoch 56/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0123 - acc: 0.7034\n",
      "Epoch 00056: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 488us/sample - loss: 1.0159 - acc: 0.6998 - val_loss: 1.2492 - val_acc: 0.6288\n",
      "Epoch 57/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0167 - acc: 0.7028\n",
      "Epoch 00057: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 493us/sample - loss: 1.0138 - acc: 0.7027 - val_loss: 1.2256 - val_acc: 0.6548\n",
      "Epoch 58/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9767 - acc: 0.7232\n",
      "Epoch 00058: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 492us/sample - loss: 0.9840 - acc: 0.7222 - val_loss: 1.2171 - val_acc: 0.6478\n",
      "Epoch 59/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0104 - acc: 0.6971\n",
      "Epoch 00059: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 488us/sample - loss: 1.0104 - acc: 0.6974 - val_loss: 1.1961 - val_acc: 0.6430\n",
      "Epoch 60/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9781 - acc: 0.7290\n",
      "Epoch 00060: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 495us/sample - loss: 0.9872 - acc: 0.7240 - val_loss: 1.3036 - val_acc: 0.6005\n",
      "Epoch 61/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9605 - acc: 0.7175\n",
      "Epoch 00061: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 489us/sample - loss: 0.9735 - acc: 0.7116 - val_loss: 1.2317 - val_acc: 0.6667\n",
      "Epoch 62/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9733 - acc: 0.7034\n",
      "Epoch 00062: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 491us/sample - loss: 0.9792 - acc: 0.7009 - val_loss: 1.2226 - val_acc: 0.6572\n",
      "Epoch 63/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9832 - acc: 0.7117\n",
      "Epoch 00063: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 0.9791 - acc: 0.7128 - val_loss: 1.2451 - val_acc: 0.6548\n",
      "Epoch 64/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9675 - acc: 0.7149\n",
      "Epoch 00064: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 496us/sample - loss: 0.9707 - acc: 0.7139 - val_loss: 1.2116 - val_acc: 0.6619\n",
      "Epoch 65/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9484 - acc: 0.7245\n",
      "Epoch 00065: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 475us/sample - loss: 0.9470 - acc: 0.7216 - val_loss: 1.2298 - val_acc: 0.6478\n",
      "Epoch 66/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9833 - acc: 0.7200\n",
      "Epoch 00066: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 486us/sample - loss: 0.9788 - acc: 0.7193 - val_loss: 1.2498 - val_acc: 0.6548\n",
      "Epoch 67/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9502 - acc: 0.7219\n",
      "Epoch 00067: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 495us/sample - loss: 0.9470 - acc: 0.7228 - val_loss: 1.1850 - val_acc: 0.6643\n",
      "Epoch 68/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9146 - acc: 0.7423\n",
      "Epoch 00068: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 496us/sample - loss: 0.9195 - acc: 0.7388 - val_loss: 1.2106 - val_acc: 0.6690\n",
      "Epoch 69/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9308 - acc: 0.7360\n",
      "Epoch 00069: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 497us/sample - loss: 0.9426 - acc: 0.7323 - val_loss: 1.1942 - val_acc: 0.6596\n",
      "Epoch 70/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9164 - acc: 0.7334\n",
      "Epoch 00070: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 502us/sample - loss: 0.9102 - acc: 0.7376 - val_loss: 1.2627 - val_acc: 0.6667\n",
      "Epoch 71/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9126 - acc: 0.7423\n",
      "Epoch 00071: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 495us/sample - loss: 0.9167 - acc: 0.7388 - val_loss: 1.2463 - val_acc: 0.6501\n",
      "Epoch 72/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9424 - acc: 0.7181\n",
      "Epoch 00072: val_loss did not improve from 1.15800\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 0.9325 - acc: 0.7228 - val_loss: 1.2230 - val_acc: 0.6596\n",
      "Epoch 73/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9435 - acc: 0.7162\n",
      "Epoch 00073: val_loss improved from 1.15800 to 1.15013, saving model to ./model_checkpoints/crnn_lstm_model\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/crnn_lstm_model/assets\n",
      "1692/1692 [==============================] - 8s 5ms/sample - loss: 0.9401 - acc: 0.7181 - val_loss: 1.1501 - val_acc: 0.6785\n",
      "Epoch 74/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9118 - acc: 0.7433\n",
      "Epoch 00074: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 608us/sample - loss: 0.9134 - acc: 0.7417 - val_loss: 1.1792 - val_acc: 0.6619\n",
      "Epoch 75/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8913 - acc: 0.7537\n",
      "Epoch 00075: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 574us/sample - loss: 0.8944 - acc: 0.7524 - val_loss: 1.1884 - val_acc: 0.6761\n",
      "Epoch 76/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9054 - acc: 0.7375\n",
      "Epoch 00076: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 594us/sample - loss: 0.8988 - acc: 0.7388 - val_loss: 1.2630 - val_acc: 0.6548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9074 - acc: 0.7322\n",
      "Epoch 00077: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 586us/sample - loss: 0.9056 - acc: 0.7335 - val_loss: 1.1831 - val_acc: 0.6714\n",
      "Epoch 78/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8688 - acc: 0.7656\n",
      "Epoch 00078: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 0.8656 - acc: 0.7665 - val_loss: 1.2386 - val_acc: 0.6667\n",
      "Epoch 79/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8793 - acc: 0.7360\n",
      "Epoch 00079: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 489us/sample - loss: 0.8931 - acc: 0.7293 - val_loss: 1.2282 - val_acc: 0.6643\n",
      "Epoch 80/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8636 - acc: 0.7634\n",
      "Epoch 00080: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 585us/sample - loss: 0.8730 - acc: 0.7571 - val_loss: 1.2699 - val_acc: 0.6478\n",
      "Epoch 81/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8468 - acc: 0.7734\n",
      "Epoch 00081: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 588us/sample - loss: 0.8433 - acc: 0.7748 - val_loss: 1.2361 - val_acc: 0.6525\n",
      "Epoch 82/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8615 - acc: 0.7580\n",
      "Epoch 00082: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 583us/sample - loss: 0.8557 - acc: 0.7606 - val_loss: 1.3490 - val_acc: 0.6288\n",
      "Epoch 83/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8824 - acc: 0.7433\n",
      "Epoch 00083: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 574us/sample - loss: 0.8774 - acc: 0.7453 - val_loss: 1.2743 - val_acc: 0.6596\n",
      "Epoch 84/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8444 - acc: 0.7681\n",
      "Epoch 00084: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 574us/sample - loss: 0.8486 - acc: 0.7677 - val_loss: 1.2507 - val_acc: 0.6738\n",
      "Epoch 85/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8480 - acc: 0.7532\n",
      "Epoch 00085: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 532us/sample - loss: 0.8393 - acc: 0.7565 - val_loss: 1.2133 - val_acc: 0.6998\n",
      "Epoch 86/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8391 - acc: 0.7781\n",
      "Epoch 00086: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 555us/sample - loss: 0.8439 - acc: 0.7754 - val_loss: 1.2444 - val_acc: 0.6832\n",
      "Epoch 87/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8442 - acc: 0.7638\n",
      "Epoch 00087: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 0.8449 - acc: 0.7642 - val_loss: 1.2409 - val_acc: 0.6809\n",
      "Epoch 88/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8167 - acc: 0.7787\n",
      "Epoch 00088: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 525us/sample - loss: 0.8225 - acc: 0.7796 - val_loss: 1.2860 - val_acc: 0.6525\n",
      "Epoch 89/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8421 - acc: 0.7635\n",
      "Epoch 00089: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 571us/sample - loss: 0.8422 - acc: 0.7630 - val_loss: 1.2728 - val_acc: 0.6903\n",
      "Epoch 90/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8045 - acc: 0.7844\n",
      "Epoch 00090: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 566us/sample - loss: 0.8106 - acc: 0.7813 - val_loss: 1.2492 - val_acc: 0.6785\n",
      "Epoch 91/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8497 - acc: 0.7544\n",
      "Epoch 00091: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 546us/sample - loss: 0.8457 - acc: 0.7541 - val_loss: 1.2152 - val_acc: 0.6738\n",
      "Epoch 92/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8131 - acc: 0.7725\n",
      "Epoch 00092: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 562us/sample - loss: 0.8255 - acc: 0.7719 - val_loss: 1.3258 - val_acc: 0.6572\n",
      "Epoch 93/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8012 - acc: 0.7881\n",
      "Epoch 00093: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 575us/sample - loss: 0.8007 - acc: 0.7861 - val_loss: 1.3325 - val_acc: 0.6761\n",
      "Epoch 94/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.7923 - acc: 0.7960\n",
      "Epoch 00094: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 578us/sample - loss: 0.7883 - acc: 0.7961 - val_loss: 1.3402 - val_acc: 0.6690\n",
      "Epoch 95/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8077 - acc: 0.7736\n",
      "Epoch 00095: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 573us/sample - loss: 0.8156 - acc: 0.7701 - val_loss: 1.2715 - val_acc: 0.7045\n",
      "Epoch 96/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8166 - acc: 0.7570\n",
      "Epoch 00096: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 575us/sample - loss: 0.8126 - acc: 0.7583 - val_loss: 1.2195 - val_acc: 0.6714\n",
      "Epoch 97/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7915 - acc: 0.7885\n",
      "Epoch 00097: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 582us/sample - loss: 0.7900 - acc: 0.7896 - val_loss: 1.2724 - val_acc: 0.6903\n",
      "Epoch 98/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8021 - acc: 0.7837\n",
      "Epoch 00098: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 570us/sample - loss: 0.8089 - acc: 0.7813 - val_loss: 1.2988 - val_acc: 0.6714\n",
      "Epoch 99/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7940 - acc: 0.7876\n",
      "Epoch 00099: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 0.7871 - acc: 0.7896 - val_loss: 1.2377 - val_acc: 0.6761\n",
      "Epoch 100/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7825 - acc: 0.7921\n",
      "Epoch 00100: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 553us/sample - loss: 0.7861 - acc: 0.7914 - val_loss: 1.2557 - val_acc: 0.6785\n",
      "443/443 [==============================] - 0s 245us/sample - loss: 1.3722 - acc: 0.6704\n",
      "--- Starting trial: run-12\n",
      "{'TIME_WINDOW': 1000, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 128, 'LAST_HIDDEN': 500, 'LAST_DROPOUT': 0.2, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_98 (InputLayer)        [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute_97 (Permute)         (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_194 (Conv1D)          (None, 997, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_194 (Bat (None, 997, 32)           3988      \n",
      "_________________________________________________________________\n",
      "activation_194 (Activation)  (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_291 (Dropout)        (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_194 (MaxPoolin (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_195 (Conv1D)          (None, 246, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_195 (Bat (None, 246, 64)           984       \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_292 (Dropout)        (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_195 (MaxPoolin (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_291 (LSTM)              (None, 61, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_292 (LSTM)              (None, 61, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_293 (LSTM)              (None, 61, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_97 (Flatten)         (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 500)               3904500   \n",
      "_________________________________________________________________\n",
      "dropout_293 (Dropout)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 4)                 2004      \n",
      "=================================================================\n",
      "Total params: 4,284,564\n",
      "Trainable params: 4,282,078\n",
      "Non-trainable params: 2,486\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.2663 - acc: 0.2972\n",
      "Epoch 00001: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 3.2479 - acc: 0.2973 - val_loss: 2.9118 - val_acc: 0.3783\n",
      "Epoch 2/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.8479 - acc: 0.3744\n",
      "Epoch 00002: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 581us/sample - loss: 2.8441 - acc: 0.3735 - val_loss: 2.7046 - val_acc: 0.3712\n",
      "Epoch 3/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.6782 - acc: 0.3928\n",
      "Epoch 00003: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 571us/sample - loss: 2.6748 - acc: 0.3954 - val_loss: 2.5497 - val_acc: 0.4019\n",
      "Epoch 4/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.5338 - acc: 0.4111\n",
      "Epoch 00004: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 581us/sample - loss: 2.5341 - acc: 0.4102 - val_loss: 2.4715 - val_acc: 0.3806\n",
      "Epoch 5/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.4255 - acc: 0.4207\n",
      "Epoch 00005: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 572us/sample - loss: 2.4247 - acc: 0.4214 - val_loss: 2.3605 - val_acc: 0.3995\n",
      "Epoch 6/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.3081 - acc: 0.4573\n",
      "Epoch 00006: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 568us/sample - loss: 2.3056 - acc: 0.4574 - val_loss: 2.2602 - val_acc: 0.4184\n",
      "Epoch 7/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.2016 - acc: 0.4800\n",
      "Epoch 00007: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 552us/sample - loss: 2.2009 - acc: 0.4781 - val_loss: 2.2224 - val_acc: 0.4161\n",
      "Epoch 8/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.1189 - acc: 0.4865\n",
      "Epoch 00008: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 585us/sample - loss: 2.1176 - acc: 0.4876 - val_loss: 2.0971 - val_acc: 0.4634\n",
      "Epoch 9/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.0508 - acc: 0.4850\n",
      "Epoch 00009: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 565us/sample - loss: 2.0485 - acc: 0.4870 - val_loss: 2.0543 - val_acc: 0.4303\n",
      "Epoch 10/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.9657 - acc: 0.4976\n",
      "Epoch 00010: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 547us/sample - loss: 1.9635 - acc: 0.4988 - val_loss: 2.0547 - val_acc: 0.4137\n",
      "Epoch 11/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.8862 - acc: 0.5344\n",
      "Epoch 00011: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 1.8821 - acc: 0.5378 - val_loss: 1.9530 - val_acc: 0.4303\n",
      "Epoch 12/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.8448 - acc: 0.5217\n",
      "Epoch 00012: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 502us/sample - loss: 1.8405 - acc: 0.5195 - val_loss: 1.8875 - val_acc: 0.4539\n",
      "Epoch 13/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.7952 - acc: 0.5293\n",
      "Epoch 00013: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 497us/sample - loss: 1.7848 - acc: 0.5319 - val_loss: 1.8481 - val_acc: 0.4586\n",
      "Epoch 14/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.7142 - acc: 0.5542\n",
      "Epoch 00014: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 1.7200 - acc: 0.5479 - val_loss: 1.7593 - val_acc: 0.5083\n",
      "Epoch 15/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.6802 - acc: 0.5593\n",
      "Epoch 00015: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 509us/sample - loss: 1.6789 - acc: 0.5609 - val_loss: 1.6948 - val_acc: 0.5272\n",
      "Epoch 16/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.6451 - acc: 0.5547\n",
      "Epoch 00016: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 511us/sample - loss: 1.6447 - acc: 0.5556 - val_loss: 1.7313 - val_acc: 0.5035\n",
      "Epoch 17/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5997 - acc: 0.5644\n",
      "Epoch 00017: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 494us/sample - loss: 1.6018 - acc: 0.5609 - val_loss: 1.6471 - val_acc: 0.5461\n",
      "Epoch 18/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5803 - acc: 0.5587\n",
      "Epoch 00018: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 500us/sample - loss: 1.5735 - acc: 0.5638 - val_loss: 1.5856 - val_acc: 0.5437\n",
      "Epoch 19/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5359 - acc: 0.5631\n",
      "Epoch 00019: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 494us/sample - loss: 1.5343 - acc: 0.5632 - val_loss: 1.5786 - val_acc: 0.5390\n",
      "Epoch 20/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4928 - acc: 0.5740\n",
      "Epoch 00020: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 1.4986 - acc: 0.5668 - val_loss: 1.8014 - val_acc: 0.3948\n",
      "Epoch 21/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4733 - acc: 0.5759\n",
      "Epoch 00021: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 502us/sample - loss: 1.4829 - acc: 0.5691 - val_loss: 1.5260 - val_acc: 0.5437\n",
      "Epoch 22/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4449 - acc: 0.5705\n",
      "Epoch 00022: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 525us/sample - loss: 1.4473 - acc: 0.5686 - val_loss: 1.4635 - val_acc: 0.5839\n",
      "Epoch 23/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4203 - acc: 0.5760\n",
      "Epoch 00023: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 577us/sample - loss: 1.4248 - acc: 0.5721 - val_loss: 1.5118 - val_acc: 0.5177\n",
      "Epoch 24/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3680 - acc: 0.6014\n",
      "Epoch 00024: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 493us/sample - loss: 1.3753 - acc: 0.5987 - val_loss: 1.5750 - val_acc: 0.5035\n",
      "Epoch 25/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3669 - acc: 0.5975\n",
      "Epoch 00025: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 588us/sample - loss: 1.3619 - acc: 0.6011 - val_loss: 1.5024 - val_acc: 0.5296\n",
      "Epoch 26/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3397 - acc: 0.6231\n",
      "Epoch 00026: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 1.3391 - acc: 0.6206 - val_loss: 1.5197 - val_acc: 0.5130\n",
      "Epoch 27/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3493 - acc: 0.5938\n",
      "Epoch 00027: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 500us/sample - loss: 1.3506 - acc: 0.5922 - val_loss: 1.4075 - val_acc: 0.5650\n",
      "Epoch 28/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3380 - acc: 0.6065\n",
      "Epoch 00028: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 493us/sample - loss: 1.3336 - acc: 0.6082 - val_loss: 1.3913 - val_acc: 0.5768\n",
      "Epoch 29/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3106 - acc: 0.6148\n",
      "Epoch 00029: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 1.3052 - acc: 0.6170 - val_loss: 1.4365 - val_acc: 0.5579\n",
      "Epoch 30/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2842 - acc: 0.6231\n",
      "Epoch 00030: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 1.2907 - acc: 0.6176 - val_loss: 1.3424 - val_acc: 0.5934\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2609 - acc: 0.6295\n",
      "Epoch 00031: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 1.2656 - acc: 0.6247 - val_loss: 1.4746 - val_acc: 0.5319\n",
      "Epoch 32/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2463 - acc: 0.6154\n",
      "Epoch 00032: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 519us/sample - loss: 1.2552 - acc: 0.6147 - val_loss: 1.3566 - val_acc: 0.6028\n",
      "Epoch 33/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2226 - acc: 0.6454\n",
      "Epoch 00033: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 514us/sample - loss: 1.2341 - acc: 0.6389 - val_loss: 1.3223 - val_acc: 0.6241\n",
      "Epoch 34/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2207 - acc: 0.6320\n",
      "Epoch 00034: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 1.2224 - acc: 0.6300 - val_loss: 1.3453 - val_acc: 0.5981\n",
      "Epoch 35/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2149 - acc: 0.6390\n",
      "Epoch 00035: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 1.2107 - acc: 0.6413 - val_loss: 1.3602 - val_acc: 0.5697\n",
      "Epoch 36/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1908 - acc: 0.6441\n",
      "Epoch 00036: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 1.1931 - acc: 0.6407 - val_loss: 1.2995 - val_acc: 0.6028\n",
      "Epoch 37/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1730 - acc: 0.6460\n",
      "Epoch 00037: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 1.1706 - acc: 0.6489 - val_loss: 1.3547 - val_acc: 0.5697\n",
      "Epoch 38/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1671 - acc: 0.6665\n",
      "Epoch 00038: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 1.1709 - acc: 0.6596 - val_loss: 1.3212 - val_acc: 0.6076\n",
      "Epoch 39/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1564 - acc: 0.6562\n",
      "Epoch 00039: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 1.1556 - acc: 0.6566 - val_loss: 1.2989 - val_acc: 0.6194\n",
      "Epoch 40/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1628 - acc: 0.6422\n",
      "Epoch 00040: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 495us/sample - loss: 1.1586 - acc: 0.6407 - val_loss: 1.3387 - val_acc: 0.6028\n",
      "Epoch 41/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1550 - acc: 0.6454\n",
      "Epoch 00041: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 1.1429 - acc: 0.6507 - val_loss: 1.3064 - val_acc: 0.5910\n",
      "Epoch 42/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1189 - acc: 0.6747\n",
      "Epoch 00042: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 497us/sample - loss: 1.1196 - acc: 0.6738 - val_loss: 1.2867 - val_acc: 0.6005\n",
      "Epoch 43/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1228 - acc: 0.6703\n",
      "Epoch 00043: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 512us/sample - loss: 1.1246 - acc: 0.6690 - val_loss: 1.3348 - val_acc: 0.5792\n",
      "Epoch 44/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0888 - acc: 0.6837\n",
      "Epoch 00044: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 1.0898 - acc: 0.6832 - val_loss: 1.3356 - val_acc: 0.5816\n",
      "Epoch 45/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1111 - acc: 0.6639\n",
      "Epoch 00045: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 496us/sample - loss: 1.1129 - acc: 0.6631 - val_loss: 1.2575 - val_acc: 0.6336\n",
      "Epoch 46/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0837 - acc: 0.6837\n",
      "Epoch 00046: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 1.0845 - acc: 0.6809 - val_loss: 1.3254 - val_acc: 0.5910\n",
      "Epoch 47/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0806 - acc: 0.6735\n",
      "Epoch 00047: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 1.0800 - acc: 0.6714 - val_loss: 1.2491 - val_acc: 0.6241\n",
      "Epoch 48/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0712 - acc: 0.6747\n",
      "Epoch 00048: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 508us/sample - loss: 1.0682 - acc: 0.6761 - val_loss: 1.2632 - val_acc: 0.6052\n",
      "Epoch 49/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0602 - acc: 0.6818\n",
      "Epoch 00049: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 508us/sample - loss: 1.0563 - acc: 0.6814 - val_loss: 1.3018 - val_acc: 0.6028\n",
      "Epoch 50/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0497 - acc: 0.6945\n",
      "Epoch 00050: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 507us/sample - loss: 1.0455 - acc: 0.6974 - val_loss: 1.3010 - val_acc: 0.6147\n",
      "Epoch 51/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0384 - acc: 0.6996\n",
      "Epoch 00051: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 497us/sample - loss: 1.0404 - acc: 0.7009 - val_loss: 1.3559 - val_acc: 0.5934\n",
      "Epoch 52/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0366 - acc: 0.7022\n",
      "Epoch 00052: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 507us/sample - loss: 1.0401 - acc: 0.7004 - val_loss: 1.2737 - val_acc: 0.6288\n",
      "Epoch 53/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0356 - acc: 0.6977\n",
      "Epoch 00053: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 1.0319 - acc: 0.6992 - val_loss: 1.2326 - val_acc: 0.6312\n",
      "Epoch 54/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0534 - acc: 0.6773\n",
      "Epoch 00054: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 1.0596 - acc: 0.6749 - val_loss: 1.2696 - val_acc: 0.6052\n",
      "Epoch 55/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0471 - acc: 0.6869\n",
      "Epoch 00055: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 513us/sample - loss: 1.0462 - acc: 0.6850 - val_loss: 1.2952 - val_acc: 0.6099\n",
      "Epoch 56/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9947 - acc: 0.7130\n",
      "Epoch 00056: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 1.0004 - acc: 0.7092 - val_loss: 1.2808 - val_acc: 0.6265\n",
      "Epoch 57/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0115 - acc: 0.7015\n",
      "Epoch 00057: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 510us/sample - loss: 1.0156 - acc: 0.7021 - val_loss: 1.3197 - val_acc: 0.5768\n",
      "Epoch 58/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9994 - acc: 0.7136\n",
      "Epoch 00058: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 507us/sample - loss: 0.9931 - acc: 0.7193 - val_loss: 1.2469 - val_acc: 0.6123\n",
      "Epoch 59/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9959 - acc: 0.7092\n",
      "Epoch 00059: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 502us/sample - loss: 1.0003 - acc: 0.7057 - val_loss: 1.3856 - val_acc: 0.5863\n",
      "Epoch 60/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0040 - acc: 0.7003\n",
      "Epoch 00060: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 505us/sample - loss: 0.9951 - acc: 0.7039 - val_loss: 1.2501 - val_acc: 0.6241\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9834 - acc: 0.7092\n",
      "Epoch 00061: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 0.9858 - acc: 0.7098 - val_loss: 1.2664 - val_acc: 0.6241\n",
      "Epoch 62/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0164 - acc: 0.6939\n",
      "Epoch 00062: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 500us/sample - loss: 1.0104 - acc: 0.6992 - val_loss: 1.3206 - val_acc: 0.6076\n",
      "Epoch 63/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9568 - acc: 0.7239\n",
      "Epoch 00063: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 0.9552 - acc: 0.7246 - val_loss: 1.3510 - val_acc: 0.6052\n",
      "Epoch 64/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9637 - acc: 0.7347\n",
      "Epoch 00064: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 507us/sample - loss: 0.9707 - acc: 0.7299 - val_loss: 1.3467 - val_acc: 0.5957\n",
      "Epoch 65/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9337 - acc: 0.7423\n",
      "Epoch 00065: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 0.9331 - acc: 0.7429 - val_loss: 1.3088 - val_acc: 0.6005\n",
      "Epoch 66/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9573 - acc: 0.7251\n",
      "Epoch 00066: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 507us/sample - loss: 0.9579 - acc: 0.7246 - val_loss: 1.3628 - val_acc: 0.6052\n",
      "Epoch 67/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9287 - acc: 0.7321\n",
      "Epoch 00067: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 511us/sample - loss: 0.9311 - acc: 0.7323 - val_loss: 1.3073 - val_acc: 0.6076\n",
      "Epoch 68/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9516 - acc: 0.7111\n",
      "Epoch 00068: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 509us/sample - loss: 0.9467 - acc: 0.7128 - val_loss: 1.3672 - val_acc: 0.5981\n",
      "Epoch 69/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9391 - acc: 0.7347\n",
      "Epoch 00069: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 500us/sample - loss: 0.9417 - acc: 0.7299 - val_loss: 1.3367 - val_acc: 0.6005\n",
      "Epoch 70/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9269 - acc: 0.7321\n",
      "Epoch 00070: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 508us/sample - loss: 0.9392 - acc: 0.7281 - val_loss: 1.3057 - val_acc: 0.6099\n",
      "Epoch 71/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9138 - acc: 0.7334\n",
      "Epoch 00071: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 509us/sample - loss: 0.9272 - acc: 0.7287 - val_loss: 1.3079 - val_acc: 0.6288\n",
      "Epoch 72/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9187 - acc: 0.7411\n",
      "Epoch 00072: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 497us/sample - loss: 0.9263 - acc: 0.7352 - val_loss: 1.2628 - val_acc: 0.6430\n",
      "Epoch 73/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8891 - acc: 0.7474\n",
      "Epoch 00073: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 510us/sample - loss: 0.8909 - acc: 0.7488 - val_loss: 1.3283 - val_acc: 0.6407\n",
      "Epoch 74/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9160 - acc: 0.7302\n",
      "Epoch 00074: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 505us/sample - loss: 0.9247 - acc: 0.7258 - val_loss: 1.2908 - val_acc: 0.6548\n",
      "Epoch 75/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9112 - acc: 0.7258\n",
      "Epoch 00075: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 507us/sample - loss: 0.9088 - acc: 0.7275 - val_loss: 1.3284 - val_acc: 0.6288\n",
      "Epoch 76/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8999 - acc: 0.7481\n",
      "Epoch 00076: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 509us/sample - loss: 0.9082 - acc: 0.7465 - val_loss: 1.3798 - val_acc: 0.5981\n",
      "Epoch 77/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8808 - acc: 0.7545\n",
      "Epoch 00077: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 0.8895 - acc: 0.7500 - val_loss: 1.3024 - val_acc: 0.6359\n",
      "Epoch 78/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8932 - acc: 0.7423\n",
      "Epoch 00078: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 505us/sample - loss: 0.8955 - acc: 0.7417 - val_loss: 1.3060 - val_acc: 0.6501\n",
      "Epoch 79/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8872 - acc: 0.7481\n",
      "Epoch 00079: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 511us/sample - loss: 0.8851 - acc: 0.7465 - val_loss: 1.4013 - val_acc: 0.6123\n",
      "Epoch 80/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8883 - acc: 0.7545\n",
      "Epoch 00080: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 510us/sample - loss: 0.8916 - acc: 0.7565 - val_loss: 1.3951 - val_acc: 0.6170\n",
      "Epoch 81/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8742 - acc: 0.7602\n",
      "Epoch 00081: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 516us/sample - loss: 0.8770 - acc: 0.7559 - val_loss: 1.3422 - val_acc: 0.6123\n",
      "Epoch 82/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8686 - acc: 0.7628\n",
      "Epoch 00082: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 509us/sample - loss: 0.8741 - acc: 0.7606 - val_loss: 1.3812 - val_acc: 0.6123\n",
      "Epoch 83/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8656 - acc: 0.7685\n",
      "Epoch 00083: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 0.8639 - acc: 0.7654 - val_loss: 1.3520 - val_acc: 0.6288\n",
      "Epoch 84/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8400 - acc: 0.7749\n",
      "Epoch 00084: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 496us/sample - loss: 0.8394 - acc: 0.7760 - val_loss: 1.3986 - val_acc: 0.6241\n",
      "Epoch 85/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8659 - acc: 0.7787\n",
      "Epoch 00085: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 509us/sample - loss: 0.8667 - acc: 0.7778 - val_loss: 1.4728 - val_acc: 0.6005\n",
      "Epoch 86/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8649 - acc: 0.7513\n",
      "Epoch 00086: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 498us/sample - loss: 0.8596 - acc: 0.7524 - val_loss: 1.3974 - val_acc: 0.6217\n",
      "Epoch 87/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8530 - acc: 0.7647\n",
      "Epoch 00087: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 0.8539 - acc: 0.7636 - val_loss: 1.3285 - val_acc: 0.6572\n",
      "Epoch 88/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8385 - acc: 0.7742\n",
      "Epoch 00088: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 0.8406 - acc: 0.7736 - val_loss: 1.4014 - val_acc: 0.6525\n",
      "Epoch 89/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8640 - acc: 0.7551\n",
      "Epoch 00089: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 0.8638 - acc: 0.7530 - val_loss: 1.4005 - val_acc: 0.6478\n",
      "Epoch 90/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8213 - acc: 0.7819\n",
      "Epoch 00090: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 502us/sample - loss: 0.8294 - acc: 0.7766 - val_loss: 1.4203 - val_acc: 0.6288\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8361 - acc: 0.7634\n",
      "Epoch 00091: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 498us/sample - loss: 0.8437 - acc: 0.7642 - val_loss: 1.4059 - val_acc: 0.6430\n",
      "Epoch 92/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8043 - acc: 0.7755\n",
      "Epoch 00092: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 498us/sample - loss: 0.8097 - acc: 0.7760 - val_loss: 1.3768 - val_acc: 0.6619\n",
      "Epoch 93/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8091 - acc: 0.7781\n",
      "Epoch 00093: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 510us/sample - loss: 0.8140 - acc: 0.7801 - val_loss: 1.3466 - val_acc: 0.6761\n",
      "Epoch 94/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8202 - acc: 0.7621\n",
      "Epoch 00094: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 500us/sample - loss: 0.8177 - acc: 0.7618 - val_loss: 1.3505 - val_acc: 0.6501\n",
      "Epoch 95/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8153 - acc: 0.7927\n",
      "Epoch 00095: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 0.8226 - acc: 0.7896 - val_loss: 1.4575 - val_acc: 0.6430\n",
      "Epoch 96/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8125 - acc: 0.7800\n",
      "Epoch 00096: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 512us/sample - loss: 0.8063 - acc: 0.7807 - val_loss: 1.4744 - val_acc: 0.6194\n",
      "Epoch 97/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7972 - acc: 0.7768\n",
      "Epoch 00097: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 509us/sample - loss: 0.8029 - acc: 0.7713 - val_loss: 1.5001 - val_acc: 0.6312\n",
      "Epoch 98/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7993 - acc: 0.7774\n",
      "Epoch 00098: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 508us/sample - loss: 0.7993 - acc: 0.7766 - val_loss: 1.4799 - val_acc: 0.6383\n",
      "Epoch 99/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7928 - acc: 0.7787\n",
      "Epoch 00099: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 0.7830 - acc: 0.7837 - val_loss: 1.4279 - val_acc: 0.6288\n",
      "Epoch 100/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7826 - acc: 0.7844\n",
      "Epoch 00100: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 495us/sample - loss: 0.7876 - acc: 0.7825 - val_loss: 1.4545 - val_acc: 0.6596\n",
      "443/443 [==============================] - 0s 185us/sample - loss: 1.4441 - acc: 0.6117\n",
      "--- Starting trial: run-13\n",
      "{'TIME_WINDOW': 1000, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 128, 'LAST_HIDDEN': 500, 'LAST_DROPOUT': 0.3, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_98\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_99 (InputLayer)        [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute_98 (Permute)         (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_196 (Conv1D)          (None, 997, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_196 (Bat (None, 997, 32)           3988      \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_294 (Dropout)        (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_196 (MaxPoolin (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_197 (Conv1D)          (None, 246, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_197 (Bat (None, 246, 64)           984       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_295 (Dropout)        (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_197 (MaxPoolin (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_294 (LSTM)              (None, 61, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_295 (LSTM)              (None, 61, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_296 (LSTM)              (None, 61, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_98 (Flatten)         (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 500)               3904500   \n",
      "_________________________________________________________________\n",
      "dropout_296 (Dropout)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 4)                 2004      \n",
      "=================================================================\n",
      "Total params: 4,284,564\n",
      "Trainable params: 4,282,078\n",
      "Non-trainable params: 2,486\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.2296 - acc: 0.2800\n",
      "Epoch 00001: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 3.2102 - acc: 0.2813 - val_loss: 2.8667 - val_acc: 0.3759\n",
      "Epoch 2/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.8327 - acc: 0.3565\n",
      "Epoch 00002: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 508us/sample - loss: 2.8242 - acc: 0.3587 - val_loss: 2.6838 - val_acc: 0.3924\n",
      "Epoch 3/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.6819 - acc: 0.3648\n",
      "Epoch 00003: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 507us/sample - loss: 2.6747 - acc: 0.3676 - val_loss: 2.5486 - val_acc: 0.4043\n",
      "Epoch 4/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.5523 - acc: 0.3903\n",
      "Epoch 00004: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 513us/sample - loss: 2.5471 - acc: 0.3907 - val_loss: 2.4422 - val_acc: 0.4113\n",
      "Epoch 5/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.4341 - acc: 0.4037\n",
      "Epoch 00005: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 496us/sample - loss: 2.4289 - acc: 0.4066 - val_loss: 2.3475 - val_acc: 0.4374\n",
      "Epoch 6/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.3444 - acc: 0.4235\n",
      "Epoch 00006: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 2.3393 - acc: 0.4243 - val_loss: 2.2533 - val_acc: 0.4374\n",
      "Epoch 7/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.2212 - acc: 0.4528\n",
      "Epoch 00007: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 2.2197 - acc: 0.4521 - val_loss: 2.1836 - val_acc: 0.4563\n",
      "Epoch 8/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.1426 - acc: 0.4547\n",
      "Epoch 00008: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 495us/sample - loss: 2.1461 - acc: 0.4504 - val_loss: 2.1096 - val_acc: 0.4303\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.0541 - acc: 0.4783\n",
      "Epoch 00009: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 508us/sample - loss: 2.0549 - acc: 0.4775 - val_loss: 2.0341 - val_acc: 0.4704\n",
      "Epoch 10/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.9945 - acc: 0.4853\n",
      "Epoch 00010: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 512us/sample - loss: 1.9929 - acc: 0.4811 - val_loss: 1.9735 - val_acc: 0.4610\n",
      "Epoch 11/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.9059 - acc: 0.5166\n",
      "Epoch 00011: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 498us/sample - loss: 1.9069 - acc: 0.5165 - val_loss: 1.8789 - val_acc: 0.5106\n",
      "Epoch 12/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.8725 - acc: 0.4970\n",
      "Epoch 00012: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 505us/sample - loss: 1.8717 - acc: 0.4970 - val_loss: 1.8830 - val_acc: 0.4657\n",
      "Epoch 13/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.7899 - acc: 0.5306\n",
      "Epoch 00013: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 508us/sample - loss: 1.7878 - acc: 0.5296 - val_loss: 1.8031 - val_acc: 0.4752\n",
      "Epoch 14/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.7265 - acc: 0.5351\n",
      "Epoch 00014: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 1.7242 - acc: 0.5372 - val_loss: 1.7539 - val_acc: 0.4917\n",
      "Epoch 15/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.6841 - acc: 0.5427\n",
      "Epoch 00015: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 502us/sample - loss: 1.6829 - acc: 0.5396 - val_loss: 1.7148 - val_acc: 0.5106\n",
      "Epoch 16/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.6437 - acc: 0.5427\n",
      "Epoch 00016: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 496us/sample - loss: 1.6416 - acc: 0.5437 - val_loss: 1.6631 - val_acc: 0.5201\n",
      "Epoch 17/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.6233 - acc: 0.5472\n",
      "Epoch 00017: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 513us/sample - loss: 1.6210 - acc: 0.5449 - val_loss: 1.6135 - val_acc: 0.5437\n",
      "Epoch 18/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5730 - acc: 0.5587\n",
      "Epoch 00018: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 1.5717 - acc: 0.5561 - val_loss: 1.6306 - val_acc: 0.5225\n",
      "Epoch 19/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5297 - acc: 0.5695\n",
      "Epoch 00019: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 509us/sample - loss: 1.5294 - acc: 0.5697 - val_loss: 1.5321 - val_acc: 0.5721\n",
      "Epoch 20/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4874 - acc: 0.5746\n",
      "Epoch 00020: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 500us/sample - loss: 1.4868 - acc: 0.5709 - val_loss: 1.5868 - val_acc: 0.5130\n",
      "Epoch 21/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4459 - acc: 0.5918\n",
      "Epoch 00021: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 512us/sample - loss: 1.4541 - acc: 0.5881 - val_loss: 1.6192 - val_acc: 0.4941\n",
      "Epoch 22/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4393 - acc: 0.5861\n",
      "Epoch 00022: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 1.4435 - acc: 0.5839 - val_loss: 1.4694 - val_acc: 0.5745\n",
      "Epoch 23/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4108 - acc: 0.5886\n",
      "Epoch 00023: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 510us/sample - loss: 1.4157 - acc: 0.5881 - val_loss: 1.4864 - val_acc: 0.5437\n",
      "Epoch 24/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4157 - acc: 0.5695\n",
      "Epoch 00024: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 1.4130 - acc: 0.5745 - val_loss: 1.4908 - val_acc: 0.5461\n",
      "Epoch 25/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3788 - acc: 0.6071\n",
      "Epoch 00025: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 502us/sample - loss: 1.3749 - acc: 0.6099 - val_loss: 1.4789 - val_acc: 0.5674\n",
      "Epoch 26/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3708 - acc: 0.5861\n",
      "Epoch 00026: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 510us/sample - loss: 1.3660 - acc: 0.5910 - val_loss: 1.4006 - val_acc: 0.5674\n",
      "Epoch 27/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3267 - acc: 0.5982\n",
      "Epoch 00027: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 505us/sample - loss: 1.3348 - acc: 0.5975 - val_loss: 1.4272 - val_acc: 0.5768\n",
      "Epoch 28/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3349 - acc: 0.5899\n",
      "Epoch 00028: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 498us/sample - loss: 1.3410 - acc: 0.5875 - val_loss: 1.3804 - val_acc: 0.6005\n",
      "Epoch 29/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2961 - acc: 0.6148\n",
      "Epoch 00029: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 510us/sample - loss: 1.2989 - acc: 0.6141 - val_loss: 1.3937 - val_acc: 0.5768\n",
      "Epoch 30/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2809 - acc: 0.6224\n",
      "Epoch 00030: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 496us/sample - loss: 1.2778 - acc: 0.6206 - val_loss: 1.3562 - val_acc: 0.6123\n",
      "Epoch 31/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2645 - acc: 0.6282\n",
      "Epoch 00031: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 1.2695 - acc: 0.6253 - val_loss: 1.3294 - val_acc: 0.6052\n",
      "Epoch 32/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2464 - acc: 0.6276\n",
      "Epoch 00032: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 507us/sample - loss: 1.2463 - acc: 0.6277 - val_loss: 1.3308 - val_acc: 0.6028\n",
      "Epoch 33/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2417 - acc: 0.6148\n",
      "Epoch 00033: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 1.2436 - acc: 0.6152 - val_loss: 1.3049 - val_acc: 0.6099\n",
      "Epoch 34/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2294 - acc: 0.6282\n",
      "Epoch 00034: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 1.2256 - acc: 0.6294 - val_loss: 1.3616 - val_acc: 0.5792\n",
      "Epoch 35/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2249 - acc: 0.6346\n",
      "Epoch 00035: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 1.2182 - acc: 0.6377 - val_loss: 1.3118 - val_acc: 0.6076\n",
      "Epoch 36/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.2074 - acc: 0.6352\n",
      "Epoch 00036: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 507us/sample - loss: 1.2028 - acc: 0.6395 - val_loss: 1.2894 - val_acc: 0.6265\n",
      "Epoch 37/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1800 - acc: 0.6531\n",
      "Epoch 00037: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 1.1817 - acc: 0.6537 - val_loss: 1.3322 - val_acc: 0.5910\n",
      "Epoch 38/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1620 - acc: 0.6409\n",
      "Epoch 00038: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 502us/sample - loss: 1.1553 - acc: 0.6472 - val_loss: 1.2758 - val_acc: 0.6336\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1593 - acc: 0.6454\n",
      "Epoch 00039: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 511us/sample - loss: 1.1637 - acc: 0.6436 - val_loss: 1.2893 - val_acc: 0.6194\n",
      "Epoch 40/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1559 - acc: 0.6531\n",
      "Epoch 00040: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 502us/sample - loss: 1.1545 - acc: 0.6525 - val_loss: 1.3592 - val_acc: 0.5697\n",
      "Epoch 41/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1584 - acc: 0.6614\n",
      "Epoch 00041: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 515us/sample - loss: 1.1582 - acc: 0.6608 - val_loss: 1.2626 - val_acc: 0.6241\n",
      "Epoch 42/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1204 - acc: 0.6677\n",
      "Epoch 00042: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 505us/sample - loss: 1.1227 - acc: 0.6649 - val_loss: 1.2764 - val_acc: 0.6336\n",
      "Epoch 43/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1259 - acc: 0.6562\n",
      "Epoch 00043: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 1.1246 - acc: 0.6572 - val_loss: 1.2788 - val_acc: 0.6147\n",
      "Epoch 44/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1195 - acc: 0.6626\n",
      "Epoch 00044: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 514us/sample - loss: 1.1193 - acc: 0.6613 - val_loss: 1.2772 - val_acc: 0.6241\n",
      "Epoch 45/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1101 - acc: 0.6761\n",
      "Epoch 00045: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 511us/sample - loss: 1.1079 - acc: 0.6767 - val_loss: 1.2618 - val_acc: 0.6312\n",
      "Epoch 46/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0916 - acc: 0.6722\n",
      "Epoch 00046: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 507us/sample - loss: 1.0884 - acc: 0.6773 - val_loss: 1.2297 - val_acc: 0.6265\n",
      "Epoch 47/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0979 - acc: 0.6633\n",
      "Epoch 00047: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 1.0894 - acc: 0.6708 - val_loss: 1.2630 - val_acc: 0.6407\n",
      "Epoch 48/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0798 - acc: 0.6792\n",
      "Epoch 00048: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 496us/sample - loss: 1.0793 - acc: 0.6809 - val_loss: 1.2282 - val_acc: 0.6194\n",
      "Epoch 49/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0756 - acc: 0.6626\n",
      "Epoch 00049: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 502us/sample - loss: 1.0714 - acc: 0.6678 - val_loss: 1.2244 - val_acc: 0.6430\n",
      "Epoch 50/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0517 - acc: 0.6907\n",
      "Epoch 00050: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 1.0593 - acc: 0.6856 - val_loss: 1.2661 - val_acc: 0.6336\n",
      "Epoch 51/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0460 - acc: 0.6907\n",
      "Epoch 00051: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 510us/sample - loss: 1.0439 - acc: 0.6903 - val_loss: 1.2765 - val_acc: 0.6123\n",
      "Epoch 52/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0453 - acc: 0.6837\n",
      "Epoch 00052: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 505us/sample - loss: 1.0407 - acc: 0.6856 - val_loss: 1.2919 - val_acc: 0.6194\n",
      "Epoch 53/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0260 - acc: 0.6798\n",
      "Epoch 00053: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 508us/sample - loss: 1.0247 - acc: 0.6832 - val_loss: 1.2122 - val_acc: 0.6383\n",
      "Epoch 54/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0179 - acc: 0.6926\n",
      "Epoch 00054: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 1.0288 - acc: 0.6885 - val_loss: 1.2475 - val_acc: 0.6288\n",
      "Epoch 55/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0167 - acc: 0.6990\n",
      "Epoch 00055: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 515us/sample - loss: 1.0176 - acc: 0.6992 - val_loss: 1.2485 - val_acc: 0.6478\n",
      "Epoch 56/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0130 - acc: 0.6856\n",
      "Epoch 00056: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 495us/sample - loss: 1.0170 - acc: 0.6826 - val_loss: 1.2585 - val_acc: 0.6241\n",
      "Epoch 57/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0437 - acc: 0.6990\n",
      "Epoch 00057: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 505us/sample - loss: 1.0437 - acc: 0.6968 - val_loss: 1.2533 - val_acc: 0.6383\n",
      "Epoch 58/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0230 - acc: 0.6958\n",
      "Epoch 00058: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 516us/sample - loss: 1.0244 - acc: 0.6921 - val_loss: 1.2239 - val_acc: 0.6170\n",
      "Epoch 59/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9953 - acc: 0.6901\n",
      "Epoch 00059: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 500us/sample - loss: 0.9911 - acc: 0.6950 - val_loss: 1.2281 - val_acc: 0.6430\n",
      "Epoch 60/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0025 - acc: 0.7092\n",
      "Epoch 00060: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 1.0087 - acc: 0.7033 - val_loss: 1.2272 - val_acc: 0.6383\n",
      "Epoch 61/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0194 - acc: 0.6901\n",
      "Epoch 00061: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 1.0095 - acc: 0.6933 - val_loss: 1.2041 - val_acc: 0.6525\n",
      "Epoch 62/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9806 - acc: 0.7098\n",
      "Epoch 00062: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 494us/sample - loss: 0.9835 - acc: 0.7092 - val_loss: 1.2339 - val_acc: 0.6407\n",
      "Epoch 63/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9686 - acc: 0.7124\n",
      "Epoch 00063: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 509us/sample - loss: 0.9725 - acc: 0.7104 - val_loss: 1.2053 - val_acc: 0.6383\n",
      "Epoch 64/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9632 - acc: 0.7219\n",
      "Epoch 00064: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 500us/sample - loss: 0.9603 - acc: 0.7210 - val_loss: 1.2970 - val_acc: 0.6265\n",
      "Epoch 65/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9352 - acc: 0.7296\n",
      "Epoch 00065: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 508us/sample - loss: 0.9314 - acc: 0.7311 - val_loss: 1.2300 - val_acc: 0.6430\n",
      "Epoch 66/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9296 - acc: 0.7328\n",
      "Epoch 00066: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 0.9222 - acc: 0.7376 - val_loss: 1.2291 - val_acc: 0.6454\n",
      "Epoch 67/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9445 - acc: 0.7321\n",
      "Epoch 00067: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 504us/sample - loss: 0.9500 - acc: 0.7305 - val_loss: 1.2429 - val_acc: 0.6430\n",
      "Epoch 68/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9262 - acc: 0.7423\n",
      "Epoch 00068: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 498us/sample - loss: 0.9317 - acc: 0.7370 - val_loss: 1.1997 - val_acc: 0.6643\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9563 - acc: 0.7219\n",
      "Epoch 00069: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 511us/sample - loss: 0.9607 - acc: 0.7199 - val_loss: 1.2316 - val_acc: 0.6430\n",
      "Epoch 70/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9211 - acc: 0.7366\n",
      "Epoch 00070: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 497us/sample - loss: 0.9193 - acc: 0.7364 - val_loss: 1.1965 - val_acc: 0.6643\n",
      "Epoch 71/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8896 - acc: 0.7615\n",
      "Epoch 00071: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 511us/sample - loss: 0.8976 - acc: 0.7571 - val_loss: 1.2418 - val_acc: 0.6596\n",
      "Epoch 72/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9152 - acc: 0.7321\n",
      "Epoch 00072: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 0.9149 - acc: 0.7299 - val_loss: 1.2465 - val_acc: 0.6525\n",
      "Epoch 73/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9067 - acc: 0.7404\n",
      "Epoch 00073: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 0.9008 - acc: 0.7441 - val_loss: 1.2656 - val_acc: 0.6359\n",
      "Epoch 74/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9004 - acc: 0.7392\n",
      "Epoch 00074: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 507us/sample - loss: 0.9068 - acc: 0.7352 - val_loss: 1.2621 - val_acc: 0.6525\n",
      "Epoch 75/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9198 - acc: 0.7380\n",
      "Epoch 00075: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 510us/sample - loss: 0.9204 - acc: 0.7370 - val_loss: 1.2531 - val_acc: 0.6478\n",
      "Epoch 76/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9053 - acc: 0.7353\n",
      "Epoch 00076: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 513us/sample - loss: 0.9002 - acc: 0.7405 - val_loss: 1.2734 - val_acc: 0.6478\n",
      "Epoch 77/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9001 - acc: 0.7423\n",
      "Epoch 00077: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 510us/sample - loss: 0.9010 - acc: 0.7417 - val_loss: 1.2231 - val_acc: 0.6596\n",
      "Epoch 78/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8763 - acc: 0.7468\n",
      "Epoch 00078: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 491us/sample - loss: 0.8806 - acc: 0.7459 - val_loss: 1.2895 - val_acc: 0.6336\n",
      "Epoch 79/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9052 - acc: 0.7481\n",
      "Epoch 00079: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 511us/sample - loss: 0.9023 - acc: 0.7512 - val_loss: 1.2673 - val_acc: 0.6454\n",
      "Epoch 80/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8725 - acc: 0.7596\n",
      "Epoch 00080: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 0.8734 - acc: 0.7583 - val_loss: 1.3406 - val_acc: 0.6312\n",
      "Epoch 81/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8852 - acc: 0.7545\n",
      "Epoch 00081: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 500us/sample - loss: 0.8952 - acc: 0.7500 - val_loss: 1.2565 - val_acc: 0.6501\n",
      "Epoch 82/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8722 - acc: 0.7570\n",
      "Epoch 00082: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 498us/sample - loss: 0.8759 - acc: 0.7541 - val_loss: 1.2345 - val_acc: 0.6501\n",
      "Epoch 83/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8477 - acc: 0.7615\n",
      "Epoch 00083: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 497us/sample - loss: 0.8519 - acc: 0.7618 - val_loss: 1.2376 - val_acc: 0.6714\n",
      "Epoch 84/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8777 - acc: 0.7577\n",
      "Epoch 00084: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 509us/sample - loss: 0.8756 - acc: 0.7583 - val_loss: 1.2628 - val_acc: 0.6572\n",
      "Epoch 85/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8714 - acc: 0.7602\n",
      "Epoch 00085: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 493us/sample - loss: 0.8718 - acc: 0.7600 - val_loss: 1.3035 - val_acc: 0.6619\n",
      "Epoch 86/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8497 - acc: 0.7608\n",
      "Epoch 00086: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 517us/sample - loss: 0.8451 - acc: 0.7648 - val_loss: 1.3041 - val_acc: 0.6501\n",
      "Epoch 87/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8307 - acc: 0.7583\n",
      "Epoch 00087: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 493us/sample - loss: 0.8342 - acc: 0.7571 - val_loss: 1.2981 - val_acc: 0.6501\n",
      "Epoch 88/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8404 - acc: 0.7640\n",
      "Epoch 00088: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 506us/sample - loss: 0.8386 - acc: 0.7630 - val_loss: 1.3406 - val_acc: 0.6336\n",
      "Epoch 89/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8465 - acc: 0.7596\n",
      "Epoch 00089: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 0.8418 - acc: 0.7606 - val_loss: 1.3101 - val_acc: 0.6383\n",
      "Epoch 90/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8225 - acc: 0.7736\n",
      "Epoch 00090: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 496us/sample - loss: 0.8219 - acc: 0.7730 - val_loss: 1.3327 - val_acc: 0.6643\n",
      "Epoch 91/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8091 - acc: 0.7755\n",
      "Epoch 00091: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 513us/sample - loss: 0.8109 - acc: 0.7766 - val_loss: 1.3406 - val_acc: 0.6430\n",
      "Epoch 92/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8254 - acc: 0.7621\n",
      "Epoch 00092: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 0.8266 - acc: 0.7612 - val_loss: 1.3121 - val_acc: 0.6383\n",
      "Epoch 93/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8188 - acc: 0.7717\n",
      "Epoch 00093: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 0.8156 - acc: 0.7736 - val_loss: 1.3853 - val_acc: 0.6336\n",
      "Epoch 94/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8221 - acc: 0.7800\n",
      "Epoch 00094: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 509us/sample - loss: 0.8164 - acc: 0.7843 - val_loss: 1.3387 - val_acc: 0.6430\n",
      "Epoch 95/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8204 - acc: 0.7666\n",
      "Epoch 00095: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 509us/sample - loss: 0.8222 - acc: 0.7683 - val_loss: 1.3460 - val_acc: 0.6241\n",
      "Epoch 96/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8161 - acc: 0.7704\n",
      "Epoch 00096: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 515us/sample - loss: 0.8184 - acc: 0.7695 - val_loss: 1.3066 - val_acc: 0.6478\n",
      "Epoch 97/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8030 - acc: 0.7742\n",
      "Epoch 00097: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 503us/sample - loss: 0.8201 - acc: 0.7689 - val_loss: 1.2752 - val_acc: 0.6809\n",
      "Epoch 98/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8108 - acc: 0.7749\n",
      "Epoch 00098: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 499us/sample - loss: 0.8071 - acc: 0.7742 - val_loss: 1.3157 - val_acc: 0.6548\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8049 - acc: 0.7844\n",
      "Epoch 00099: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 501us/sample - loss: 0.8018 - acc: 0.7866 - val_loss: 1.3912 - val_acc: 0.6407\n",
      "Epoch 100/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.7998 - acc: 0.7768\n",
      "Epoch 00100: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 502us/sample - loss: 0.8062 - acc: 0.7742 - val_loss: 1.3589 - val_acc: 0.6407\n",
      "443/443 [==============================] - 0s 197us/sample - loss: 1.4784 - acc: 0.5937\n",
      "--- Starting trial: run-14\n",
      "{'TIME_WINDOW': 1000, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 128, 'LAST_HIDDEN': 1000, 'LAST_DROPOUT': 0.2, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_99\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_100 (InputLayer)       [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute_99 (Permute)         (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_198 (Conv1D)          (None, 997, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_198 (Bat (None, 997, 32)           3988      \n",
      "_________________________________________________________________\n",
      "activation_198 (Activation)  (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_297 (Dropout)        (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_198 (MaxPoolin (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_199 (Conv1D)          (None, 246, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_199 (Bat (None, 246, 64)           984       \n",
      "_________________________________________________________________\n",
      "activation_199 (Activation)  (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_298 (Dropout)        (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_199 (MaxPoolin (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_297 (LSTM)              (None, 61, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_298 (LSTM)              (None, 61, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_299 (LSTM)              (None, 61, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_99 (Flatten)         (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 1000)              7809000   \n",
      "_________________________________________________________________\n",
      "dropout_299 (Dropout)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_199 (Dense)            (None, 4)                 4004      \n",
      "=================================================================\n",
      "Total params: 8,191,064\n",
      "Trainable params: 8,188,578\n",
      "Non-trainable params: 2,486\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 4.0100 - acc: 0.2869\n",
      "Epoch 00001: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 4s 2ms/sample - loss: 3.9945 - acc: 0.2878 - val_loss: 3.6091 - val_acc: 0.4232\n",
      "Epoch 2/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.5587 - acc: 0.3686\n",
      "Epoch 00002: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 553us/sample - loss: 3.5504 - acc: 0.3682 - val_loss: 3.3482 - val_acc: 0.3995\n",
      "Epoch 3/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 3.3140 - acc: 0.3798\n",
      "Epoch 00003: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 3.3104 - acc: 0.3830 - val_loss: 3.1324 - val_acc: 0.4184\n",
      "Epoch 4/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 3.1027 - acc: 0.4056\n",
      "Epoch 00004: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 563us/sample - loss: 3.0985 - acc: 0.4054 - val_loss: 2.9270 - val_acc: 0.4563\n",
      "Epoch 5/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.9136 - acc: 0.4313\n",
      "Epoch 00005: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 2.9096 - acc: 0.4326 - val_loss: 2.7480 - val_acc: 0.4704\n",
      "Epoch 6/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.7454 - acc: 0.4477\n",
      "Epoch 00006: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 566us/sample - loss: 2.7448 - acc: 0.4421 - val_loss: 2.5918 - val_acc: 0.4775\n",
      "Epoch 7/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.5734 - acc: 0.4669\n",
      "Epoch 00007: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 2.5753 - acc: 0.4634 - val_loss: 2.4461 - val_acc: 0.4657\n",
      "Epoch 8/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.4295 - acc: 0.4784\n",
      "Epoch 00008: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 2.4291 - acc: 0.4799 - val_loss: 2.3144 - val_acc: 0.4988\n",
      "Epoch 9/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.3028 - acc: 0.4856\n",
      "Epoch 00009: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 563us/sample - loss: 2.2999 - acc: 0.4852 - val_loss: 2.2496 - val_acc: 0.4468\n",
      "Epoch 10/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.1794 - acc: 0.5049\n",
      "Epoch 00010: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 562us/sample - loss: 2.1789 - acc: 0.5059 - val_loss: 2.0832 - val_acc: 0.5177\n",
      "Epoch 11/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.0733 - acc: 0.5132\n",
      "Epoch 00011: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 568us/sample - loss: 2.0728 - acc: 0.5142 - val_loss: 1.9808 - val_acc: 0.5414\n",
      "Epoch 12/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.9825 - acc: 0.5168\n",
      "Epoch 00012: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 556us/sample - loss: 1.9824 - acc: 0.5177 - val_loss: 2.0030 - val_acc: 0.4563\n",
      "Epoch 13/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.9046 - acc: 0.5222\n",
      "Epoch 00013: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 552us/sample - loss: 1.9058 - acc: 0.5189 - val_loss: 1.8721 - val_acc: 0.5130\n",
      "Epoch 14/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.8293 - acc: 0.5404\n",
      "Epoch 00014: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 562us/sample - loss: 1.8270 - acc: 0.5402 - val_loss: 1.7985 - val_acc: 0.5225\n",
      "Epoch 15/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.7596 - acc: 0.5457\n",
      "Epoch 00015: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 553us/sample - loss: 1.7594 - acc: 0.5455 - val_loss: 1.7229 - val_acc: 0.5177\n",
      "Epoch 16/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6932 - acc: 0.5656\n",
      "Epoch 00016: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 574us/sample - loss: 1.6920 - acc: 0.5650 - val_loss: 1.6741 - val_acc: 0.5083\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.6443 - acc: 0.5576\n",
      "Epoch 00017: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 1.6405 - acc: 0.5579 - val_loss: 1.6294 - val_acc: 0.5437\n",
      "Epoch 18/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5928 - acc: 0.5631\n",
      "Epoch 00018: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 560us/sample - loss: 1.5898 - acc: 0.5650 - val_loss: 1.5879 - val_acc: 0.5414\n",
      "Epoch 19/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5647 - acc: 0.5663\n",
      "Epoch 00019: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 558us/sample - loss: 1.5616 - acc: 0.5686 - val_loss: 1.6263 - val_acc: 0.5248\n",
      "Epoch 20/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5149 - acc: 0.5817\n",
      "Epoch 00020: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 557us/sample - loss: 1.5194 - acc: 0.5798 - val_loss: 1.5926 - val_acc: 0.5225\n",
      "Epoch 21/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4680 - acc: 0.5944\n",
      "Epoch 00021: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 1.4663 - acc: 0.5940 - val_loss: 1.4954 - val_acc: 0.5508\n",
      "Epoch 22/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.4387 - acc: 0.5804\n",
      "Epoch 00022: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 549us/sample - loss: 1.4341 - acc: 0.5810 - val_loss: 1.5755 - val_acc: 0.4988\n",
      "Epoch 23/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3965 - acc: 0.6062\n",
      "Epoch 00023: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 554us/sample - loss: 1.4096 - acc: 0.5993 - val_loss: 1.4744 - val_acc: 0.5485\n",
      "Epoch 24/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.3898 - acc: 0.5962\n",
      "Epoch 00024: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 568us/sample - loss: 1.3887 - acc: 0.5969 - val_loss: 1.4149 - val_acc: 0.5792\n",
      "Epoch 25/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3497 - acc: 0.6122\n",
      "Epoch 00025: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 1.3561 - acc: 0.6082 - val_loss: 1.4440 - val_acc: 0.5579\n",
      "Epoch 26/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3351 - acc: 0.5974\n",
      "Epoch 00026: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 1.3320 - acc: 0.6017 - val_loss: 1.4609 - val_acc: 0.5414\n",
      "Epoch 27/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2863 - acc: 0.6232\n",
      "Epoch 00027: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 560us/sample - loss: 1.2853 - acc: 0.6241 - val_loss: 1.3906 - val_acc: 0.5579\n",
      "Epoch 28/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3074 - acc: 0.6027\n",
      "Epoch 00028: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 553us/sample - loss: 1.3059 - acc: 0.6087 - val_loss: 1.3543 - val_acc: 0.5603\n",
      "Epoch 29/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2736 - acc: 0.6202\n",
      "Epoch 00029: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 560us/sample - loss: 1.2745 - acc: 0.6182 - val_loss: 1.3876 - val_acc: 0.5556\n",
      "Epoch 30/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2543 - acc: 0.6316\n",
      "Epoch 00030: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 557us/sample - loss: 1.2538 - acc: 0.6300 - val_loss: 1.4050 - val_acc: 0.5579\n",
      "Epoch 31/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2453 - acc: 0.6238\n",
      "Epoch 00031: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 1.2457 - acc: 0.6241 - val_loss: 1.5059 - val_acc: 0.4917\n",
      "Epoch 32/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2300 - acc: 0.6292\n",
      "Epoch 00032: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 555us/sample - loss: 1.2284 - acc: 0.6324 - val_loss: 1.3425 - val_acc: 0.5887\n",
      "Epoch 33/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2034 - acc: 0.6385\n",
      "Epoch 00033: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 567us/sample - loss: 1.2053 - acc: 0.6365 - val_loss: 1.3454 - val_acc: 0.5792\n",
      "Epoch 34/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2087 - acc: 0.6397\n",
      "Epoch 00034: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 557us/sample - loss: 1.2034 - acc: 0.6418 - val_loss: 1.3583 - val_acc: 0.5603\n",
      "Epoch 35/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1582 - acc: 0.6679\n",
      "Epoch 00035: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 565us/sample - loss: 1.1572 - acc: 0.6702 - val_loss: 1.3387 - val_acc: 0.5863\n",
      "Epoch 36/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1655 - acc: 0.6532\n",
      "Epoch 00036: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 566us/sample - loss: 1.1655 - acc: 0.6525 - val_loss: 1.3401 - val_acc: 0.5697\n",
      "Epoch 37/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1723 - acc: 0.6520\n",
      "Epoch 00037: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 552us/sample - loss: 1.1742 - acc: 0.6501 - val_loss: 1.2911 - val_acc: 0.6170\n",
      "Epoch 38/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1654 - acc: 0.6477\n",
      "Epoch 00038: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 1.1649 - acc: 0.6489 - val_loss: 1.2742 - val_acc: 0.6123\n",
      "Epoch 39/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1459 - acc: 0.6569\n",
      "Epoch 00039: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 1.1425 - acc: 0.6578 - val_loss: 1.3356 - val_acc: 0.5863\n",
      "Epoch 40/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1234 - acc: 0.6601\n",
      "Epoch 00040: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 563us/sample - loss: 1.1215 - acc: 0.6619 - val_loss: 1.2799 - val_acc: 0.6265\n",
      "Epoch 41/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1071 - acc: 0.6639\n",
      "Epoch 00041: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 562us/sample - loss: 1.1090 - acc: 0.6655 - val_loss: 1.2713 - val_acc: 0.6028\n",
      "Epoch 42/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0993 - acc: 0.6681\n",
      "Epoch 00042: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 562us/sample - loss: 1.1017 - acc: 0.6655 - val_loss: 1.2418 - val_acc: 0.6052\n",
      "Epoch 43/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0813 - acc: 0.6795\n",
      "Epoch 00043: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 562us/sample - loss: 1.0922 - acc: 0.6755 - val_loss: 1.2802 - val_acc: 0.6099\n",
      "Epoch 44/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0970 - acc: 0.6620\n",
      "Epoch 00044: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 558us/sample - loss: 1.1124 - acc: 0.6531 - val_loss: 1.2650 - val_acc: 0.5910\n",
      "Epoch 45/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0848 - acc: 0.6845\n",
      "Epoch 00045: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 562us/sample - loss: 1.0839 - acc: 0.6856 - val_loss: 1.2506 - val_acc: 0.6194\n",
      "Epoch 46/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0757 - acc: 0.6779\n",
      "Epoch 00046: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 558us/sample - loss: 1.0721 - acc: 0.6791 - val_loss: 1.3056 - val_acc: 0.6028\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0893 - acc: 0.6740\n",
      "Epoch 00047: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 561us/sample - loss: 1.0840 - acc: 0.6767 - val_loss: 1.2676 - val_acc: 0.6052\n",
      "Epoch 48/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0696 - acc: 0.6789\n",
      "Epoch 00048: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 564us/sample - loss: 1.0645 - acc: 0.6803 - val_loss: 1.2339 - val_acc: 0.6359\n",
      "Epoch 49/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0688 - acc: 0.6734\n",
      "Epoch 00049: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 563us/sample - loss: 1.0738 - acc: 0.6696 - val_loss: 1.2331 - val_acc: 0.6052\n",
      "Epoch 50/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0587 - acc: 0.6881\n",
      "Epoch 00050: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 567us/sample - loss: 1.0625 - acc: 0.6874 - val_loss: 1.2341 - val_acc: 0.6430\n",
      "Epoch 51/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0336 - acc: 0.6979\n",
      "Epoch 00051: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 564us/sample - loss: 1.0431 - acc: 0.6939 - val_loss: 1.2282 - val_acc: 0.6407\n",
      "Epoch 52/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0054 - acc: 0.7102\n",
      "Epoch 00052: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 1.0079 - acc: 0.7092 - val_loss: 1.2243 - val_acc: 0.6478\n",
      "Epoch 53/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0387 - acc: 0.7022\n",
      "Epoch 00053: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 562us/sample - loss: 1.0340 - acc: 0.7033 - val_loss: 1.2399 - val_acc: 0.6407\n",
      "Epoch 54/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0229 - acc: 0.6995\n",
      "Epoch 00054: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 564us/sample - loss: 1.0235 - acc: 0.6992 - val_loss: 1.2193 - val_acc: 0.6548\n",
      "Epoch 55/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0077 - acc: 0.6969\n",
      "Epoch 00055: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 566us/sample - loss: 1.0083 - acc: 0.6968 - val_loss: 1.2561 - val_acc: 0.6099\n",
      "Epoch 56/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0026 - acc: 0.7031\n",
      "Epoch 00056: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 554us/sample - loss: 1.0025 - acc: 0.7021 - val_loss: 1.2771 - val_acc: 0.6383\n",
      "Epoch 57/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9615 - acc: 0.7245\n",
      "Epoch 00057: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 557us/sample - loss: 0.9681 - acc: 0.7222 - val_loss: 1.2773 - val_acc: 0.6478\n",
      "Epoch 58/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0035 - acc: 0.6912\n",
      "Epoch 00058: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 552us/sample - loss: 1.0081 - acc: 0.6927 - val_loss: 1.2033 - val_acc: 0.6454\n",
      "Epoch 59/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9890 - acc: 0.7028\n",
      "Epoch 00059: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 555us/sample - loss: 0.9899 - acc: 0.7033 - val_loss: 1.3538 - val_acc: 0.6241\n",
      "Epoch 60/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9889 - acc: 0.7102\n",
      "Epoch 00060: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 563us/sample - loss: 0.9828 - acc: 0.7116 - val_loss: 1.2124 - val_acc: 0.6619\n",
      "Epoch 61/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0018 - acc: 0.7010\n",
      "Epoch 00061: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 557us/sample - loss: 1.0013 - acc: 0.6998 - val_loss: 1.2238 - val_acc: 0.6690\n",
      "Epoch 62/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9655 - acc: 0.7279\n",
      "Epoch 00062: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 564us/sample - loss: 0.9697 - acc: 0.7252 - val_loss: 1.1862 - val_acc: 0.6738\n",
      "Epoch 63/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9739 - acc: 0.7163\n",
      "Epoch 00063: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 563us/sample - loss: 0.9760 - acc: 0.7139 - val_loss: 1.2194 - val_acc: 0.6619\n",
      "Epoch 64/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9660 - acc: 0.7267\n",
      "Epoch 00064: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 571us/sample - loss: 0.9680 - acc: 0.7252 - val_loss: 1.2222 - val_acc: 0.6619\n",
      "Epoch 65/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9732 - acc: 0.7181\n",
      "Epoch 00065: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 565us/sample - loss: 0.9750 - acc: 0.7169 - val_loss: 1.1864 - val_acc: 0.6596\n",
      "Epoch 66/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9699 - acc: 0.7127\n",
      "Epoch 00066: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 555us/sample - loss: 0.9699 - acc: 0.7128 - val_loss: 1.2845 - val_acc: 0.6288\n",
      "Epoch 67/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9650 - acc: 0.7117\n",
      "Epoch 00067: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 548us/sample - loss: 0.9734 - acc: 0.7092 - val_loss: 1.3248 - val_acc: 0.6170\n",
      "Epoch 68/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9737 - acc: 0.7181\n",
      "Epoch 00068: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 555us/sample - loss: 0.9707 - acc: 0.7199 - val_loss: 1.2817 - val_acc: 0.6194\n",
      "Epoch 69/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9613 - acc: 0.7091\n",
      "Epoch 00069: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 566us/sample - loss: 0.9612 - acc: 0.7098 - val_loss: 1.2972 - val_acc: 0.6383\n",
      "Epoch 70/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.9280 - acc: 0.7423\n",
      "Epoch 00070: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 558us/sample - loss: 0.9299 - acc: 0.7405 - val_loss: 1.3106 - val_acc: 0.6525\n",
      "Epoch 71/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9372 - acc: 0.7335\n",
      "Epoch 00071: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 556us/sample - loss: 0.9392 - acc: 0.7323 - val_loss: 1.2670 - val_acc: 0.6596\n",
      "Epoch 72/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9230 - acc: 0.7362\n",
      "Epoch 00072: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 558us/sample - loss: 0.9250 - acc: 0.7364 - val_loss: 1.2644 - val_acc: 0.6525\n",
      "Epoch 73/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8898 - acc: 0.7437\n",
      "Epoch 00073: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 565us/sample - loss: 0.9005 - acc: 0.7388 - val_loss: 1.2626 - val_acc: 0.6407\n",
      "Epoch 74/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9201 - acc: 0.7284\n",
      "Epoch 00074: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 554us/sample - loss: 0.9243 - acc: 0.7264 - val_loss: 1.2276 - val_acc: 0.6785\n",
      "Epoch 75/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8882 - acc: 0.7512\n",
      "Epoch 00075: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 559us/sample - loss: 0.8894 - acc: 0.7506 - val_loss: 1.1859 - val_acc: 0.6714\n",
      "Epoch 76/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8986 - acc: 0.7443\n",
      "Epoch 00076: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 554us/sample - loss: 0.9056 - acc: 0.7400 - val_loss: 1.2413 - val_acc: 0.6761\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9345 - acc: 0.7188\n",
      "Epoch 00077: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 577us/sample - loss: 0.9260 - acc: 0.7240 - val_loss: 1.1881 - val_acc: 0.6690\n",
      "Epoch 78/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8573 - acc: 0.7567\n",
      "Epoch 00078: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 582us/sample - loss: 0.8590 - acc: 0.7577 - val_loss: 1.2360 - val_acc: 0.6903\n",
      "Epoch 79/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8762 - acc: 0.7430\n",
      "Epoch 00079: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 573us/sample - loss: 0.8845 - acc: 0.7411 - val_loss: 1.2370 - val_acc: 0.6856\n",
      "Epoch 80/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8789 - acc: 0.7506\n",
      "Epoch 00080: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 586us/sample - loss: 0.8817 - acc: 0.7535 - val_loss: 1.2292 - val_acc: 0.6430\n",
      "Epoch 81/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8886 - acc: 0.7536\n",
      "Epoch 00081: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 573us/sample - loss: 0.8884 - acc: 0.7541 - val_loss: 1.2283 - val_acc: 0.6927\n",
      "Epoch 82/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8634 - acc: 0.7530\n",
      "Epoch 00082: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 585us/sample - loss: 0.8661 - acc: 0.7524 - val_loss: 1.2005 - val_acc: 0.6761\n",
      "Epoch 83/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8991 - acc: 0.7428\n",
      "Epoch 00083: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 583us/sample - loss: 0.8973 - acc: 0.7435 - val_loss: 1.3946 - val_acc: 0.6217\n",
      "Epoch 84/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8763 - acc: 0.7494\n",
      "Epoch 00084: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 583us/sample - loss: 0.8792 - acc: 0.7453 - val_loss: 1.3147 - val_acc: 0.6667\n",
      "Epoch 85/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8654 - acc: 0.7555\n",
      "Epoch 00085: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 578us/sample - loss: 0.8595 - acc: 0.7589 - val_loss: 1.2728 - val_acc: 0.6690\n",
      "Epoch 86/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8609 - acc: 0.7588\n",
      "Epoch 00086: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 558us/sample - loss: 0.8579 - acc: 0.7589 - val_loss: 1.3506 - val_acc: 0.6548\n",
      "Epoch 87/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8537 - acc: 0.7613\n",
      "Epoch 00087: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 585us/sample - loss: 0.8573 - acc: 0.7595 - val_loss: 1.2675 - val_acc: 0.6690\n",
      "Epoch 88/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8416 - acc: 0.7653\n",
      "Epoch 00088: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 572us/sample - loss: 0.8454 - acc: 0.7648 - val_loss: 1.2785 - val_acc: 0.6738\n",
      "Epoch 89/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8383 - acc: 0.7698\n",
      "Epoch 00089: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 577us/sample - loss: 0.8368 - acc: 0.7701 - val_loss: 1.2636 - val_acc: 0.6927\n",
      "Epoch 90/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8661 - acc: 0.7600\n",
      "Epoch 00090: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 575us/sample - loss: 0.8633 - acc: 0.7589 - val_loss: 1.2787 - val_acc: 0.6856\n",
      "Epoch 91/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8436 - acc: 0.7698\n",
      "Epoch 00091: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 586us/sample - loss: 0.8400 - acc: 0.7707 - val_loss: 1.3473 - val_acc: 0.6596\n",
      "Epoch 92/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8526 - acc: 0.7632\n",
      "Epoch 00092: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 583us/sample - loss: 0.8492 - acc: 0.7660 - val_loss: 1.3044 - val_acc: 0.6596\n",
      "Epoch 93/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8108 - acc: 0.7731\n",
      "Epoch 00093: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 585us/sample - loss: 0.8094 - acc: 0.7730 - val_loss: 1.3503 - val_acc: 0.6548\n",
      "Epoch 94/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8398 - acc: 0.7641\n",
      "Epoch 00094: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 563us/sample - loss: 0.8408 - acc: 0.7630 - val_loss: 1.3315 - val_acc: 0.6572\n",
      "Epoch 95/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8295 - acc: 0.7620\n",
      "Epoch 00095: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 577us/sample - loss: 0.8246 - acc: 0.7648 - val_loss: 1.3164 - val_acc: 0.6785\n",
      "Epoch 96/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8212 - acc: 0.7787\n",
      "Epoch 00096: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 582us/sample - loss: 0.8188 - acc: 0.7784 - val_loss: 1.2625 - val_acc: 0.6690\n",
      "Epoch 97/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8004 - acc: 0.7800\n",
      "Epoch 00097: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 581us/sample - loss: 0.8073 - acc: 0.7760 - val_loss: 1.3083 - val_acc: 0.6809\n",
      "Epoch 98/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8204 - acc: 0.7776\n",
      "Epoch 00098: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 572us/sample - loss: 0.8187 - acc: 0.7772 - val_loss: 1.3177 - val_acc: 0.6927\n",
      "Epoch 99/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8018 - acc: 0.7880\n",
      "Epoch 00099: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 578us/sample - loss: 0.8042 - acc: 0.7861 - val_loss: 1.4010 - val_acc: 0.6572\n",
      "Epoch 100/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8002 - acc: 0.7782\n",
      "Epoch 00100: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 569us/sample - loss: 0.8096 - acc: 0.7736 - val_loss: 1.2668 - val_acc: 0.6832\n",
      "443/443 [==============================] - 0s 221us/sample - loss: 1.3398 - acc: 0.6456\n",
      "--- Starting trial: run-15\n",
      "{'TIME_WINDOW': 1000, 'BATCH_SIZE': 32, 'HIDDEN': 128, 'DROPOUT': 0.3, 'REGULARIZER': 0.001, 'LAST_LSTM': 128, 'LAST_HIDDEN': 1000, 'LAST_DROPOUT': 0.3, 'LEARNING': 0.0001, 'BETA': 0.9}\n",
      "Model: \"model_100\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_101 (InputLayer)       [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "permute_100 (Permute)        (None, 1000, 22)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_200 (Conv1D)          (None, 997, 32)           2848      \n",
      "_________________________________________________________________\n",
      "batch_normalization_200 (Bat (None, 997, 32)           3988      \n",
      "_________________________________________________________________\n",
      "activation_200 (Activation)  (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_300 (Dropout)        (None, 997, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_200 (MaxPoolin (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_201 (Conv1D)          (None, 246, 64)           8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_201 (Bat (None, 246, 64)           984       \n",
      "_________________________________________________________________\n",
      "activation_201 (Activation)  (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_301 (Dropout)        (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_201 (MaxPoolin (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_300 (LSTM)              (None, 61, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_301 (LSTM)              (None, 61, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_302 (LSTM)              (None, 61, 128)           131584    \n",
      "_________________________________________________________________\n",
      "flatten_100 (Flatten)        (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_200 (Dense)            (None, 1000)              7809000   \n",
      "_________________________________________________________________\n",
      "dropout_302 (Dropout)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_201 (Dense)            (None, 4)                 4004      \n",
      "=================================================================\n",
      "Total params: 8,191,064\n",
      "Trainable params: 8,188,578\n",
      "Non-trainable params: 2,486\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1692 [===========================>..] - ETA: 0s - loss: 4.0662 - acc: 0.2880\n",
      "Epoch 00001: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 5s 3ms/sample - loss: 4.0528 - acc: 0.2926 - val_loss: 3.7023 - val_acc: 0.3546\n",
      "Epoch 2/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 3.5784 - acc: 0.3558\n",
      "Epoch 00002: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 585us/sample - loss: 3.5763 - acc: 0.3582 - val_loss: 3.3836 - val_acc: 0.3475\n",
      "Epoch 3/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 3.3387 - acc: 0.3824\n",
      "Epoch 00003: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 588us/sample - loss: 3.3352 - acc: 0.3830 - val_loss: 3.1711 - val_acc: 0.3570\n",
      "Epoch 4/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 3.1345 - acc: 0.4011\n",
      "Epoch 00004: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 580us/sample - loss: 3.1339 - acc: 0.3972 - val_loss: 2.9805 - val_acc: 0.4090\n",
      "Epoch 5/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.9603 - acc: 0.4094\n",
      "Epoch 00005: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 569us/sample - loss: 2.9545 - acc: 0.4072 - val_loss: 2.8200 - val_acc: 0.4043\n",
      "Epoch 6/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.7925 - acc: 0.4339\n",
      "Epoch 00006: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 586us/sample - loss: 2.7901 - acc: 0.4356 - val_loss: 2.6772 - val_acc: 0.4303\n",
      "Epoch 7/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.6326 - acc: 0.4429\n",
      "Epoch 00007: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 590us/sample - loss: 2.6303 - acc: 0.4456 - val_loss: 2.5419 - val_acc: 0.4255\n",
      "Epoch 8/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.4990 - acc: 0.4544\n",
      "Epoch 00008: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 576us/sample - loss: 2.4919 - acc: 0.4604 - val_loss: 2.4249 - val_acc: 0.4137\n",
      "Epoch 9/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.3591 - acc: 0.4688\n",
      "Epoch 00009: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 585us/sample - loss: 2.3572 - acc: 0.4699 - val_loss: 2.3368 - val_acc: 0.4137\n",
      "Epoch 10/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 2.2470 - acc: 0.4866\n",
      "Epoch 00010: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 590us/sample - loss: 2.2439 - acc: 0.4846 - val_loss: 2.2426 - val_acc: 0.4137\n",
      "Epoch 11/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 2.1530 - acc: 0.4896\n",
      "Epoch 00011: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 593us/sample - loss: 2.1527 - acc: 0.4894 - val_loss: 2.1005 - val_acc: 0.4704\n",
      "Epoch 12/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 2.0629 - acc: 0.4856\n",
      "Epoch 00012: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 584us/sample - loss: 2.0607 - acc: 0.4852 - val_loss: 2.0594 - val_acc: 0.4468\n",
      "Epoch 13/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.9789 - acc: 0.5233\n",
      "Epoch 00013: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 584us/sample - loss: 1.9764 - acc: 0.5236 - val_loss: 1.9583 - val_acc: 0.4870\n",
      "Epoch 14/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.9006 - acc: 0.5038\n",
      "Epoch 00014: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 592us/sample - loss: 1.8940 - acc: 0.5071 - val_loss: 1.8766 - val_acc: 0.4894\n",
      "Epoch 15/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.8252 - acc: 0.5141\n",
      "Epoch 00015: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 587us/sample - loss: 1.8236 - acc: 0.5177 - val_loss: 1.8652 - val_acc: 0.4634\n",
      "Epoch 16/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.7759 - acc: 0.5165\n",
      "Epoch 00016: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 576us/sample - loss: 1.7758 - acc: 0.5177 - val_loss: 1.7799 - val_acc: 0.5012\n",
      "Epoch 17/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.7188 - acc: 0.5208\n",
      "Epoch 00017: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 578us/sample - loss: 1.7188 - acc: 0.5195 - val_loss: 1.7687 - val_acc: 0.4728\n",
      "Epoch 18/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.6538 - acc: 0.5535\n",
      "Epoch 00018: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 609us/sample - loss: 1.6562 - acc: 0.5526 - val_loss: 1.7051 - val_acc: 0.4965\n",
      "Epoch 19/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.6352 - acc: 0.5406\n",
      "Epoch 00019: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 625us/sample - loss: 1.6303 - acc: 0.5426 - val_loss: 1.6678 - val_acc: 0.4917\n",
      "Epoch 20/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.5831 - acc: 0.5427\n",
      "Epoch 00020: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 575us/sample - loss: 1.5804 - acc: 0.5414 - val_loss: 1.6410 - val_acc: 0.4941\n",
      "Epoch 21/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.5408 - acc: 0.5527\n",
      "Epoch 00021: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 563us/sample - loss: 1.5472 - acc: 0.5485 - val_loss: 1.6323 - val_acc: 0.4917\n",
      "Epoch 22/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.5185 - acc: 0.5398\n",
      "Epoch 00022: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 560us/sample - loss: 1.5157 - acc: 0.5414 - val_loss: 1.5948 - val_acc: 0.4752\n",
      "Epoch 23/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.4678 - acc: 0.5539\n",
      "Epoch 00023: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 567us/sample - loss: 1.4717 - acc: 0.5526 - val_loss: 1.6082 - val_acc: 0.4681\n",
      "Epoch 24/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4477 - acc: 0.5589\n",
      "Epoch 00024: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 570us/sample - loss: 1.4456 - acc: 0.5597 - val_loss: 1.6473 - val_acc: 0.4752\n",
      "Epoch 25/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4232 - acc: 0.5811\n",
      "Epoch 00025: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 608us/sample - loss: 1.4219 - acc: 0.5816 - val_loss: 1.5424 - val_acc: 0.5083\n",
      "Epoch 26/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.4126 - acc: 0.5669\n",
      "Epoch 00026: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 610us/sample - loss: 1.4195 - acc: 0.5662 - val_loss: 1.4949 - val_acc: 0.5485\n",
      "Epoch 27/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3569 - acc: 0.5999\n",
      "Epoch 00027: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 602us/sample - loss: 1.3614 - acc: 0.5975 - val_loss: 1.5249 - val_acc: 0.5177\n",
      "Epoch 28/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3297 - acc: 0.5944\n",
      "Epoch 00028: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 585us/sample - loss: 1.3401 - acc: 0.5910 - val_loss: 1.5062 - val_acc: 0.5343\n",
      "Epoch 29/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.3394 - acc: 0.5987\n",
      "Epoch 00029: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 610us/sample - loss: 1.3421 - acc: 0.5987 - val_loss: 1.4541 - val_acc: 0.5437\n",
      "Epoch 30/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.3136 - acc: 0.5918\n",
      "Epoch 00030: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 570us/sample - loss: 1.3135 - acc: 0.5946 - val_loss: 1.4051 - val_acc: 0.5626\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2912 - acc: 0.6005\n",
      "Epoch 00031: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 567us/sample - loss: 1.2964 - acc: 0.5969 - val_loss: 1.4351 - val_acc: 0.5414\n",
      "Epoch 32/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2896 - acc: 0.6010\n",
      "Epoch 00032: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 551us/sample - loss: 1.2855 - acc: 0.6034 - val_loss: 1.4108 - val_acc: 0.5485\n",
      "Epoch 33/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2485 - acc: 0.5975\n",
      "Epoch 00033: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 579us/sample - loss: 1.2411 - acc: 0.6046 - val_loss: 1.4731 - val_acc: 0.5603\n",
      "Epoch 34/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.2627 - acc: 0.6156\n",
      "Epoch 00034: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 557us/sample - loss: 1.2562 - acc: 0.6164 - val_loss: 1.4130 - val_acc: 0.5296\n",
      "Epoch 35/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2309 - acc: 0.6189\n",
      "Epoch 00035: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 548us/sample - loss: 1.2272 - acc: 0.6229 - val_loss: 1.4250 - val_acc: 0.5414\n",
      "Epoch 36/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2418 - acc: 0.6060\n",
      "Epoch 00036: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 576us/sample - loss: 1.2372 - acc: 0.6076 - val_loss: 1.3810 - val_acc: 0.5603\n",
      "Epoch 37/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.2114 - acc: 0.6219\n",
      "Epoch 00037: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 592us/sample - loss: 1.2095 - acc: 0.6217 - val_loss: 1.3654 - val_acc: 0.5626\n",
      "Epoch 38/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1908 - acc: 0.6513\n",
      "Epoch 00038: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 581us/sample - loss: 1.1941 - acc: 0.6495 - val_loss: 1.4777 - val_acc: 0.5414\n",
      "Epoch 39/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2104 - acc: 0.6082\n",
      "Epoch 00039: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 581us/sample - loss: 1.2088 - acc: 0.6076 - val_loss: 1.3583 - val_acc: 0.5461\n",
      "Epoch 40/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1817 - acc: 0.6250\n",
      "Epoch 00040: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 580us/sample - loss: 1.1800 - acc: 0.6271 - val_loss: 1.4632 - val_acc: 0.5437\n",
      "Epoch 41/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.1581 - acc: 0.6550\n",
      "Epoch 00041: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 592us/sample - loss: 1.1674 - acc: 0.6507 - val_loss: 1.4010 - val_acc: 0.5532\n",
      "Epoch 42/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1258 - acc: 0.6611\n",
      "Epoch 00042: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 585us/sample - loss: 1.1333 - acc: 0.6596 - val_loss: 1.4741 - val_acc: 0.5508\n",
      "Epoch 43/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1406 - acc: 0.6416\n",
      "Epoch 00043: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 605us/sample - loss: 1.1408 - acc: 0.6401 - val_loss: 1.3930 - val_acc: 0.5745\n",
      "Epoch 44/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1294 - acc: 0.6537\n",
      "Epoch 00044: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 582us/sample - loss: 1.1321 - acc: 0.6507 - val_loss: 1.3003 - val_acc: 0.5887\n",
      "Epoch 45/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1223 - acc: 0.6582\n",
      "Epoch 00045: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 580us/sample - loss: 1.1254 - acc: 0.6543 - val_loss: 1.4145 - val_acc: 0.5816\n",
      "Epoch 46/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1173 - acc: 0.6562\n",
      "Epoch 00046: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 600us/sample - loss: 1.1172 - acc: 0.6566 - val_loss: 1.3866 - val_acc: 0.5603\n",
      "Epoch 47/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.1357 - acc: 0.6467\n",
      "Epoch 00047: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 593us/sample - loss: 1.1237 - acc: 0.6519 - val_loss: 1.4735 - val_acc: 0.5508\n",
      "Epoch 48/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.1087 - acc: 0.6605\n",
      "Epoch 00048: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 606us/sample - loss: 1.1088 - acc: 0.6590 - val_loss: 1.4161 - val_acc: 0.5721\n",
      "Epoch 49/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0985 - acc: 0.6665\n",
      "Epoch 00049: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 604us/sample - loss: 1.1050 - acc: 0.6631 - val_loss: 1.2841 - val_acc: 0.5957\n",
      "Epoch 50/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0686 - acc: 0.6731\n",
      "Epoch 00050: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 570us/sample - loss: 1.0699 - acc: 0.6749 - val_loss: 1.3814 - val_acc: 0.5839\n",
      "Epoch 51/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0684 - acc: 0.6762\n",
      "Epoch 00051: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 575us/sample - loss: 1.0757 - acc: 0.6690 - val_loss: 1.4006 - val_acc: 0.5934\n",
      "Epoch 52/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0741 - acc: 0.6731\n",
      "Epoch 00052: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 594us/sample - loss: 1.0683 - acc: 0.6755 - val_loss: 1.5141 - val_acc: 0.5437\n",
      "Epoch 53/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0675 - acc: 0.6773\n",
      "Epoch 00053: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 597us/sample - loss: 1.0735 - acc: 0.6755 - val_loss: 1.3790 - val_acc: 0.5839\n",
      "Epoch 54/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0353 - acc: 0.6926\n",
      "Epoch 00054: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 598us/sample - loss: 1.0469 - acc: 0.6885 - val_loss: 1.3785 - val_acc: 0.6005\n",
      "Epoch 55/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0230 - acc: 0.6938\n",
      "Epoch 00055: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 583us/sample - loss: 1.0328 - acc: 0.6879 - val_loss: 1.3867 - val_acc: 0.6052\n",
      "Epoch 56/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 1.0370 - acc: 0.6945\n",
      "Epoch 00056: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 592us/sample - loss: 1.0406 - acc: 0.6903 - val_loss: 1.4483 - val_acc: 0.5697\n",
      "Epoch 57/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0222 - acc: 0.6985\n",
      "Epoch 00057: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 588us/sample - loss: 1.0246 - acc: 0.6944 - val_loss: 1.4042 - val_acc: 0.5934\n",
      "Epoch 58/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 1.0056 - acc: 0.7028\n",
      "Epoch 00058: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 597us/sample - loss: 1.0044 - acc: 0.7027 - val_loss: 1.3207 - val_acc: 0.6194\n",
      "Epoch 59/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0025 - acc: 0.7050\n",
      "Epoch 00059: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 608us/sample - loss: 1.0047 - acc: 0.7045 - val_loss: 1.3989 - val_acc: 0.5816\n",
      "Epoch 60/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9935 - acc: 0.6965\n",
      "Epoch 00060: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 589us/sample - loss: 0.9974 - acc: 0.6950 - val_loss: 1.4253 - val_acc: 0.5934\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0039 - acc: 0.7079\n",
      "Epoch 00061: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 585us/sample - loss: 1.0041 - acc: 0.7074 - val_loss: 1.3448 - val_acc: 0.6028\n",
      "Epoch 62/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0113 - acc: 0.6977\n",
      "Epoch 00062: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 595us/sample - loss: 1.0062 - acc: 0.7004 - val_loss: 1.4149 - val_acc: 0.5816\n",
      "Epoch 63/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9766 - acc: 0.7145\n",
      "Epoch 00063: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 590us/sample - loss: 0.9771 - acc: 0.7134 - val_loss: 1.4804 - val_acc: 0.5745\n",
      "Epoch 64/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9958 - acc: 0.7126\n",
      "Epoch 00064: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 607us/sample - loss: 0.9914 - acc: 0.7128 - val_loss: 1.5735 - val_acc: 0.5650\n",
      "Epoch 65/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 1.0077 - acc: 0.6956\n",
      "Epoch 00065: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 573us/sample - loss: 1.0054 - acc: 0.7004 - val_loss: 1.4738 - val_acc: 0.5792\n",
      "Epoch 66/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0003 - acc: 0.7037\n",
      "Epoch 00066: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 605us/sample - loss: 0.9983 - acc: 0.7039 - val_loss: 1.4787 - val_acc: 0.5697\n",
      "Epoch 67/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9719 - acc: 0.7127\n",
      "Epoch 00067: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 609us/sample - loss: 0.9664 - acc: 0.7157 - val_loss: 1.5397 - val_acc: 0.5626\n",
      "Epoch 68/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9657 - acc: 0.7151\n",
      "Epoch 00068: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 598us/sample - loss: 0.9708 - acc: 0.7128 - val_loss: 1.3976 - val_acc: 0.6123\n",
      "Epoch 69/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9446 - acc: 0.7290\n",
      "Epoch 00069: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 602us/sample - loss: 0.9538 - acc: 0.7275 - val_loss: 1.5507 - val_acc: 0.5650\n",
      "Epoch 70/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9600 - acc: 0.7296\n",
      "Epoch 00070: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 608us/sample - loss: 0.9593 - acc: 0.7305 - val_loss: 1.4182 - val_acc: 0.6123\n",
      "Epoch 71/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9566 - acc: 0.7163\n",
      "Epoch 00071: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 613us/sample - loss: 0.9569 - acc: 0.7175 - val_loss: 1.4085 - val_acc: 0.5957\n",
      "Epoch 72/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9678 - acc: 0.7139\n",
      "Epoch 00072: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 592us/sample - loss: 0.9655 - acc: 0.7157 - val_loss: 1.4724 - val_acc: 0.5839\n",
      "Epoch 73/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9403 - acc: 0.7365\n",
      "Epoch 00073: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 597us/sample - loss: 0.9385 - acc: 0.7370 - val_loss: 1.4354 - val_acc: 0.6005\n",
      "Epoch 74/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9404 - acc: 0.7279\n",
      "Epoch 00074: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 601us/sample - loss: 0.9350 - acc: 0.7329 - val_loss: 1.3448 - val_acc: 0.6265\n",
      "Epoch 75/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9077 - acc: 0.7531\n",
      "Epoch 00075: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 606us/sample - loss: 0.9069 - acc: 0.7518 - val_loss: 1.4966 - val_acc: 0.6099\n",
      "Epoch 76/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8971 - acc: 0.7513\n",
      "Epoch 00076: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 603us/sample - loss: 0.8947 - acc: 0.7488 - val_loss: 1.5253 - val_acc: 0.5863\n",
      "Epoch 77/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9325 - acc: 0.7456\n",
      "Epoch 00077: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 610us/sample - loss: 0.9312 - acc: 0.7453 - val_loss: 1.4850 - val_acc: 0.5957\n",
      "Epoch 78/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9215 - acc: 0.7392\n",
      "Epoch 00078: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 623us/sample - loss: 0.9226 - acc: 0.7376 - val_loss: 1.5028 - val_acc: 0.5981\n",
      "Epoch 79/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9100 - acc: 0.7396\n",
      "Epoch 00079: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 594us/sample - loss: 0.9084 - acc: 0.7405 - val_loss: 1.5058 - val_acc: 0.6170\n",
      "Epoch 80/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9005 - acc: 0.7531\n",
      "Epoch 00080: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 599us/sample - loss: 0.9029 - acc: 0.7541 - val_loss: 1.5848 - val_acc: 0.5957\n",
      "Epoch 81/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.9261 - acc: 0.7287\n",
      "Epoch 00081: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 600us/sample - loss: 0.9200 - acc: 0.7311 - val_loss: 1.4073 - val_acc: 0.6336\n",
      "Epoch 82/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8947 - acc: 0.7602\n",
      "Epoch 00082: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 596us/sample - loss: 0.8957 - acc: 0.7595 - val_loss: 1.4718 - val_acc: 0.6147\n",
      "Epoch 83/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.9099 - acc: 0.7488\n",
      "Epoch 00083: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 619us/sample - loss: 0.9160 - acc: 0.7441 - val_loss: 1.4340 - val_acc: 0.6359\n",
      "Epoch 84/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8808 - acc: 0.7512\n",
      "Epoch 00084: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 604us/sample - loss: 0.8772 - acc: 0.7535 - val_loss: 1.4337 - val_acc: 0.6336\n",
      "Epoch 85/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8616 - acc: 0.7598\n",
      "Epoch 00085: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 597us/sample - loss: 0.8675 - acc: 0.7571 - val_loss: 1.5022 - val_acc: 0.6170\n",
      "Epoch 86/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8757 - acc: 0.7596\n",
      "Epoch 00086: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 601us/sample - loss: 0.8753 - acc: 0.7589 - val_loss: 1.5590 - val_acc: 0.5957\n",
      "Epoch 87/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8622 - acc: 0.7598\n",
      "Epoch 00087: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 595us/sample - loss: 0.8658 - acc: 0.7559 - val_loss: 1.4871 - val_acc: 0.6170\n",
      "Epoch 88/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8581 - acc: 0.7641\n",
      "Epoch 00088: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 599us/sample - loss: 0.8606 - acc: 0.7630 - val_loss: 1.4442 - val_acc: 0.6359\n",
      "Epoch 89/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8646 - acc: 0.7606\n",
      "Epoch 00089: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 589us/sample - loss: 0.8583 - acc: 0.7624 - val_loss: 1.4567 - val_acc: 0.6359\n",
      "Epoch 90/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8231 - acc: 0.7752\n",
      "Epoch 00090: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 618us/sample - loss: 0.8267 - acc: 0.7730 - val_loss: 1.6170 - val_acc: 0.6028\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8312 - acc: 0.7887\n",
      "Epoch 00091: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 607us/sample - loss: 0.8293 - acc: 0.7884 - val_loss: 1.5887 - val_acc: 0.6241\n",
      "Epoch 92/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8381 - acc: 0.7675\n",
      "Epoch 00092: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 592us/sample - loss: 0.8349 - acc: 0.7701 - val_loss: 1.7309 - val_acc: 0.5934\n",
      "Epoch 93/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8749 - acc: 0.7590\n",
      "Epoch 00093: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 601us/sample - loss: 0.8764 - acc: 0.7595 - val_loss: 1.4885 - val_acc: 0.6336\n",
      "Epoch 94/100\n",
      "1568/1692 [==========================>...] - ETA: 0s - loss: 0.8346 - acc: 0.7704\n",
      "Epoch 00094: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 572us/sample - loss: 0.8384 - acc: 0.7683 - val_loss: 1.4672 - val_acc: 0.6288\n",
      "Epoch 95/100\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8307 - acc: 0.7704\n",
      "Epoch 00095: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 598us/sample - loss: 0.8268 - acc: 0.7713 - val_loss: 1.5467 - val_acc: 0.6052\n",
      "Epoch 96/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8329 - acc: 0.7763\n",
      "Epoch 00096: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 600us/sample - loss: 0.8318 - acc: 0.7772 - val_loss: 1.5473 - val_acc: 0.6241\n",
      "Epoch 97/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8196 - acc: 0.7843\n",
      "Epoch 00097: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 609us/sample - loss: 0.8174 - acc: 0.7837 - val_loss: 1.5880 - val_acc: 0.6265\n",
      "Epoch 98/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.8089 - acc: 0.7887\n",
      "Epoch 00098: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 618us/sample - loss: 0.8183 - acc: 0.7855 - val_loss: 1.5030 - val_acc: 0.6359\n",
      "Epoch 99/100\n",
      "1632/1692 [===========================>..] - ETA: 0s - loss: 0.8000 - acc: 0.7806\n",
      "Epoch 00099: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 593us/sample - loss: 0.7970 - acc: 0.7855 - val_loss: 1.5494 - val_acc: 0.6383\n",
      "Epoch 100/100\n",
      "1600/1692 [===========================>..] - ETA: 0s - loss: 0.7867 - acc: 0.7887\n",
      "Epoch 00100: val_loss did not improve from 1.15013\n",
      "1692/1692 [==============================] - 1s 606us/sample - loss: 0.7910 - acc: 0.7884 - val_loss: 1.5796 - val_acc: 0.6359\n",
      "443/443 [==============================] - 0s 206us/sample - loss: 1.5000 - acc: 0.6253\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "TIME_STRIDE = 1000\n",
    "for window_size in HP_TIME_WINDOW.domain.values:\n",
    "            \n",
    "    X_train_slices, y_train_slices = sliding_window(X_train_norm, \n",
    "                                                    y_train, \n",
    "                                                    time_window=window_size,  \n",
    "                                                    time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "    X_valid_slices, y_valid_slices = sliding_window(X_valid_norm, \n",
    "                                                    y_valid, \n",
    "                                                    time_window=window_size, \n",
    "                                                    time_stride=TIME_STRIDE)\n",
    "\n",
    "    X_test_slices, y_test_slices = sliding_window(X_test_norm, \n",
    "                                                    y_test, \n",
    "                                                    time_window=window_size, \n",
    "                                                    time_stride=TIME_STRIDE)\n",
    "\n",
    "    print(\"Training data shape with slices: {}\".format(X_train_slices.shape))\n",
    "    print(\"Training label shape with slice: {}\".format(y_train_slices.shape))\n",
    "    print(\"Validation data shape with slices: {}\".format(X_valid_slices.shape))\n",
    "    print(\"Validation label shape with slice: {}\".format(y_valid_slices.shape))\n",
    "    print(\"Testing data shape with slices: {}\".format(X_test_slices.shape))\n",
    "    print(\"Testing label shape with slice: {}\".format(y_test_slices.shape))\n",
    "    for num_units in HP_HIDDEN.domain.values:\n",
    "        for batch_size in HP_BATCH_SIZE.domain.values:\n",
    "            for dropout_rate in HP_DROPOUT.domain.values:\n",
    "                for last_lstm in HP_last_lstm_size.domain.values:\n",
    "                    for last_num_units in HP_last_hidden_layer.domain.values:\n",
    "                        for last_dropout_rate in HP_last_dropout.domain.values:\n",
    "                            for reg in HP_REGULARIZER.domain.values:\n",
    "                                for learning_rate in HP_LEARNING.domain.values:\n",
    "                                    for beta in HP_BETA.domain.values:\n",
    "                                        hparams = {\n",
    "                                            HP_TIME_WINDOW: window_size,\n",
    "                                            HP_BATCH_SIZE: batch_size,\n",
    "                                            HP_HIDDEN: num_units,\n",
    "                                            HP_DROPOUT: dropout_rate,\n",
    "                                            HP_REGULARIZER: reg,\n",
    "                                            HP_last_lstm_size: last_lstm,\n",
    "                                            HP_last_hidden_layer: last_num_units,\n",
    "                                            HP_last_dropout: last_dropout_rate,\n",
    "                                            HP_LEARNING: learning_rate,\n",
    "                                            HP_BETA: beta,\n",
    "                                        }\n",
    "\n",
    "                                        run_name = \"run-%d\" % session_num\n",
    "                                        print('--- Starting trial: %s' % run_name)\n",
    "                                        print({h.name: hparams[h] for h in hparams})\n",
    "                                        model = run('logs/crnn_hparam_tuning/' + run_name, hparams)\n",
    "                                        session_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 9483), started 0:08:28 ago. (Use '!kill 9483' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2f4acdf27a8d837f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2f4acdf27a8d837f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6008;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs/crnn_hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 27448\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
