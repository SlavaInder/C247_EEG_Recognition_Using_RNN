{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "This architecture was described in \"Deep learning with convolutional neural networks for brain mapping and decoding of movement-related information from the human EEG\", by R. T. Schmirrmester et al, 2018. In this notebook we conduct experimetns showing the importance of using batchnorm/dropout layers, and dependacy between accuracy and the number of timestamps in a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# import tf\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# import os functions\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sriramsonti/Desktop/project_C247\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"./EEG_data/X_test.npy\")\n",
    "y_test = np.load(\"./EEG_data/y_test.npy\") - 769\n",
    "person_train_valid = np.load(\"./EEG_data/person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"./EEG_data/X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"./EEG_data/y_train_valid.npy\") - 769\n",
    "person_test = np.load(\"./EEG_data/person_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/Valid data shape: (2115, 22, 1000)\n",
      "Test data shape: (443, 22, 1000)\n",
      "Training/Valid target shape: (2115,)\n",
      "Test target shape: (443,)\n",
      "Person train/valid  shape: (2115, 1)\n",
      "Person test shape: (443, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"training/Valid data shape: {}\".format(X_train_valid.shape))       # training data of many persons\n",
    "print(\"Test data shape: {}\".format(X_test.shape))                        # test data of many persons\n",
    "print(\"Training/Valid target shape: {}\".format(y_train_valid.shape))     # training labels of many persons\n",
    "print(\"Test target shape: {}\".format(y_test.shape))                      # test labels of many persons\n",
    "print(\"Person train/valid  shape: {}\".format(person_train_valid.shape))  # which person correspond to the trail in test set\n",
    "print(\"Person test shape: {}\".format(person_test.shape))                 # which person correspond to the trail in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### divide dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1692, 22, 1000)\n",
      "Training label shape: (1692,)\n",
      "Validation data shape: (423, 22, 1000)\n",
      "Validation label shape: (423,)\n",
      "Test data shape: (443, 22, 1000)\n",
      "Test label shape: (443,)\n"
     ]
    }
   ],
   "source": [
    "perm = np.random.permutation(X_train_valid.shape[0])\n",
    "num_train = int(0.8 * X_train_valid.shape[0])\n",
    "num_valid = X_train_valid.shape[0] - num_train\n",
    "X_train =  X_train_valid[perm[0:num_train]]\n",
    "y_train =  y_train_valid[perm[0:num_train]]\n",
    "X_valid = X_train_valid[perm[num_train: ]]\n",
    "y_valid = y_train_valid[perm[num_train: ]]\n",
    "\n",
    "\n",
    "print(\"Training data shape: {}\".format(X_train.shape))\n",
    "print(\"Training label shape: {}\".format(y_train.shape))\n",
    "print(\"Validation data shape: {}\".format(X_valid.shape))\n",
    "print(\"Validation label shape: {}\".format(y_valid.shape))\n",
    "print(\"Test data shape: {}\".format(X_test.shape))\n",
    "print(\"Test label shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(X_arr, y_arr, time_window=100, time_step=1, time_stride=1):\n",
    "    temp_x = np.moveaxis(X_arr, 2, 0)\n",
    "    temp_x = temp_x.astype(np.float32)\n",
    "    buff = []\n",
    "    \n",
    "    num_slices = (len(temp_x)-time_window*time_step) // time_stride + 1\n",
    "    \n",
    "    # get time slices for data\n",
    "    for i in range(num_slices):\n",
    "        buff.append(temp_x[i*time_stride:i*time_stride + time_window*time_step:time_step])\n",
    "        buff[i] = np.moveaxis(buff[i], 0, 2)\n",
    "        # uncomment this if additional dimension is needed\n",
    "        # buff[i] = buff[i].reshape(1, buff[i].shape[0], buff[i].shape[1], buff[i].shape[2])\n",
    "        \n",
    "    temp_x = np.concatenate(buff)\n",
    "        \n",
    "    # get time slice for labels\n",
    "    temp_y = np.ones((X_arr.shape[0],num_slices))\n",
    "    \n",
    "    for i in range(len(y_arr)):\n",
    "        temp_y[i] = temp_y[i] * y_arr[i]\n",
    "        \n",
    "    temp_y = temp_y.reshape((-1))\n",
    "    \n",
    "    return temp_x, temp_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: naive implementation of deep model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we show that naive implementation of deep convolutional model can not return good results because of overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "deep_input = layers.Input(shape=(22, 1000))\n",
    "\n",
    "\n",
    "\n",
    "# ================================== CONV1 ================================== #\n",
    "\n",
    "# conv accross time domain\n",
    "r1 = layers.Reshape((22, 1000, 1))(deep_input)\n",
    "c1 = layers.Conv2D(25, (1, 10), strides=(1, 1), activation=\"elu\")(r1)\n",
    "t1 = tf.keras.layers.Permute((2, 3, 1))(c1)\n",
    "\n",
    "# conv accross channels\n",
    "r2 = layers.Reshape((991, 25*22, 1))(t1)\n",
    "c2 = layers.Conv2D(25, (1, 25*22), strides=(1, 1), activation=\"elu\")(r2)\n",
    "\n",
    "# max pool across time domain\n",
    "r3 = layers.Reshape((991, 25, 1))(c2)\n",
    "maxpool3 = layers.MaxPooling2D(pool_size=(3, 1), strides=(3, 1))(r3)\n",
    "\n",
    "# =========================================================================== #\n",
    "\n",
    "\n",
    "\n",
    "# ================================= CONV2-4 ================================= #\n",
    "\n",
    "c4 = layers.Conv2D(50, (10, 25), strides=(1, 1), activation=\"elu\")(maxpool3)\n",
    "r4 = layers.Reshape((321, 50, 1))(c4)\n",
    "maxpool4 = layers.MaxPooling2D(pool_size=(3, 1), strides=(3, 1))(r4)\n",
    "\n",
    "c5 = layers.Conv2D(100, (10, 50), strides=(1, 1), activation=\"elu\")(maxpool4)\n",
    "r5 = layers.Reshape((98, 100, 1))(c5)\n",
    "maxpool5 = layers.MaxPooling2D(pool_size=(3, 1), strides=(3, 1))(r5)\n",
    "\n",
    "c6 = layers.Conv2D(200, (10, 100), strides=(1, 1), activation=\"elu\")(maxpool5)\n",
    "r6 = layers.Reshape((23, 200, 1))(c6)\n",
    "maxpool6 = layers.MaxPooling2D(pool_size=(3, 1), strides=(3, 1))(r6)\n",
    "\n",
    "# =========================================================================== #\n",
    "\n",
    "f7 = layers.Flatten()(r6)\n",
    "\n",
    "# output\n",
    "deep_output = layers.Dense(4, activation=\"softmax\")(f7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model = keras.Model(inputs = deep_input, outputs = deep_output, name=\"deep_model\")\n",
    "deep_model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"deep_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 22, 1000, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 22, 991, 25)       275       \n",
      "_________________________________________________________________\n",
      "permute (Permute)            (None, 991, 25, 22)       0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 991, 550, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 991, 1, 25)        13775     \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 991, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 330, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 321, 1, 50)        12550     \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 321, 50, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 107, 50, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 98, 1, 100)        50100     \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 98, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 23, 1, 200)        200200    \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 23, 200, 1)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 18404     \n",
      "=================================================================\n",
      "Total params: 295,304\n",
      "Trainable params: 295,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/deep_model_1000',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model on single person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape for 1 person: (189, 22, 1000)\n",
      "Training label shape for 1 person: (189,)\n",
      "Validation data shape for 1 person: (48, 22, 1000)\n",
      "Validation label shape for 1 person: (48,)\n",
      "Test data shape for 1 person: (50, 22, 1000)\n",
      "Test label shape for 1 person: (50,)\n"
     ]
    }
   ],
   "source": [
    "person_num = 0\n",
    "indices_train_valid = np.where(person_train_valid == person_num)[0]\n",
    "indices_test = np.where(person_test == person_num)[0]\n",
    "\n",
    "single_person_X_train_valid = X_train_valid[indices_train_valid]\n",
    "single_person_y_train_valid = y_train_valid[indices_train_valid]\n",
    "\n",
    "perm = np.random.permutation(single_person_X_train_valid.shape[0])\n",
    "num_train = int(0.8 * single_person_X_train_valid.shape[0])\n",
    "num_valid = single_person_X_train_valid.shape[0] - num_train\n",
    "single_person_X_train =  single_person_X_train_valid[perm[0:num_train]]\n",
    "single_person_y_train =  single_person_y_train_valid[perm[0:num_train]]\n",
    "single_person_X_valid = single_person_X_train_valid[perm[num_train: ]]\n",
    "single_person_y_valid = single_person_y_train_valid[perm[num_train: ]]\n",
    "\n",
    "single_person_X_test = X_test[indices_test]\n",
    "single_person_y_test = y_test[indices_test]\n",
    "\n",
    "\n",
    "print(\"Training data shape for 1 person: {}\".format(single_person_X_train.shape))\n",
    "print(\"Training label shape for 1 person: {}\".format(single_person_y_train.shape))\n",
    "print(\"Validation data shape for 1 person: {}\".format(single_person_X_valid.shape))\n",
    "print(\"Validation label shape for 1 person: {}\".format(single_person_y_valid.shape))\n",
    "print(\"Test data shape for 1 person: {}\".format(single_person_X_test.shape))\n",
    "print(\"Test label shape for 1 person: {}\".format(single_person_y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 189 samples, validate on 48 samples\n",
      "Epoch 1/30\n",
      "189/189 [==============================] - 14s 74ms/sample - loss: 1.3916 - acc: 0.2011 - val_loss: 1.3866 - val_acc: 0.2292\n",
      "Epoch 2/30\n",
      "189/189 [==============================] - 11s 59ms/sample - loss: 1.3955 - acc: 0.2751 - val_loss: 1.3862 - val_acc: 0.2500\n",
      "Epoch 3/30\n",
      "189/189 [==============================] - 11s 59ms/sample - loss: 1.3864 - acc: 0.2751 - val_loss: 1.3866 - val_acc: 0.2083\n",
      "Epoch 4/30\n",
      "189/189 [==============================] - 11s 57ms/sample - loss: 1.3863 - acc: 0.2593 - val_loss: 1.3865 - val_acc: 0.2083\n",
      "Epoch 5/30\n",
      "189/189 [==============================] - 11s 56ms/sample - loss: 1.3862 - acc: 0.2593 - val_loss: 1.3861 - val_acc: 0.2083\n",
      "Epoch 6/30\n",
      "189/189 [==============================] - 10s 55ms/sample - loss: 1.3884 - acc: 0.2751 - val_loss: 1.3744 - val_acc: 0.2917\n",
      "Epoch 7/30\n",
      "189/189 [==============================] - 10s 53ms/sample - loss: 1.3728 - acc: 0.3175 - val_loss: 1.3721 - val_acc: 0.2500\n",
      "Epoch 8/30\n",
      "189/189 [==============================] - 10s 53ms/sample - loss: 1.3361 - acc: 0.3968 - val_loss: 1.3803 - val_acc: 0.3333\n",
      "Epoch 9/30\n",
      "189/189 [==============================] - 11s 58ms/sample - loss: 1.1684 - acc: 0.4974 - val_loss: 1.4408 - val_acc: 0.2708\n",
      "Epoch 10/30\n",
      "189/189 [==============================] - 12s 63ms/sample - loss: 1.0739 - acc: 0.5026 - val_loss: 1.3499 - val_acc: 0.3542\n",
      "Epoch 11/30\n",
      "189/189 [==============================] - 11s 61ms/sample - loss: 0.9408 - acc: 0.5979 - val_loss: 1.3614 - val_acc: 0.3542\n",
      "Epoch 12/30\n",
      "189/189 [==============================] - 11s 60ms/sample - loss: 0.9428 - acc: 0.6508 - val_loss: 1.5749 - val_acc: 0.3125\n",
      "Epoch 13/30\n",
      "189/189 [==============================] - 11s 59ms/sample - loss: 0.7449 - acc: 0.7407 - val_loss: 1.9317 - val_acc: 0.3333\n",
      "Epoch 14/30\n",
      "189/189 [==============================] - 11s 60ms/sample - loss: 0.5504 - acc: 0.8148 - val_loss: 1.6005 - val_acc: 0.3125\n",
      "Epoch 15/30\n",
      "189/189 [==============================] - 11s 59ms/sample - loss: 0.5481 - acc: 0.8201 - val_loss: 2.4893 - val_acc: 0.2708\n",
      "Epoch 16/30\n",
      "189/189 [==============================] - 11s 59ms/sample - loss: 0.3556 - acc: 0.8571 - val_loss: 2.3947 - val_acc: 0.2708\n",
      "Epoch 17/30\n",
      "189/189 [==============================] - 11s 60ms/sample - loss: 0.5191 - acc: 0.8254 - val_loss: 1.8488 - val_acc: 0.2292\n",
      "Epoch 18/30\n",
      "189/189 [==============================] - 11s 58ms/sample - loss: 0.4193 - acc: 0.8889 - val_loss: 3.0534 - val_acc: 0.2708\n",
      "Epoch 19/30\n",
      "189/189 [==============================] - 12s 62ms/sample - loss: 0.4053 - acc: 0.8519 - val_loss: 3.4078 - val_acc: 0.2500\n",
      "Epoch 20/30\n",
      "189/189 [==============================] - 12s 62ms/sample - loss: 0.2999 - acc: 0.8942 - val_loss: 2.8020 - val_acc: 0.3125\n",
      "Epoch 21/30\n",
      "189/189 [==============================] - 12s 61ms/sample - loss: 0.2796 - acc: 0.9101 - val_loss: 4.3720 - val_acc: 0.2708\n",
      "Epoch 22/30\n",
      "189/189 [==============================] - 10s 55ms/sample - loss: 0.1050 - acc: 0.9577 - val_loss: 5.4189 - val_acc: 0.2917\n",
      "Epoch 23/30\n",
      "189/189 [==============================] - 10s 55ms/sample - loss: 0.1547 - acc: 0.9418 - val_loss: 6.0506 - val_acc: 0.2917\n",
      "Epoch 24/30\n",
      "189/189 [==============================] - 10s 55ms/sample - loss: 0.1320 - acc: 0.9524 - val_loss: 8.9397 - val_acc: 0.2292\n",
      "Epoch 25/30\n",
      "189/189 [==============================] - 11s 58ms/sample - loss: 0.1429 - acc: 0.9630 - val_loss: 9.9143 - val_acc: 0.2500\n",
      "Epoch 26/30\n",
      "189/189 [==============================] - 11s 56ms/sample - loss: 0.0878 - acc: 0.9735 - val_loss: 10.8937 - val_acc: 0.2917\n",
      "Epoch 27/30\n",
      "189/189 [==============================] - 11s 56ms/sample - loss: 0.1602 - acc: 0.9683 - val_loss: 8.3151 - val_acc: 0.2917\n",
      "Epoch 28/30\n",
      "189/189 [==============================] - 13s 67ms/sample - loss: 0.1507 - acc: 0.9630 - val_loss: 7.8116 - val_acc: 0.3333\n",
      "Epoch 29/30\n",
      "189/189 [==============================] - 13s 69ms/sample - loss: 0.1586 - acc: 0.9735 - val_loss: 10.8333 - val_acc: 0.2083\n",
      "Epoch 30/30\n",
      "189/189 [==============================] - 11s 60ms/sample - loss: 0.1807 - acc: 0.9577 - val_loss: 20.6853 - val_acc: 0.3125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x67a35ee50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model_single = keras.Model(inputs = deep_input, outputs = deep_output, name=\"deep_model\")\n",
    "deep_model_single.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "deep_model_single.fit(single_person_X_train, single_person_y_train,\n",
    "                                            validation_data = (single_person_X_valid, single_person_y_valid),\n",
    "                                            epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "50/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 23ms/sample - loss: 18.2833 - acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14.465117111206055, 0.5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model_single.evaluate(single_person_X_test, single_person_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "443/1 [==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 5s 12ms/sample - loss: 12.7616 - acc: 0.2551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15.780269883287142, 0.255079]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model_single.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3757 - acc: 0.3035\n",
      "Epoch 00001: val_loss did not improve from 1.32418\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.3776 - acc: 0.3026 - val_loss: 1.3581 - val_acc: 0.3641\n",
      "Epoch 2/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2591 - acc: 0.4111\n",
      "Epoch 00002: val_loss improved from 1.32418 to 1.26177, saving model to ./model_checkpoints/deep_model_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/deep_model_1000\\assets\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 1.2600 - acc: 0.4119 - val_loss: 1.2618 - val_acc: 0.4232\n",
      "Epoch 3/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2439 - acc: 0.4411\n",
      "Epoch 00003: val_loss improved from 1.26177 to 1.23481, saving model to ./model_checkpoints/deep_model_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/deep_model_1000\\assets\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 1.2463 - acc: 0.4403 - val_loss: 1.2348 - val_acc: 0.4326\n",
      "Epoch 4/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1615 - acc: 0.4994\n",
      "Epoch 00004: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.1722 - acc: 0.4959 - val_loss: 1.2383 - val_acc: 0.4326\n",
      "Epoch 5/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0111 - acc: 0.5907\n",
      "Epoch 00005: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.0135 - acc: 0.5881 - val_loss: 1.3196 - val_acc: 0.4515\n",
      "Epoch 6/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7830 - acc: 0.6869\n",
      "Epoch 00006: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 0.7835 - acc: 0.6879 - val_loss: 1.4717 - val_acc: 0.4728\n",
      "Epoch 7/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4851 - acc: 0.8287\n",
      "Epoch 00007: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 0.4905 - acc: 0.8257 - val_loss: 1.9341 - val_acc: 0.4090\n",
      "Epoch 8/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9062\n",
      "Epoch 00008: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.2717 - acc: 0.9066 - val_loss: 2.7414 - val_acc: 0.3735\n",
      "Epoch 9/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9315\n",
      "Epoch 00009: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.1953 - acc: 0.9309 - val_loss: 3.0183 - val_acc: 0.4279\n",
      "Epoch 10/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9297\n",
      "Epoch 00010: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.1985 - acc: 0.9285 - val_loss: 3.3248 - val_acc: 0.3948\n",
      "Epoch 11/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2863 - acc: 0.9105\n",
      "Epoch 00011: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.2856 - acc: 0.9090 - val_loss: 4.1610 - val_acc: 0.4161\n",
      "Epoch 12/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4142 - acc: 0.8888\n",
      "Epoch 00012: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.4100 - acc: 0.8901 - val_loss: 3.6934 - val_acc: 0.4255\n",
      "Epoch 13/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3464 - acc: 0.9081\n",
      "Epoch 00013: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.3431 - acc: 0.9090 - val_loss: 4.5983 - val_acc: 0.4019\n",
      "Epoch 14/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9471\n",
      "Epoch 00014: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.2040 - acc: 0.9462 - val_loss: 5.4618 - val_acc: 0.3972\n",
      "Epoch 15/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9501\n",
      "Epoch 00015: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.1883 - acc: 0.9492 - val_loss: 4.7157 - val_acc: 0.3924\n",
      "Epoch 16/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9639\n",
      "Epoch 00016: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.1205 - acc: 0.9639 - val_loss: 5.2568 - val_acc: 0.3995\n",
      "Epoch 17/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9657\n",
      "Epoch 00017: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.1223 - acc: 0.9663 - val_loss: 5.3257 - val_acc: 0.3901\n",
      "Epoch 18/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9784\n",
      "Epoch 00018: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.0621 - acc: 0.9787 - val_loss: 5.6026 - val_acc: 0.4043\n",
      "Epoch 19/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9820\n",
      "Epoch 00019: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.0590 - acc: 0.9823 - val_loss: 5.6382 - val_acc: 0.4232\n",
      "Epoch 20/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9874\n",
      "Epoch 00020: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.0404 - acc: 0.9870 - val_loss: 6.6005 - val_acc: 0.4043\n",
      "Epoch 21/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9802\n",
      "Epoch 00021: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.0654 - acc: 0.9805 - val_loss: 7.1111 - val_acc: 0.3452\n",
      "Epoch 22/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9838\n",
      "Epoch 00022: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.0666 - acc: 0.9835 - val_loss: 6.3463 - val_acc: 0.4019\n",
      "Epoch 23/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9730\n",
      "Epoch 00023: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.1212 - acc: 0.9734 - val_loss: 7.0743 - val_acc: 0.4421\n",
      "Epoch 24/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9579\n",
      "Epoch 00024: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.1872 - acc: 0.9586 - val_loss: 7.1897 - val_acc: 0.3901\n",
      "Epoch 25/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9567\n",
      "Epoch 00025: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 0.2111 - acc: 0.9557 - val_loss: 8.1032 - val_acc: 0.3924\n",
      "Epoch 26/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9525\n",
      "Epoch 00026: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 0.2371 - acc: 0.9504 - val_loss: 6.7777 - val_acc: 0.3877\n",
      "Epoch 27/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9501\n",
      "Epoch 00027: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 0.2048 - acc: 0.9504 - val_loss: 6.0602 - val_acc: 0.3924\n",
      "Epoch 28/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9651\n",
      "Epoch 00028: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 14s 9ms/sample - loss: 0.1768 - acc: 0.9657 - val_loss: 6.5768 - val_acc: 0.4019\n",
      "Epoch 29/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9778\n",
      "Epoch 00029: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 0.0774 - acc: 0.9775 - val_loss: 8.4878 - val_acc: 0.4279\n",
      "Epoch 30/30\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9718\n",
      "Epoch 00030: val_loss did not improve from 1.23481\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 0.1077 - acc: 0.9716 - val_loss: 8.3307 - val_acc: 0.4113\n"
     ]
    }
   ],
   "source": [
    "deep_model_loss_hist = deep_model.fit(X_train, y_train,\n",
    "                                      validation_data = (X_valid, y_valid),\n",
    "                                      epochs = 30,\n",
    "                                      callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x140e2947278>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = deep_model_loss_hist### Train model.history\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist['loss'])\n",
    "plt.plot(hist['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hist['acc'])\n",
    "plt.plot(hist['val_acc'])\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: deep model with augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we show that deep convolutional model can achieve good results if dropout and batchnorm are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_augmented_deep_model(TIME_WINDOW):\n",
    "    # input\n",
    "    deep_aug_input = layers.Input(shape=(22, TIME_WINDOW))\n",
    "\n",
    "\n",
    "    # ================================== CONV1 ================================== #\n",
    "\n",
    "    # conv accross time domain\n",
    "    r1 = layers.Reshape((22, TIME_WINDOW, 1))(deep_aug_input)\n",
    "    c1 = layers.Conv2D(25, (1, 10), strides=(1, 1))(r1)\n",
    "    new_size = TIME_WINDOW - 10 + 1\n",
    "    t1 = tf.keras.layers.Permute((2, 3, 1))(c1)\n",
    "\n",
    "    # # conv accross channels\n",
    "    r2 = layers.Reshape((new_size, 25*22, 1))(t1)\n",
    "    c2 = layers.Conv2D(25, (1, 25*22), strides=(1, 1))(r2)\n",
    "    bn2 = layers.BatchNormalization(axis=1)(c2)                 # do I use the right filter?\n",
    "    a2 = layers.Activation(\"elu\")(bn2)\n",
    "\n",
    "\n",
    "    # max pool across time domain\n",
    "    r3 = layers.Reshape((new_size, 25, 1))(a2)\n",
    "    maxpool3 = layers.MaxPooling2D(pool_size=(3, 1), strides=(3, 1))(r3)\n",
    "    new_size = (new_size - 3)//3 + 1\n",
    "    \n",
    "    # # =========================================================================== #\n",
    "\n",
    "\n",
    "\n",
    "    # # ================================= CONV2-4 ================================= #\n",
    "\n",
    "    c4 = layers.Conv2D(50, (10, 25), strides=(1, 1))(maxpool3)\n",
    "    new_size = new_size - 10 + 1\n",
    "    bn4 = layers.BatchNormalization(axis=1)(c4)\n",
    "    a4 = layers.Activation(\"elu\")(bn4)\n",
    "    do4 = layers.Dropout(0.5)(a4)\n",
    "    r4 = layers.Reshape((new_size, 50, 1))(do4)\n",
    "    maxpool4 = layers.MaxPooling2D(pool_size=(3, 1), strides=(3, 1))(r4)\n",
    "    new_size = (new_size - 3)//3 + 1\n",
    "    \n",
    "\n",
    "    c5 = layers.Conv2D(100, (10, 50), strides=(1, 1))(maxpool4)\n",
    "    new_size = new_size - 10 + 1\n",
    "    bn5 = layers.BatchNormalization(axis=1)(c5)\n",
    "    a5 = layers.Activation(\"elu\")(bn5)\n",
    "    do5 = layers.Dropout(0.5)(a5)\n",
    "    r5 = layers.Reshape((new_size, 100, 1))(do5)\n",
    "    maxpool5 = layers.MaxPooling2D(pool_size=(3, 1), strides=(3, 1))(r5)\n",
    "    new_size = (new_size - 3)//3 + 1\n",
    "    \n",
    "\n",
    "    c6 = layers.Conv2D(200, (10, 100), strides=(1, 1))(maxpool5)\n",
    "    new_size = new_size - 10 + 1\n",
    "    bn6 = layers.BatchNormalization(axis=1)(c6)\n",
    "    a6 = layers.Activation(\"elu\")(bn6)\n",
    "    do6 = layers.Dropout(0.5)(a6)\n",
    "    r6 = layers.Reshape((new_size, 200, 1))(do6)\n",
    "    maxpool6 = layers.MaxPooling2D(pool_size=(3, 1), strides=(3, 1))(r6)\n",
    "\n",
    "    # # =========================================================================== #\n",
    "\n",
    "    f7 = layers.Flatten()(do6)\n",
    "\n",
    "    # output\n",
    "    deep_aug_output = layers.Dense(4, activation=\"softmax\")(f7)\n",
    "    \n",
    "    return keras.Model(inputs = deep_aug_input, outputs = deep_aug_output, name=\"deep_aug_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_aug_model_1000 = construct_augmented_deep_model(1000)\n",
    "deep_aug_model_1000.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"deep_aug_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 22, 1000)]        0         \n",
      "_________________________________________________________________\n",
      "reshape_24 (Reshape)         (None, 22, 1000, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 22, 991, 25)       275       \n",
      "_________________________________________________________________\n",
      "permute_4 (Permute)          (None, 991, 25, 22)       0         \n",
      "_________________________________________________________________\n",
      "reshape_25 (Reshape)         (None, 991, 550, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 991, 1, 25)        13775     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 991, 1, 25)        3964      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 991, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "reshape_26 (Reshape)         (None, 991, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 330, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 321, 1, 50)        12550     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 321, 1, 50)        1284      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 321, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 321, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "reshape_27 (Reshape)         (None, 321, 50, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 107, 50, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 98, 1, 100)        50100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 98, 1, 100)        392       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 98, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 98, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "reshape_28 (Reshape)         (None, 98, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 32, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 23, 1, 200)        200200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 23, 1, 200)        92        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 23, 1, 200)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 23, 1, 200)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4600)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 18404     \n",
      "=================================================================\n",
      "Total params: 301,036\n",
      "Trainable params: 298,170\n",
      "Non-trainable params: 2,866\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deep_aug_model_1000.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/augmented_deep_model_1000',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.0531 - acc: 0.2488\n",
      "Epoch 00001: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 2.0498 - acc: 0.2476 - val_loss: 2.2321 - val_acc: 0.2695\n",
      "Epoch 2/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5540 - acc: 0.2698\n",
      "Epoch 00002: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.5516 - acc: 0.2701 - val_loss: 1.4844 - val_acc: 0.2742\n",
      "Epoch 3/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4981 - acc: 0.2855\n",
      "Epoch 00003: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.4991 - acc: 0.2849 - val_loss: 1.4466 - val_acc: 0.2955\n",
      "Epoch 4/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4622 - acc: 0.3161\n",
      "Epoch 00004: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.4601 - acc: 0.3174 - val_loss: 1.3443 - val_acc: 0.3381\n",
      "Epoch 5/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3931 - acc: 0.3347\n",
      "Epoch 00005: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.3942 - acc: 0.3339 - val_loss: 1.3439 - val_acc: 0.3428\n",
      "Epoch 6/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3316 - acc: 0.3774\n",
      "Epoch 00006: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 9ms/sample - loss: 1.3312 - acc: 0.3777 - val_loss: 1.3470 - val_acc: 0.4066\n",
      "Epoch 7/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3176 - acc: 0.3912\n",
      "Epoch 00007: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 1.3174 - acc: 0.3918 - val_loss: 1.3035 - val_acc: 0.4113\n",
      "Epoch 8/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2886 - acc: 0.4050\n",
      "Epoch 00008: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 16s 9ms/sample - loss: 1.2871 - acc: 0.4060 - val_loss: 1.2948 - val_acc: 0.3877\n",
      "Epoch 9/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2795 - acc: 0.4141\n",
      "Epoch 00009: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 1.2771 - acc: 0.4155 - val_loss: 1.2823 - val_acc: 0.4113\n",
      "Epoch 10/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2500 - acc: 0.4255\n",
      "Epoch 00010: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.2501 - acc: 0.4243 - val_loss: 1.3343 - val_acc: 0.3428\n",
      "Epoch 11/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1994 - acc: 0.4579\n",
      "Epoch 00011: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 1.2014 - acc: 0.4592 - val_loss: 1.2852 - val_acc: 0.4066\n",
      "Epoch 12/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2030 - acc: 0.4585\n",
      "Epoch 00012: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.2022 - acc: 0.4592 - val_loss: 1.3288 - val_acc: 0.3570\n",
      "Epoch 13/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1988 - acc: 0.4663\n",
      "Epoch 00013: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.1959 - acc: 0.4669 - val_loss: 1.2794 - val_acc: 0.4019\n",
      "Epoch 14/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1910 - acc: 0.4483\n",
      "Epoch 00014: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.1918 - acc: 0.4480 - val_loss: 1.2762 - val_acc: 0.4066\n",
      "Epoch 15/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1539 - acc: 0.4603\n",
      "Epoch 00015: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 1.1562 - acc: 0.4586 - val_loss: 1.3147 - val_acc: 0.3428\n",
      "Epoch 16/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1635 - acc: 0.4663\n",
      "Epoch 00016: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 1.1570 - acc: 0.4704 - val_loss: 1.2796 - val_acc: 0.4019\n",
      "Epoch 17/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1229 - acc: 0.4994\n",
      "Epoch 00017: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 1.1220 - acc: 0.4994 - val_loss: 1.2440 - val_acc: 0.4374\n",
      "Epoch 18/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1011 - acc: 0.5138\n",
      "Epoch 00018: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.1039 - acc: 0.5124 - val_loss: 1.1978 - val_acc: 0.4704\n",
      "Epoch 19/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0833 - acc: 0.5300\n",
      "Epoch 00019: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.0855 - acc: 0.5301 - val_loss: 1.3193 - val_acc: 0.3712\n",
      "Epoch 20/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0776 - acc: 0.5361\n",
      "Epoch 00020: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.0781 - acc: 0.5349 - val_loss: 1.2784 - val_acc: 0.3901\n",
      "Epoch 21/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0492 - acc: 0.5493\n",
      "Epoch 00021: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 1.0512 - acc: 0.5485 - val_loss: 1.2110 - val_acc: 0.4397\n",
      "Epoch 22/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0416 - acc: 0.5559\n",
      "Epoch 00022: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 1.0429 - acc: 0.5550 - val_loss: 1.2549 - val_acc: 0.4019\n",
      "Epoch 23/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0291 - acc: 0.5625\n",
      "Epoch 00023: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.0304 - acc: 0.5621 - val_loss: 1.1916 - val_acc: 0.4563\n",
      "Epoch 24/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0037 - acc: 0.5853\n",
      "Epoch 00024: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.0077 - acc: 0.5839 - val_loss: 1.2026 - val_acc: 0.4492\n",
      "Epoch 25/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9849 - acc: 0.5907\n",
      "Epoch 00025: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.9839 - acc: 0.5928 - val_loss: 1.2019 - val_acc: 0.4563\n",
      "Epoch 26/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9417 - acc: 0.6010\n",
      "Epoch 00026: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 0.9419 - acc: 0.6011 - val_loss: 1.1535 - val_acc: 0.4988\n",
      "Epoch 27/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9194 - acc: 0.6178\n",
      "Epoch 00027: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 0.9194 - acc: 0.6188 - val_loss: 1.1221 - val_acc: 0.5106\n",
      "Epoch 28/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9080 - acc: 0.6220\n",
      "Epoch 00028: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.9077 - acc: 0.6217 - val_loss: 1.1792 - val_acc: 0.4775\n",
      "Epoch 29/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8508 - acc: 0.6635\n",
      "Epoch 00029: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.8490 - acc: 0.6631 - val_loss: 1.2236 - val_acc: 0.4657\n",
      "Epoch 30/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8426 - acc: 0.6514\n",
      "Epoch 00030: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 0.8447 - acc: 0.6495 - val_loss: 1.1221 - val_acc: 0.4870\n",
      "Epoch 31/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8160 - acc: 0.6605\n",
      "Epoch 00031: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 0.8163 - acc: 0.6613 - val_loss: 1.1560 - val_acc: 0.4988\n",
      "Epoch 32/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8065 - acc: 0.6773\n",
      "Epoch 00032: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.8067 - acc: 0.6761 - val_loss: 1.1137 - val_acc: 0.5225\n",
      "Epoch 33/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7663 - acc: 0.6947\n",
      "Epoch 00033: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 0.7641 - acc: 0.6962 - val_loss: 1.1560 - val_acc: 0.5012\n",
      "Epoch 34/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7494 - acc: 0.7121\n",
      "Epoch 00034: val_loss did not improve from 1.09119\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 0.7494 - acc: 0.7116 - val_loss: 1.0939 - val_acc: 0.5366\n",
      "Epoch 35/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6887 - acc: 0.7230\n",
      "Epoch 00035: val_loss improved from 1.09119 to 1.08675, saving model to ./model_checkpoints/augmented_deep_model_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_1000\\assets\n",
      "1692/1692 [==============================] - 16s 10ms/sample - loss: 0.6997 - acc: 0.7181 - val_loss: 1.0867 - val_acc: 0.5485\n",
      "Epoch 36/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6646 - acc: 0.7362\n",
      "Epoch 00036: val_loss improved from 1.08675 to 1.05357, saving model to ./model_checkpoints/augmented_deep_model_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_1000\\assets\n",
      "1692/1692 [==============================] - 16s 10ms/sample - loss: 0.6647 - acc: 0.7388 - val_loss: 1.0536 - val_acc: 0.5556\n",
      "Epoch 37/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6298 - acc: 0.7596\n",
      "Epoch 00037: val_loss improved from 1.05357 to 1.04911, saving model to ./model_checkpoints/augmented_deep_model_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_1000\\assets\n",
      "1692/1692 [==============================] - 16s 10ms/sample - loss: 0.6311 - acc: 0.7589 - val_loss: 1.0491 - val_acc: 0.5579\n",
      "Epoch 38/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5911 - acc: 0.7758\n",
      "Epoch 00038: val_loss improved from 1.04911 to 1.03984, saving model to ./model_checkpoints/augmented_deep_model_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_1000\\assets\n",
      "1692/1692 [==============================] - 16s 10ms/sample - loss: 0.5890 - acc: 0.7760 - val_loss: 1.0398 - val_acc: 0.5934\n",
      "Epoch 39/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5750 - acc: 0.7825\n",
      "Epoch 00039: val_loss did not improve from 1.03984\n",
      "1692/1692 [==============================] - 16s 9ms/sample - loss: 0.5778 - acc: 0.7807 - val_loss: 1.1020 - val_acc: 0.5579\n",
      "Epoch 40/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5726 - acc: 0.7885\n",
      "Epoch 00040: val_loss did not improve from 1.03984\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.5694 - acc: 0.7890 - val_loss: 1.1283 - val_acc: 0.5319\n",
      "Epoch 41/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5226 - acc: 0.8047\n",
      "Epoch 00041: val_loss did not improve from 1.03984\n",
      "1692/1692 [==============================] - 16s 9ms/sample - loss: 0.5210 - acc: 0.8061 - val_loss: 1.1342 - val_acc: 0.5225\n",
      "Epoch 42/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.8017\n",
      "Epoch 00042: val_loss did not improve from 1.03984\n",
      "1692/1692 [==============================] - 16s 9ms/sample - loss: 0.5216 - acc: 0.7991 - val_loss: 1.1151 - val_acc: 0.5603\n",
      "Epoch 43/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4897 - acc: 0.8131\n",
      "Epoch 00043: val_loss did not improve from 1.03984\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.4920 - acc: 0.8121 - val_loss: 1.1693 - val_acc: 0.5485\n",
      "Epoch 44/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4459 - acc: 0.8353\n",
      "Epoch 00044: val_loss did not improve from 1.03984\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.4519 - acc: 0.8327 - val_loss: 1.0627 - val_acc: 0.5556\n",
      "Epoch 45/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4459 - acc: 0.8269\n",
      "Epoch 00045: val_loss did not improve from 1.03984\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.4453 - acc: 0.8274 - val_loss: 1.0618 - val_acc: 0.6005\n",
      "Epoch 46/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4568 - acc: 0.8251\n",
      "Epoch 00046: val_loss did not improve from 1.03984\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.4633 - acc: 0.8233 - val_loss: 1.1314 - val_acc: 0.5508\n",
      "Epoch 47/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4134 - acc: 0.8425\n",
      "Epoch 00047: val_loss did not improve from 1.03984\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.4131 - acc: 0.8434 - val_loss: 1.1328 - val_acc: 0.5603\n",
      "Epoch 48/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3652 - acc: 0.8630\n",
      "Epoch 00048: val_loss did not improve from 1.03984\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.3654 - acc: 0.8641 - val_loss: 1.1246 - val_acc: 0.5839\n",
      "Epoch 49/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3660 - acc: 0.8660\n",
      "Epoch 00049: val_loss did not improve from 1.03984\n",
      "1692/1692 [==============================] - 16s 9ms/sample - loss: 0.3648 - acc: 0.8664 - val_loss: 1.0710 - val_acc: 0.6028\n",
      "Epoch 50/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3514 - acc: 0.8690\n",
      "Epoch 00050: val_loss improved from 1.03984 to 1.03585, saving model to ./model_checkpoints/augmented_deep_model_1000\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_1000\\assets\n",
      "1692/1692 [==============================] - 16s 10ms/sample - loss: 0.3502 - acc: 0.8694 - val_loss: 1.0358 - val_acc: 0.5934\n",
      "Epoch 51/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3418 - acc: 0.8594\n",
      "Epoch 00051: val_loss did not improve from 1.03585\n",
      "1692/1692 [==============================] - 14s 9ms/sample - loss: 0.3460 - acc: 0.8576 - val_loss: 1.0866 - val_acc: 0.5887\n",
      "Epoch 52/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8690\n",
      "Epoch 00052: val_loss did not improve from 1.03585\n",
      "1692/1692 [==============================] - 14s 9ms/sample - loss: 0.3497 - acc: 0.8706 - val_loss: 1.2220 - val_acc: 0.5579\n",
      "Epoch 53/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2791 - acc: 0.9026\n",
      "Epoch 00053: val_loss did not improve from 1.03585\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.2792 - acc: 0.9031 - val_loss: 1.6984 - val_acc: 0.4846\n",
      "Epoch 54/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3049 - acc: 0.8834\n",
      "Epoch 00054: val_loss did not improve from 1.03585\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.3109 - acc: 0.8818 - val_loss: 1.2395 - val_acc: 0.5745\n",
      "Epoch 55/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3165 - acc: 0.8840\n",
      "Epoch 00055: val_loss did not improve from 1.03585\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.3144 - acc: 0.8842 - val_loss: 1.3292 - val_acc: 0.5437\n",
      "Epoch 56/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.8894\n",
      "Epoch 00056: val_loss did not improve from 1.03585\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.2846 - acc: 0.8913 - val_loss: 1.3939 - val_acc: 0.5579\n",
      "Epoch 57/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.8840\n",
      "Epoch 00057: val_loss did not improve from 1.03585\n",
      "1692/1692 [==============================] - 16s 10ms/sample - loss: 0.2784 - acc: 0.8853 - val_loss: 1.3818 - val_acc: 0.5532\n",
      "Epoch 58/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.8996\n",
      "Epoch 00058: val_loss did not improve from 1.03585\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.2668 - acc: 0.9007 - val_loss: 1.2104 - val_acc: 0.6005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.8960\n",
      "Epoch 00059: val_loss did not improve from 1.03585\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.2806 - acc: 0.8972 - val_loss: 1.2719 - val_acc: 0.5863\n",
      "Epoch 60/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.9050\n",
      "Epoch 00060: val_loss did not improve from 1.03585\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.2601 - acc: 0.9060 - val_loss: 1.2812 - val_acc: 0.5957\n"
     ]
    }
   ],
   "source": [
    "deep_aug_model_1000_loss_hist = deep_aug_model_1000.fit(X_train, y_train,\n",
    "                                                        validation_data = (X_valid, y_valid),\n",
    "                                                        epochs = 60,\n",
    "                                                        callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x140e54493c8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = deep_aug_model_1000_loss_hist.history\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist['loss'])\n",
    "plt.plot(hist['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hist['acc'])\n",
    "plt.plot(hist['val_acc'])\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Augmented deep model - accuracy vs number of timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape with slices: (1692, 22, 900)\n",
      "Training label shape with slice: (1692,)\n",
      "Validation data shape with slices: (423, 22, 900)\n",
      "Validation label shape with slice: (423,)\n",
      "Model: \"deep_aug_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 22, 900)]         0         \n",
      "_________________________________________________________________\n",
      "reshape_36 (Reshape)         (None, 22, 900, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 22, 891, 25)       275       \n",
      "_________________________________________________________________\n",
      "permute_6 (Permute)          (None, 891, 25, 22)       0         \n",
      "_________________________________________________________________\n",
      "reshape_37 (Reshape)         (None, 891, 550, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 891, 1, 25)        13775     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 891, 1, 25)        3564      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 891, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "reshape_38 (Reshape)         (None, 891, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 297, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 288, 1, 50)        12550     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 288, 1, 50)        1152      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 288, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 288, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "reshape_39 (Reshape)         (None, 288, 50, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 96, 50, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 87, 1, 100)        50100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 87, 1, 100)        348       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 87, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 87, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "reshape_40 (Reshape)         (None, 87, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 29, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 20, 1, 200)        200200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 20, 1, 200)        80        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 20, 1, 200)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 20, 1, 200)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 16004     \n",
      "=================================================================\n",
      "Total params: 298,048\n",
      "Trainable params: 295,476\n",
      "Non-trainable params: 2,572\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.1889 - acc: 0.2879\n",
      "Epoch 00001: val_loss improved from inf to 1.39285, saving model to ./model_checkpoints/augmented_deep_model_900\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_900\\assets\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 2.1749 - acc: 0.2878 - val_loss: 1.3928 - val_acc: 0.3073\n",
      "Epoch 2/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5541 - acc: 0.3011\n",
      "Epoch 00002: val_loss did not improve from 1.39285\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.5549 - acc: 0.3008 - val_loss: 1.4755 - val_acc: 0.3286\n",
      "Epoch 3/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4457 - acc: 0.3450\n",
      "Epoch 00003: val_loss improved from 1.39285 to 1.31248, saving model to ./model_checkpoints/augmented_deep_model_900\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_900\\assets\n",
      "1692/1692 [==============================] - 16s 9ms/sample - loss: 1.4425 - acc: 0.3463 - val_loss: 1.3125 - val_acc: 0.3688\n",
      "Epoch 4/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3492 - acc: 0.3882\n",
      "Epoch 00004: val_loss did not improve from 1.31248\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.3518 - acc: 0.3865 - val_loss: 1.3209 - val_acc: 0.3853\n",
      "Epoch 5/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3389 - acc: 0.3888\n",
      "Epoch 00005: val_loss improved from 1.31248 to 1.28172, saving model to ./model_checkpoints/augmented_deep_model_900\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_900\\assets\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 1.3360 - acc: 0.3918 - val_loss: 1.2817 - val_acc: 0.3688\n",
      "Epoch 6/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3222 - acc: 0.3984\n",
      "Epoch 00006: val_loss did not improve from 1.28172\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.3253 - acc: 0.3983 - val_loss: 1.3593 - val_acc: 0.3475\n",
      "Epoch 7/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2687 - acc: 0.4117\n",
      "Epoch 00007: val_loss did not improve from 1.28172\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.2712 - acc: 0.4090 - val_loss: 1.3134 - val_acc: 0.3712\n",
      "Epoch 8/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2770 - acc: 0.4213\n",
      "Epoch 00008: val_loss did not improve from 1.28172\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.2762 - acc: 0.4226 - val_loss: 1.3054 - val_acc: 0.3499\n",
      "Epoch 9/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2402 - acc: 0.4261\n",
      "Epoch 00009: val_loss did not improve from 1.28172\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.2428 - acc: 0.4238 - val_loss: 1.2901 - val_acc: 0.3995\n",
      "Epoch 10/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2262 - acc: 0.4465\n",
      "Epoch 00010: val_loss improved from 1.28172 to 1.24686, saving model to ./model_checkpoints/augmented_deep_model_900\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_900\\assets\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 1.2239 - acc: 0.4498 - val_loss: 1.2469 - val_acc: 0.4184\n",
      "Epoch 11/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2056 - acc: 0.4663\n",
      "Epoch 00011: val_loss did not improve from 1.24686\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.2054 - acc: 0.4669 - val_loss: 1.3185 - val_acc: 0.3759\n",
      "Epoch 12/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1599 - acc: 0.4748\n",
      "Epoch 00012: val_loss did not improve from 1.24686\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.1600 - acc: 0.4740 - val_loss: 1.2923 - val_acc: 0.3712\n",
      "Epoch 13/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1701 - acc: 0.4796\n",
      "Epoch 00013: val_loss improved from 1.24686 to 1.23529, saving model to ./model_checkpoints/augmented_deep_model_900\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_900\\assets\n",
      "1692/1692 [==============================] - 16s 9ms/sample - loss: 1.1677 - acc: 0.4811 - val_loss: 1.2353 - val_acc: 0.4421\n",
      "Epoch 14/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1288 - acc: 0.4946\n",
      "Epoch 00014: val_loss improved from 1.23529 to 1.22429, saving model to ./model_checkpoints/augmented_deep_model_900\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_900\\assets\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 1.1286 - acc: 0.4947 - val_loss: 1.2243 - val_acc: 0.4610\n",
      "Epoch 15/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1035 - acc: 0.5300\n",
      "Epoch 00015: val_loss did not improve from 1.22429\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.1045 - acc: 0.5301 - val_loss: 1.2273 - val_acc: 0.4421\n",
      "Epoch 16/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0761 - acc: 0.5373\n",
      "Epoch 00016: val_loss did not improve from 1.22429\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.0785 - acc: 0.5349 - val_loss: 1.2727 - val_acc: 0.4232\n",
      "Epoch 17/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0624 - acc: 0.5499\n",
      "Epoch 00017: val_loss improved from 1.22429 to 1.21432, saving model to ./model_checkpoints/augmented_deep_model_900\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_900\\assets\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 1.0621 - acc: 0.5502 - val_loss: 1.2143 - val_acc: 0.4704\n",
      "Epoch 18/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0509 - acc: 0.5619\n",
      "Epoch 00018: val_loss improved from 1.21432 to 1.13558, saving model to ./model_checkpoints/augmented_deep_model_900\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_900\\assets\n",
      "1692/1692 [==============================] - 16s 9ms/sample - loss: 1.0519 - acc: 0.5615 - val_loss: 1.1356 - val_acc: 0.5083\n",
      "Epoch 19/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9932 - acc: 0.5919\n",
      "Epoch 00019: val_loss did not improve from 1.13558\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.9903 - acc: 0.5934 - val_loss: 1.1515 - val_acc: 0.5083\n",
      "Epoch 20/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9705 - acc: 0.6004\n",
      "Epoch 00020: val_loss improved from 1.13558 to 1.13227, saving model to ./model_checkpoints/augmented_deep_model_900\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_900\\assets\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.9757 - acc: 0.5999 - val_loss: 1.1323 - val_acc: 0.4941\n",
      "Epoch 21/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9758 - acc: 0.5823\n",
      "Epoch 00021: val_loss improved from 1.13227 to 1.10390, saving model to ./model_checkpoints/augmented_deep_model_900\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_900\\assets\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.9716 - acc: 0.5839 - val_loss: 1.1039 - val_acc: 0.5059\n",
      "Epoch 22/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9225 - acc: 0.6250\n",
      "Epoch 00022: val_loss did not improve from 1.10390\n",
      "1692/1692 [==============================] - 13s 7ms/sample - loss: 0.9230 - acc: 0.6265 - val_loss: 1.1142 - val_acc: 0.5154\n",
      "Epoch 23/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8824 - acc: 0.6466\n",
      "Epoch 00023: val_loss improved from 1.10390 to 1.08266, saving model to ./model_checkpoints/augmented_deep_model_900\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_900\\assets\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.8833 - acc: 0.6448 - val_loss: 1.0827 - val_acc: 0.5414\n",
      "Epoch 24/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8478 - acc: 0.6587\n",
      "Epoch 00024: val_loss did not improve from 1.08266\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.8443 - acc: 0.6608 - val_loss: 1.0905 - val_acc: 0.5296\n",
      "Epoch 25/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8295 - acc: 0.6593\n",
      "Epoch 00025: val_loss did not improve from 1.08266\n",
      "1692/1692 [==============================] - 13s 7ms/sample - loss: 0.8303 - acc: 0.6584 - val_loss: 1.1556 - val_acc: 0.5225\n",
      "Epoch 26/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7888 - acc: 0.6845\n",
      "Epoch 00026: val_loss did not improve from 1.08266\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.7867 - acc: 0.6856 - val_loss: 1.0988 - val_acc: 0.5414\n",
      "Epoch 27/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7542 - acc: 0.7019\n",
      "Epoch 00027: val_loss did not improve from 1.08266\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.7543 - acc: 0.7015 - val_loss: 1.1043 - val_acc: 0.5201\n",
      "Epoch 28/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7438 - acc: 0.6971\n",
      "Epoch 00028: val_loss did not improve from 1.08266\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.7498 - acc: 0.6956 - val_loss: 1.1159 - val_acc: 0.5366\n",
      "Epoch 29/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7184 - acc: 0.7266\n",
      "Epoch 00029: val_loss did not improve from 1.08266\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.7195 - acc: 0.7264 - val_loss: 1.0922 - val_acc: 0.5556\n",
      "Epoch 30/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6657 - acc: 0.7434\n",
      "Epoch 00030: val_loss did not improve from 1.08266\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.6632 - acc: 0.7441 - val_loss: 1.0990 - val_acc: 0.5414\n",
      "Epoch 31/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6459 - acc: 0.7530\n",
      "Epoch 00031: val_loss improved from 1.08266 to 1.07223, saving model to ./model_checkpoints/augmented_deep_model_900\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_900\\assets\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.6478 - acc: 0.7518 - val_loss: 1.0722 - val_acc: 0.5603\n",
      "Epoch 32/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6101 - acc: 0.7626\n",
      "Epoch 00032: val_loss did not improve from 1.07223\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.6077 - acc: 0.7630 - val_loss: 1.0782 - val_acc: 0.5745\n",
      "Epoch 33/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5950 - acc: 0.7806\n",
      "Epoch 00033: val_loss improved from 1.07223 to 1.05046, saving model to ./model_checkpoints/augmented_deep_model_900\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_900\\assets\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.5937 - acc: 0.7813 - val_loss: 1.0505 - val_acc: 0.5414\n",
      "Epoch 34/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5567 - acc: 0.7933\n",
      "Epoch 00034: val_loss did not improve from 1.05046\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.5568 - acc: 0.7937 - val_loss: 1.0866 - val_acc: 0.5626\n",
      "Epoch 35/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.7963\n",
      "Epoch 00035: val_loss did not improve from 1.05046\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.5419 - acc: 0.7937 - val_loss: 1.1300 - val_acc: 0.5579\n",
      "Epoch 36/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.7993\n",
      "Epoch 00036: val_loss did not improve from 1.05046\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.5118 - acc: 0.7996 - val_loss: 1.1016 - val_acc: 0.5674\n",
      "Epoch 37/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4876 - acc: 0.8077\n",
      "Epoch 00037: val_loss did not improve from 1.05046\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.4941 - acc: 0.8032 - val_loss: 1.1148 - val_acc: 0.5508\n",
      "Epoch 38/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.8023\n",
      "Epoch 00038: val_loss did not improve from 1.05046\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.5154 - acc: 0.8014 - val_loss: 1.1181 - val_acc: 0.5626\n",
      "Epoch 39/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4662 - acc: 0.8257\n",
      "Epoch 00039: val_loss did not improve from 1.05046\n",
      "1692/1692 [==============================] - 13s 7ms/sample - loss: 0.4656 - acc: 0.8262 - val_loss: 1.0760 - val_acc: 0.5792\n",
      "Epoch 40/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4243 - acc: 0.8347\n",
      "Epoch 00040: val_loss improved from 1.05046 to 1.04176, saving model to ./model_checkpoints/augmented_deep_model_900\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_900\\assets\n",
      "1692/1692 [==============================] - 15s 9ms/sample - loss: 0.4210 - acc: 0.8369 - val_loss: 1.0418 - val_acc: 0.5981\n",
      "Epoch 41/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8636\n",
      "Epoch 00041: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.3810 - acc: 0.8623 - val_loss: 1.1421 - val_acc: 0.5887\n",
      "Epoch 42/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4305 - acc: 0.8329\n",
      "Epoch 00042: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.4313 - acc: 0.8322 - val_loss: 1.2437 - val_acc: 0.5603\n",
      "Epoch 43/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4128 - acc: 0.8407\n",
      "Epoch 00043: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.4130 - acc: 0.8410 - val_loss: 1.2294 - val_acc: 0.5626\n",
      "Epoch 44/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8335\n",
      "Epoch 00044: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.4078 - acc: 0.8345 - val_loss: 1.1481 - val_acc: 0.5934\n",
      "Epoch 45/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.8504\n",
      "Epoch 00045: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.3948 - acc: 0.8499 - val_loss: 1.1326 - val_acc: 0.5957\n",
      "Epoch 46/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3152 - acc: 0.8828\n",
      "Epoch 00046: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.3170 - acc: 0.8812 - val_loss: 1.1474 - val_acc: 0.5863\n",
      "Epoch 47/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8660\n",
      "Epoch 00047: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.3548 - acc: 0.8635 - val_loss: 1.1981 - val_acc: 0.6028\n",
      "Epoch 48/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.8894\n",
      "Epoch 00048: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.3235 - acc: 0.8889 - val_loss: 1.1583 - val_acc: 0.5721\n",
      "Epoch 49/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.8792\n",
      "Epoch 00049: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 7ms/sample - loss: 0.3211 - acc: 0.8800 - val_loss: 1.1588 - val_acc: 0.5957\n",
      "Epoch 50/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.8828\n",
      "Epoch 00050: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.3207 - acc: 0.8842 - val_loss: 1.2395 - val_acc: 0.5816\n",
      "Epoch 51/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.8666\n",
      "Epoch 00051: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.3343 - acc: 0.8670 - val_loss: 1.2811 - val_acc: 0.6005\n",
      "Epoch 52/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2857 - acc: 0.8906\n",
      "Epoch 00052: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.2834 - acc: 0.8918 - val_loss: 1.4103 - val_acc: 0.5697\n",
      "Epoch 53/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.8894\n",
      "Epoch 00053: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.2828 - acc: 0.8913 - val_loss: 1.2105 - val_acc: 0.6028\n",
      "Epoch 54/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3300 - acc: 0.8660\n",
      "Epoch 00054: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.3299 - acc: 0.8658 - val_loss: 1.2040 - val_acc: 0.5816\n",
      "Epoch 55/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.9020\n",
      "Epoch 00055: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.2783 - acc: 0.9025 - val_loss: 1.2179 - val_acc: 0.5816\n",
      "Epoch 56/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9087\n",
      "Epoch 00056: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.2305 - acc: 0.9102 - val_loss: 1.3703 - val_acc: 0.5721\n",
      "Epoch 57/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9171\n",
      "Epoch 00057: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.2377 - acc: 0.9161 - val_loss: 1.1525 - val_acc: 0.5887\n",
      "Epoch 58/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9159\n",
      "Epoch 00058: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.2305 - acc: 0.9155 - val_loss: 1.4034 - val_acc: 0.5579\n",
      "Epoch 59/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2592 - acc: 0.9014\n",
      "Epoch 00059: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.2575 - acc: 0.9025 - val_loss: 1.2718 - val_acc: 0.5745\n",
      "Epoch 60/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9141\n",
      "Epoch 00060: val_loss did not improve from 1.04176\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.2140 - acc: 0.9149 - val_loss: 1.2333 - val_acc: 0.5981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x140e9574da0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIME_WINDOW = 900\n",
    "TIME_STRIDE = 1000\n",
    "\n",
    "# cut the slices\n",
    "X_train_slices, y_train_slices = sliding_window(X_train, \n",
    "                                                y_train, \n",
    "                                                time_window=TIME_WINDOW,  \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "X_valid_slices, y_valid_slices = sliding_window(X_valid, \n",
    "                                                y_valid, \n",
    "                                                time_window=TIME_WINDOW, \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "print(\"Training data shape with slices: {}\".format(X_train_slices.shape))\n",
    "print(\"Training label shape with slice: {}\".format(y_train_slices.shape))\n",
    "print(\"Validation data shape with slices: {}\".format(X_valid_slices.shape))\n",
    "print(\"Validation label shape with slice: {}\".format(y_valid_slices.shape))\n",
    "\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/augmented_deep_model_' + str(TIME_WINDOW),\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "deep_aug_model_900 = construct_augmented_deep_model(TIME_WINDOW)\n",
    "deep_aug_model_900.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "deep_aug_model_900.summary()\n",
    "\n",
    "deep_aug_model_900.fit(X_train_slices, y_train_slices,\n",
    "                       validation_data = (X_valid_slices, y_valid_slices),\n",
    "                       epochs = 60,\n",
    "                       callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape with slices: (1692, 22, 800)\n",
      "Training label shape with slice: (1692,)\n",
      "Validation data shape with slices: (423, 22, 800)\n",
      "Validation label shape with slice: (423,)\n",
      "Model: \"deep_aug_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 22, 800)]         0         \n",
      "_________________________________________________________________\n",
      "reshape_42 (Reshape)         (None, 22, 800, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 22, 791, 25)       275       \n",
      "_________________________________________________________________\n",
      "permute_7 (Permute)          (None, 791, 25, 22)       0         \n",
      "_________________________________________________________________\n",
      "reshape_43 (Reshape)         (None, 791, 550, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 791, 1, 25)        13775     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 791, 1, 25)        3164      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 791, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "reshape_44 (Reshape)         (None, 791, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 263, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 254, 1, 50)        12550     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 254, 1, 50)        1016      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 254, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 254, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "reshape_45 (Reshape)         (None, 254, 50, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 84, 50, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 75, 1, 100)        50100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 75, 1, 100)        300       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 75, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 75, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "reshape_46 (Reshape)         (None, 75, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 25, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 16, 1, 200)        200200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 16, 1, 200)        64        \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16, 1, 200)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 16, 1, 200)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 12804     \n",
      "=================================================================\n",
      "Total params: 294,248\n",
      "Trainable params: 291,976\n",
      "Non-trainable params: 2,272\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.1238 - acc: 0.2500\n",
      "Epoch 00001: val_loss improved from inf to 1.82139, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 2.1184 - acc: 0.2500 - val_loss: 1.8214 - val_acc: 0.2671\n",
      "Epoch 2/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5580 - acc: 0.2470\n",
      "Epoch 00002: val_loss improved from 1.82139 to 1.40812, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.5605 - acc: 0.2447 - val_loss: 1.4081 - val_acc: 0.2648\n",
      "Epoch 3/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5266 - acc: 0.2554\n",
      "Epoch 00003: val_loss improved from 1.40812 to 1.39738, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.5301 - acc: 0.2535 - val_loss: 1.3974 - val_acc: 0.2624\n",
      "Epoch 4/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4670 - acc: 0.2794\n",
      "Epoch 00004: val_loss improved from 1.39738 to 1.38711, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.4665 - acc: 0.2813 - val_loss: 1.3871 - val_acc: 0.2931\n",
      "Epoch 5/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4113 - acc: 0.3083\n",
      "Epoch 00005: val_loss improved from 1.38711 to 1.37861, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.4118 - acc: 0.3073 - val_loss: 1.3786 - val_acc: 0.3026\n",
      "Epoch 6/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3964 - acc: 0.3221\n",
      "Epoch 00006: val_loss improved from 1.37861 to 1.37275, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.3950 - acc: 0.3221 - val_loss: 1.3728 - val_acc: 0.3191\n",
      "Epoch 7/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3543 - acc: 0.3762\n",
      "Epoch 00007: val_loss did not improve from 1.37275\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 1.3541 - acc: 0.3753 - val_loss: 1.3775 - val_acc: 0.3262\n",
      "Epoch 8/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3267 - acc: 0.3786\n",
      "Epoch 00008: val_loss improved from 1.37275 to 1.30737, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.3253 - acc: 0.3783 - val_loss: 1.3074 - val_acc: 0.3735\n",
      "Epoch 9/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3027 - acc: 0.3858\n",
      "Epoch 00009: val_loss improved from 1.30737 to 1.27078, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.3017 - acc: 0.3871 - val_loss: 1.2708 - val_acc: 0.3972\n",
      "Epoch 10/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2760 - acc: 0.4291\n",
      "Epoch 00010: val_loss did not improve from 1.27078\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 1.2745 - acc: 0.4309 - val_loss: 1.3314 - val_acc: 0.3428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2358 - acc: 0.4423\n",
      "Epoch 00011: val_loss did not improve from 1.27078\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 1.2320 - acc: 0.4450 - val_loss: 1.2886 - val_acc: 0.3830\n",
      "Epoch 12/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2271 - acc: 0.4489\n",
      "Epoch 00012: val_loss did not improve from 1.27078\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 1.2295 - acc: 0.4456 - val_loss: 1.3083 - val_acc: 0.3830\n",
      "Epoch 13/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1999 - acc: 0.4561\n",
      "Epoch 00013: val_loss did not improve from 1.27078\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 1.2022 - acc: 0.4551 - val_loss: 1.3434 - val_acc: 0.3522\n",
      "Epoch 14/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1779 - acc: 0.4657\n",
      "Epoch 00014: val_loss improved from 1.27078 to 1.26032, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 1.1803 - acc: 0.4657 - val_loss: 1.2603 - val_acc: 0.4113\n",
      "Epoch 15/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1589 - acc: 0.4862\n",
      "Epoch 00015: val_loss did not improve from 1.26032\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 1.1560 - acc: 0.4876 - val_loss: 1.2629 - val_acc: 0.4208\n",
      "Epoch 16/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1479 - acc: 0.4916\n",
      "Epoch 00016: val_loss did not improve from 1.26032\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 1.1482 - acc: 0.4917 - val_loss: 1.3005 - val_acc: 0.3783\n",
      "Epoch 17/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1287 - acc: 0.5066\n",
      "Epoch 00017: val_loss did not improve from 1.26032\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 1.1307 - acc: 0.5071 - val_loss: 1.2959 - val_acc: 0.3948\n",
      "Epoch 18/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0679 - acc: 0.5391\n",
      "Epoch 00018: val_loss did not improve from 1.26032\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 1.0694 - acc: 0.5390 - val_loss: 1.3664 - val_acc: 0.3215\n",
      "Epoch 19/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0904 - acc: 0.5222\n",
      "Epoch 00019: val_loss improved from 1.26032 to 1.20079, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 14s 8ms/sample - loss: 1.0916 - acc: 0.5201 - val_loss: 1.2008 - val_acc: 0.4917\n",
      "Epoch 20/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0377 - acc: 0.5649\n",
      "Epoch 00020: val_loss did not improve from 1.20079\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 1.0410 - acc: 0.5650 - val_loss: 1.2045 - val_acc: 0.4704\n",
      "Epoch 21/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9982 - acc: 0.5974\n",
      "Epoch 00021: val_loss improved from 1.20079 to 1.17318, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.9968 - acc: 0.5969 - val_loss: 1.1732 - val_acc: 0.5154\n",
      "Epoch 22/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9741 - acc: 0.5974\n",
      "Epoch 00022: val_loss improved from 1.17318 to 1.17200, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.9729 - acc: 0.5981 - val_loss: 1.1720 - val_acc: 0.5130\n",
      "Epoch 23/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9947 - acc: 0.5901\n",
      "Epoch 00023: val_loss did not improve from 1.17200\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.9945 - acc: 0.5904 - val_loss: 1.1858 - val_acc: 0.5154\n",
      "Epoch 24/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9269 - acc: 0.6112\n",
      "Epoch 00024: val_loss did not improve from 1.17200\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.9272 - acc: 0.6111 - val_loss: 1.1796 - val_acc: 0.4775\n",
      "Epoch 25/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8994 - acc: 0.6322\n",
      "Epoch 00025: val_loss did not improve from 1.17200\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.9011 - acc: 0.6312 - val_loss: 1.1808 - val_acc: 0.4799\n",
      "Epoch 26/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8657 - acc: 0.6514\n",
      "Epoch 00026: val_loss did not improve from 1.17200\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.8710 - acc: 0.6472 - val_loss: 1.2036 - val_acc: 0.4657\n",
      "Epoch 27/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8453 - acc: 0.6448\n",
      "Epoch 00027: val_loss improved from 1.17200 to 1.13641, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.8486 - acc: 0.6436 - val_loss: 1.1364 - val_acc: 0.5461\n",
      "Epoch 28/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8479 - acc: 0.6611\n",
      "Epoch 00028: val_loss did not improve from 1.13641\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.8466 - acc: 0.6613 - val_loss: 1.2160 - val_acc: 0.4941\n",
      "Epoch 29/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8337 - acc: 0.6599\n",
      "Epoch 00029: val_loss improved from 1.13641 to 1.09754, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.8333 - acc: 0.6590 - val_loss: 1.0975 - val_acc: 0.5697\n",
      "Epoch 30/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7637 - acc: 0.7001\n",
      "Epoch 00030: val_loss did not improve from 1.09754\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.7655 - acc: 0.6998 - val_loss: 1.1767 - val_acc: 0.5272\n",
      "Epoch 31/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7536 - acc: 0.7079\n",
      "Epoch 00031: val_loss did not improve from 1.09754\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.7512 - acc: 0.7086 - val_loss: 1.1288 - val_acc: 0.5532\n",
      "Epoch 32/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7243 - acc: 0.7067\n",
      "Epoch 00032: val_loss did not improve from 1.09754\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.7313 - acc: 0.7057 - val_loss: 1.2326 - val_acc: 0.5272\n",
      "Epoch 33/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6980 - acc: 0.7236\n",
      "Epoch 00033: val_loss did not improve from 1.09754\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.6955 - acc: 0.7240 - val_loss: 1.2018 - val_acc: 0.5414\n",
      "Epoch 34/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6374 - acc: 0.7536\n",
      "Epoch 00034: val_loss did not improve from 1.09754\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.6393 - acc: 0.7547 - val_loss: 1.1043 - val_acc: 0.5768\n",
      "Epoch 35/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6724 - acc: 0.7374\n",
      "Epoch 00035: val_loss did not improve from 1.09754\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.6727 - acc: 0.7364 - val_loss: 1.1295 - val_acc: 0.5508\n",
      "Epoch 36/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6346 - acc: 0.7614\n",
      "Epoch 00036: val_loss did not improve from 1.09754\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.6385 - acc: 0.7595 - val_loss: 1.2569 - val_acc: 0.5390\n",
      "Epoch 37/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5949 - acc: 0.7620\n",
      "Epoch 00037: val_loss did not improve from 1.09754\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.5898 - acc: 0.7636 - val_loss: 1.2035 - val_acc: 0.5225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5815 - acc: 0.7752\n",
      "Epoch 00038: val_loss did not improve from 1.09754\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.5824 - acc: 0.7742 - val_loss: 1.2311 - val_acc: 0.5508\n",
      "Epoch 39/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6060 - acc: 0.7698\n",
      "Epoch 00039: val_loss did not improve from 1.09754\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.6056 - acc: 0.7689 - val_loss: 1.1718 - val_acc: 0.5626\n",
      "Epoch 40/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5043 - acc: 0.8053\n",
      "Epoch 00040: val_loss did not improve from 1.09754\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.5124 - acc: 0.8020 - val_loss: 1.2479 - val_acc: 0.5697\n",
      "Epoch 41/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5031 - acc: 0.8023\n",
      "Epoch 00041: val_loss did not improve from 1.09754\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.5080 - acc: 0.8008 - val_loss: 1.5297 - val_acc: 0.5012\n",
      "Epoch 42/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5016 - acc: 0.8161\n",
      "Epoch 00042: val_loss improved from 1.09754 to 1.08319, saving model to ./model_checkpoints/augmented_deep_model_800\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_800\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 0.5144 - acc: 0.8109 - val_loss: 1.0832 - val_acc: 0.6099\n",
      "Epoch 43/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4769 - acc: 0.8077\n",
      "Epoch 00043: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.4806 - acc: 0.8079 - val_loss: 1.0930 - val_acc: 0.6005\n",
      "Epoch 44/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4633 - acc: 0.8119\n",
      "Epoch 00044: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.4585 - acc: 0.8144 - val_loss: 1.1343 - val_acc: 0.5863\n",
      "Epoch 45/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4277 - acc: 0.8365\n",
      "Epoch 00045: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.4368 - acc: 0.8327 - val_loss: 1.2837 - val_acc: 0.5579\n",
      "Epoch 46/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4400 - acc: 0.8383\n",
      "Epoch 00046: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.4411 - acc: 0.8381 - val_loss: 1.3729 - val_acc: 0.5626\n",
      "Epoch 47/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3977 - acc: 0.8660\n",
      "Epoch 00047: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.4014 - acc: 0.8641 - val_loss: 1.5686 - val_acc: 0.5272\n",
      "Epoch 48/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4464 - acc: 0.8341\n",
      "Epoch 00048: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.4477 - acc: 0.8339 - val_loss: 1.1321 - val_acc: 0.6170\n",
      "Epoch 49/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3740 - acc: 0.8552\n",
      "Epoch 00049: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.3703 - acc: 0.8570 - val_loss: 1.1050 - val_acc: 0.6194\n",
      "Epoch 50/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8413\n",
      "Epoch 00050: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.4076 - acc: 0.8416 - val_loss: 1.1365 - val_acc: 0.6194\n",
      "Epoch 51/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3116 - acc: 0.8906\n",
      "Epoch 00051: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.3079 - acc: 0.8924 - val_loss: 1.2076 - val_acc: 0.5957\n",
      "Epoch 52/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3562 - acc: 0.8636\n",
      "Epoch 00052: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.3575 - acc: 0.8629 - val_loss: 1.4194 - val_acc: 0.5721\n",
      "Epoch 53/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3613 - acc: 0.8726\n",
      "Epoch 00053: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.3615 - acc: 0.8729 - val_loss: 1.4945 - val_acc: 0.5414\n",
      "Epoch 54/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.8792\n",
      "Epoch 00054: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.3341 - acc: 0.8788 - val_loss: 1.3468 - val_acc: 0.5674\n",
      "Epoch 55/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3598 - acc: 0.8576\n",
      "Epoch 00055: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.3602 - acc: 0.8582 - val_loss: 1.3201 - val_acc: 0.5887\n",
      "Epoch 56/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3043 - acc: 0.8792\n",
      "Epoch 00056: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.3071 - acc: 0.8788 - val_loss: 1.1534 - val_acc: 0.5957\n",
      "Epoch 57/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.9050\n",
      "Epoch 00057: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.2638 - acc: 0.9054 - val_loss: 1.8221 - val_acc: 0.5414\n",
      "Epoch 58/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.8798\n",
      "Epoch 00058: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 0.3182 - acc: 0.8800 - val_loss: 1.4430 - val_acc: 0.5650\n",
      "Epoch 59/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9008\n",
      "Epoch 00059: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 0.2677 - acc: 0.9001 - val_loss: 1.5902 - val_acc: 0.5508\n",
      "Epoch 60/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2624 - acc: 0.9099\n",
      "Epoch 00060: val_loss did not improve from 1.08319\n",
      "1692/1692 [==============================] - 13s 7ms/sample - loss: 0.2679 - acc: 0.9078 - val_loss: 1.3135 - val_acc: 0.5957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x140e42392b0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIME_WINDOW = 800\n",
    "TIME_STRIDE = 1000\n",
    "\n",
    "# cut the slices\n",
    "X_train_slices, y_train_slices = sliding_window(X_train, \n",
    "                                                y_train, \n",
    "                                                time_window=TIME_WINDOW,  \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "X_valid_slices, y_valid_slices = sliding_window(X_valid, \n",
    "                                                y_valid, \n",
    "                                                time_window=TIME_WINDOW, \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "print(\"Training data shape with slices: {}\".format(X_train_slices.shape))\n",
    "print(\"Training label shape with slice: {}\".format(y_train_slices.shape))\n",
    "print(\"Validation data shape with slices: {}\".format(X_valid_slices.shape))\n",
    "print(\"Validation label shape with slice: {}\".format(y_valid_slices.shape))\n",
    "\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/augmented_deep_model_' + str(TIME_WINDOW),\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "deep_aug_model_800 = construct_augmented_deep_model(TIME_WINDOW)\n",
    "deep_aug_model_800.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "deep_aug_model_800.summary()\n",
    "\n",
    "deep_aug_model_800.fit(X_train_slices, y_train_slices,\n",
    "                       validation_data = (X_valid_slices, y_valid_slices),\n",
    "                       epochs = 60,\n",
    "                       callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape with slices: (1692, 22, 700)\n",
      "Training label shape with slice: (1692,)\n",
      "Validation data shape with slices: (423, 22, 700)\n",
      "Validation label shape with slice: (423,)\n",
      "Model: \"deep_aug_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 22, 700)]         0         \n",
      "_________________________________________________________________\n",
      "reshape_48 (Reshape)         (None, 22, 700, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 22, 691, 25)       275       \n",
      "_________________________________________________________________\n",
      "permute_8 (Permute)          (None, 691, 25, 22)       0         \n",
      "_________________________________________________________________\n",
      "reshape_49 (Reshape)         (None, 691, 550, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 691, 1, 25)        13775     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 691, 1, 25)        2764      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 691, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "reshape_50 (Reshape)         (None, 691, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 230, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 221, 1, 50)        12550     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 221, 1, 50)        884       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 221, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 221, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "reshape_51 (Reshape)         (None, 221, 50, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 73, 50, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 64, 1, 100)        50100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 64, 1, 100)        256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 64, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 64, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "reshape_52 (Reshape)         (None, 64, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 21, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 12, 1, 200)        200200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 12, 1, 200)        48        \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 12, 1, 200)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 12, 1, 200)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 9604      \n",
      "=================================================================\n",
      "Total params: 290,456\n",
      "Trainable params: 288,480\n",
      "Non-trainable params: 1,976\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 2.0171 - acc: 0.2746\n",
      "Epoch 00001: val_loss improved from inf to 1.93286, saving model to ./model_checkpoints/augmented_deep_model_700\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_700\\assets\n",
      "1692/1692 [==============================] - 13s 8ms/sample - loss: 2.0124 - acc: 0.2760 - val_loss: 1.9329 - val_acc: 0.2388\n",
      "Epoch 2/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5738 - acc: 0.2782- E\n",
      "Epoch 00002: val_loss improved from 1.93286 to 1.41419, saving model to ./model_checkpoints/augmented_deep_model_700\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_700\\assets\n",
      "1692/1692 [==============================] - 13s 7ms/sample - loss: 1.5764 - acc: 0.2778 - val_loss: 1.4142 - val_acc: 0.2955\n",
      "Epoch 3/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4732 - acc: 0.3119\n",
      "Epoch 00003: val_loss improved from 1.41419 to 1.33678, saving model to ./model_checkpoints/augmented_deep_model_700\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_700\\assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 1.4730 - acc: 0.3126 - val_loss: 1.3368 - val_acc: 0.3522\n",
      "Epoch 4/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3745 - acc: 0.3588\n",
      "Epoch 00004: val_loss improved from 1.33678 to 1.28265, saving model to ./model_checkpoints/augmented_deep_model_700\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_700\\assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 1.3791 - acc: 0.3576 - val_loss: 1.2826 - val_acc: 0.3830\n",
      "Epoch 5/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3582 - acc: 0.3828- ETA: 2s - loss: 1.3427 \n",
      "Epoch 00005: val_loss did not improve from 1.28265\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.3566 - acc: 0.3830 - val_loss: 1.4217 - val_acc: 0.3381\n",
      "Epoch 6/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3216 - acc: 0.4014\n",
      "Epoch 00006: val_loss did not improve from 1.28265\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.3186 - acc: 0.4031 - val_loss: 1.3076 - val_acc: 0.3593\n",
      "Epoch 7/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2923 - acc: 0.4056\n",
      "Epoch 00007: val_loss improved from 1.28265 to 1.27585, saving model to ./model_checkpoints/augmented_deep_model_700\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_700\\assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 1.2947 - acc: 0.4037 - val_loss: 1.2759 - val_acc: 0.4019\n",
      "Epoch 8/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2536 - acc: 0.4363- ETA: 0s - loss: 1.2506 - acc: 0.\n",
      "Epoch 00008: val_loss did not improve from 1.27585\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.2496 - acc: 0.4397 - val_loss: 1.2872 - val_acc: 0.4113\n",
      "Epoch 9/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2375 - acc: 0.4237- ETA: 3s - loss: 1.2\n",
      "Epoch 00009: val_loss did not improve from 1.27585\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.2346 - acc: 0.4261 - val_loss: 1.3258 - val_acc: 0.3522\n",
      "Epoch 10/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2142 - acc: 0.4351\n",
      "Epoch 00010: val_loss did not improve from 1.27585\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.2122 - acc: 0.4374 - val_loss: 1.3251 - val_acc: 0.3593\n",
      "Epoch 11/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2106 - acc: 0.4579\n",
      "Epoch 00011: val_loss did not improve from 1.27585\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.2143 - acc: 0.4557 - val_loss: 1.3440 - val_acc: 0.3522\n",
      "Epoch 12/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1948 - acc: 0.4459\n",
      "Epoch 00012: val_loss did not improve from 1.27585\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.1980 - acc: 0.4427 - val_loss: 1.2947 - val_acc: 0.3901\n",
      "Epoch 13/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1759 - acc: 0.4760\n",
      "Epoch 00013: val_loss did not improve from 1.27585\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.1832 - acc: 0.4734 - val_loss: 1.3384 - val_acc: 0.3452\n",
      "Epoch 14/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1651 - acc: 0.4651\n",
      "Epoch 00014: val_loss did not improve from 1.27585\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.1612 - acc: 0.4681 - val_loss: 1.3645 - val_acc: 0.2979\n",
      "Epoch 15/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1489 - acc: 0.4844\n",
      "Epoch 00015: val_loss did not improve from 1.27585\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.1479 - acc: 0.4870 - val_loss: 1.3161 - val_acc: 0.3664\n",
      "Epoch 16/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1320 - acc: 0.4850\n",
      "Epoch 00016: val_loss did not improve from 1.27585\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.1346 - acc: 0.4852 - val_loss: 1.2925 - val_acc: 0.3593\n",
      "Epoch 17/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0986 - acc: 0.5024\n",
      "Epoch 00017: val_loss improved from 1.27585 to 1.19364, saving model to ./model_checkpoints/augmented_deep_model_700\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_700\\assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 1.1032 - acc: 0.4976 - val_loss: 1.1936 - val_acc: 0.4610\n",
      "Epoch 18/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1108 - acc: 0.5036\n",
      "Epoch 00018: val_loss did not improve from 1.19364\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.1066 - acc: 0.5053 - val_loss: 1.2284 - val_acc: 0.4043\n",
      "Epoch 19/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0718 - acc: 0.5325- ETA: 1s - loss: 1.0716 - acc: \n",
      "Epoch 00019: val_loss did not improve from 1.19364\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.0707 - acc: 0.5337 - val_loss: 1.2438 - val_acc: 0.4184\n",
      "Epoch 20/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0447 - acc: 0.544 - ETA: 0s - loss: 1.0444 - acc: 0.5457\n",
      "Epoch 00020: val_loss did not improve from 1.19364\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.0390 - acc: 0.5491 - val_loss: 1.1978 - val_acc: 0.4232\n",
      "Epoch 21/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0519 - acc: 0.5367\n",
      "Epoch 00021: val_loss did not improve from 1.19364\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.0521 - acc: 0.5366 - val_loss: 1.2597 - val_acc: 0.4043\n",
      "Epoch 22/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0152 - acc: 0.5643\n",
      "Epoch 00022: val_loss improved from 1.19364 to 1.17626, saving model to ./model_checkpoints/augmented_deep_model_700\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_700\\assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 1.0153 - acc: 0.5638 - val_loss: 1.1763 - val_acc: 0.4303\n",
      "Epoch 23/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9842 - acc: 0.5745- E\n",
      "Epoch 00023: val_loss improved from 1.17626 to 1.16523, saving model to ./model_checkpoints/augmented_deep_model_700\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_700\\assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 0.9852 - acc: 0.5727 - val_loss: 1.1652 - val_acc: 0.4775\n",
      "Epoch 24/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9730 - acc: 0.5980- ETA: 5s -\n",
      "Epoch 00024: val_loss improved from 1.16523 to 1.15427, saving model to ./model_checkpoints/augmented_deep_model_700\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_700\\assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 0.9743 - acc: 0.5975 - val_loss: 1.1543 - val_acc: 0.4634\n",
      "Epoch 25/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9517 - acc: 0.5962\n",
      "Epoch 00025: val_loss improved from 1.15427 to 1.11443, saving model to ./model_checkpoints/augmented_deep_model_700\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_700\\assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 0.9489 - acc: 0.5957 - val_loss: 1.1144 - val_acc: 0.5177\n",
      "Epoch 26/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9121 - acc: 0.6112- ETA: 8s - loss: 0.8363 - a - \n",
      "Epoch 00026: val_loss did not improve from 1.11443\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.9102 - acc: 0.6123 - val_loss: 1.1156 - val_acc: 0.5414\n",
      "Epoch 27/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9471 - acc: 0.6238\n",
      "Epoch 00027: val_loss improved from 1.11443 to 1.08979, saving model to ./model_checkpoints/augmented_deep_model_700\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_700\\assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 0.9453 - acc: 0.6241 - val_loss: 1.0898 - val_acc: 0.5319\n",
      "Epoch 28/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8837 - acc: 0.6424\n",
      "Epoch 00028: val_loss did not improve from 1.08979\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.8828 - acc: 0.6418 - val_loss: 1.1610 - val_acc: 0.4846\n",
      "Epoch 29/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8417 - acc: 0.6635\n",
      "Epoch 00029: val_loss did not improve from 1.08979\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.8439 - acc: 0.6619 - val_loss: 1.0991 - val_acc: 0.5485\n",
      "Epoch 30/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8310 - acc: 0.6562\n",
      "Epoch 00030: val_loss did not improve from 1.08979\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.8323 - acc: 0.6560 - val_loss: 1.0943 - val_acc: 0.5437\n",
      "Epoch 31/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7853 - acc: 0.6767\n",
      "Epoch 00031: val_loss improved from 1.08979 to 1.04448, saving model to ./model_checkpoints/augmented_deep_model_700\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_700\\assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 0.7882 - acc: 0.6755 - val_loss: 1.0445 - val_acc: 0.5721\n",
      "Epoch 32/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7490 - acc: 0.7091- ETA: 4s - lo\n",
      "Epoch 00032: val_loss did not improve from 1.04448\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.7469 - acc: 0.7098 - val_loss: 1.0793 - val_acc: 0.5721\n",
      "Epoch 33/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6984 - acc: 0.7206\n",
      "Epoch 00033: val_loss did not improve from 1.04448\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.7005 - acc: 0.7199 - val_loss: 1.0451 - val_acc: 0.5721\n",
      "Epoch 34/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7003 - acc: 0.7260\n",
      "Epoch 00034: val_loss did not improve from 1.04448\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.7005 - acc: 0.7258 - val_loss: 1.1046 - val_acc: 0.5579\n",
      "Epoch 35/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6826 - acc: 0.7332\n",
      "Epoch 00035: val_loss did not improve from 1.04448\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.6892 - acc: 0.7305 - val_loss: 1.0604 - val_acc: 0.5674\n",
      "Epoch 36/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6268 - acc: 0.7476\n",
      "Epoch 00036: val_loss did not improve from 1.04448\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.6277 - acc: 0.7465 - val_loss: 1.0527 - val_acc: 0.5626\n",
      "Epoch 37/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6179 - acc: 0.7482\n",
      "Epoch 00037: val_loss did not improve from 1.04448\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.6214 - acc: 0.7476 - val_loss: 1.0668 - val_acc: 0.5674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6128 - acc: 0.7602\n",
      "Epoch 00038: val_loss did not improve from 1.04448\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.6185 - acc: 0.7571 - val_loss: 1.1109 - val_acc: 0.5508\n",
      "Epoch 39/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5904 - acc: 0.7734\n",
      "Epoch 00039: val_loss did not improve from 1.04448\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.5873 - acc: 0.7742 - val_loss: 1.1586 - val_acc: 0.5366\n",
      "Epoch 40/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5344 - acc: 0.7975\n",
      "Epoch 00040: val_loss did not improve from 1.04448\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.5348 - acc: 0.7973 - val_loss: 1.0581 - val_acc: 0.5839\n",
      "Epoch 41/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5315 - acc: 0.7837- ETA: 4s - loss\n",
      "Epoch 00041: val_loss did not improve from 1.04448\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.5387 - acc: 0.7807 - val_loss: 1.0866 - val_acc: 0.5650\n",
      "Epoch 42/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5032 - acc: 0.8173- ETA: 3s - loss:\n",
      "Epoch 00042: val_loss did not improve from 1.04448\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.5099 - acc: 0.8132 - val_loss: 1.1254 - val_acc: 0.5816\n",
      "Epoch 43/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4937 - acc: 0.8131- ETA: 0s - loss: 0.4945 - acc: 0.8\n",
      "Epoch 00043: val_loss did not improve from 1.04448\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.4945 - acc: 0.8115 - val_loss: 1.3045 - val_acc: 0.5390\n",
      "Epoch 44/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4864 - acc: 0.8179\n",
      "Epoch 00044: val_loss did not improve from 1.04448\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.4895 - acc: 0.8156 - val_loss: 1.0457 - val_acc: 0.6170\n",
      "Epoch 45/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4703 - acc: 0.8185\n",
      "Epoch 00045: val_loss improved from 1.04448 to 1.02912, saving model to ./model_checkpoints/augmented_deep_model_700\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_700\\assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 0.4729 - acc: 0.8174 - val_loss: 1.0291 - val_acc: 0.6147\n",
      "Epoch 46/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8317\n",
      "Epoch 00046: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.4196 - acc: 0.8316 - val_loss: 1.1035 - val_acc: 0.5768\n",
      "Epoch 47/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4305 - acc: 0.8359\n",
      "Epoch 00047: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.4307 - acc: 0.8357 - val_loss: 1.1658 - val_acc: 0.5579\n",
      "Epoch 48/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4029 - acc: 0.8456- ETA: 6s - loss - ETA: 2s - loss: 0.402\n",
      "Epoch 00048: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.3997 - acc: 0.8463 - val_loss: 1.0888 - val_acc: 0.6265\n",
      "Epoch 49/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3631 - acc: 0.8648\n",
      "Epoch 00049: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.3637 - acc: 0.8629 - val_loss: 1.1112 - val_acc: 0.6076\n",
      "Epoch 50/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3395 - acc: 0.8690\n",
      "Epoch 00050: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.3428 - acc: 0.8682 - val_loss: 1.0942 - val_acc: 0.6147\n",
      "Epoch 51/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8660- ETA: 2s - loss: 0.3607\n",
      "Epoch 00051: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.3713 - acc: 0.8652 - val_loss: 1.1526 - val_acc: 0.6123\n",
      "Epoch 52/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3830 - acc: 0.8552\n",
      "Epoch 00052: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.3805 - acc: 0.8552 - val_loss: 1.2774 - val_acc: 0.5603\n",
      "Epoch 53/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8606\n",
      "Epoch 00053: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.3571 - acc: 0.8611 - val_loss: 1.0807 - val_acc: 0.6147\n",
      "Epoch 54/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3144 - acc: 0.8762- ETA: 4s - loss: 0.3144 - ac - ETA: 3s - loss: 0.\n",
      "Epoch 00054: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.3155 - acc: 0.8753 - val_loss: 1.2126 - val_acc: 0.5792\n",
      "Epoch 55/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8768-\n",
      "Epoch 00055: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.3219 - acc: 0.8771 - val_loss: 1.1545 - val_acc: 0.6123\n",
      "Epoch 56/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.8900\n",
      "Epoch 00056: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.2707 - acc: 0.8901 - val_loss: 1.1461 - val_acc: 0.6147\n",
      "Epoch 57/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3543 - acc: 0.8720\n",
      "Epoch 00057: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.3553 - acc: 0.8706 - val_loss: 1.3406 - val_acc: 0.5792\n",
      "Epoch 58/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2598 - acc: 0.9002- ETA:  - ETA: 3s - loss: 0.26 - ETA: 0s - loss: 0.2601 - acc: 0.90\n",
      "Epoch 00058: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.2591 - acc: 0.9007 - val_loss: 1.1486 - val_acc: 0.6028\n",
      "Epoch 59/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9189\n",
      "Epoch 00059: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.2077 - acc: 0.9196 - val_loss: 1.2031 - val_acc: 0.6076\n",
      "Epoch 60/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2428 - acc: 0.9002\n",
      "Epoch 00060: val_loss did not improve from 1.02912\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.2502 - acc: 0.8983 - val_loss: 1.2662 - val_acc: 0.6076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x140e8d5d908>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIME_WINDOW = 700\n",
    "TIME_STRIDE = 1000\n",
    "\n",
    "# cut the slices\n",
    "X_train_slices, y_train_slices = sliding_window(X_train, \n",
    "                                                y_train, \n",
    "                                                time_window=TIME_WINDOW,  \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "X_valid_slices, y_valid_slices = sliding_window(X_valid, \n",
    "                                                y_valid, \n",
    "                                                time_window=TIME_WINDOW, \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "print(\"Training data shape with slices: {}\".format(X_train_slices.shape))\n",
    "print(\"Training label shape with slice: {}\".format(y_train_slices.shape))\n",
    "print(\"Validation data shape with slices: {}\".format(X_valid_slices.shape))\n",
    "print(\"Validation label shape with slice: {}\".format(y_valid_slices.shape))\n",
    "\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/augmented_deep_model_' + str(TIME_WINDOW),\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "deep_aug_model_700 = construct_augmented_deep_model(TIME_WINDOW)\n",
    "deep_aug_model_700.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "deep_aug_model_700.summary()\n",
    "\n",
    "deep_aug_model_700.fit(X_train_slices, y_train_slices,\n",
    "                       validation_data = (X_valid_slices, y_valid_slices),\n",
    "                       epochs = 60,\n",
    "                       callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape with slices: (1692, 22, 600)\n",
      "Training label shape with slice: (1692,)\n",
      "Validation data shape with slices: (423, 22, 600)\n",
      "Validation label shape with slice: (423,)\n",
      "Model: \"deep_aug_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 22, 600)]         0         \n",
      "_________________________________________________________________\n",
      "reshape_54 (Reshape)         (None, 22, 600, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 22, 591, 25)       275       \n",
      "_________________________________________________________________\n",
      "permute_9 (Permute)          (None, 591, 25, 22)       0         \n",
      "_________________________________________________________________\n",
      "reshape_55 (Reshape)         (None, 591, 550, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 591, 1, 25)        13775     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 591, 1, 25)        2364      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 591, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "reshape_56 (Reshape)         (None, 591, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 197, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 188, 1, 50)        12550     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 188, 1, 50)        752       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 188, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 188, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "reshape_57 (Reshape)         (None, 188, 50, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 62, 50, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 53, 1, 100)        50100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 53, 1, 100)        212       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 53, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 53, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "reshape_58 (Reshape)         (None, 53, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 17, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 8, 1, 200)         200200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 8, 1, 200)         32        \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 8, 1, 200)         0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 8, 1, 200)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 6404      \n",
      "=================================================================\n",
      "Total params: 286,664\n",
      "Trainable params: 284,984\n",
      "Non-trainable params: 1,680\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.8639 - acc: 0.2518\n",
      "Epoch 00001: val_loss improved from inf to 1.44898, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 12s 7ms/sample - loss: 1.8554 - acc: 0.2530 - val_loss: 1.4490 - val_acc: 0.2790\n",
      "Epoch 2/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5671 - acc: 0.2957\n",
      "Epoch 00002: val_loss improved from 1.44898 to 1.38383, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 11s 6ms/sample - loss: 1.5667 - acc: 0.2926 - val_loss: 1.3838 - val_acc: 0.2931\n",
      "Epoch 3/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4358 - acc: 0.3353- ETA: 0s - loss: 1.4339 - acc: 0.3\n",
      "Epoch 00003: val_loss improved from 1.38383 to 1.34647, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 1.4322 - acc: 0.3369 - val_loss: 1.3465 - val_acc: 0.3499\n",
      "Epoch 4/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3902 - acc: 0.3516\n",
      "Epoch 00004: val_loss improved from 1.34647 to 1.28086, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 11s 7ms/sample - loss: 1.3939 - acc: 0.3517 - val_loss: 1.2809 - val_acc: 0.3948\n",
      "Epoch 5/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3497 - acc: 0.3858\n",
      "Epoch 00005: val_loss did not improve from 1.28086\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.3532 - acc: 0.3830 - val_loss: 1.3538 - val_acc: 0.3026\n",
      "Epoch 6/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3002 - acc: 0.4026\n",
      "Epoch 00006: val_loss did not improve from 1.28086\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.2977 - acc: 0.4037 - val_loss: 1.3215 - val_acc: 0.3381\n",
      "Epoch 7/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2731 - acc: 0.4117\n",
      "Epoch 00007: val_loss improved from 1.28086 to 1.26904, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 11s 6ms/sample - loss: 1.2713 - acc: 0.4131 - val_loss: 1.2690 - val_acc: 0.4232\n",
      "Epoch 8/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2422 - acc: 0.4171\n",
      "Epoch 00008: val_loss did not improve from 1.26904\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.2444 - acc: 0.4149 - val_loss: 1.2866 - val_acc: 0.3688\n",
      "Epoch 9/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2214 - acc: 0.4513\n",
      "Epoch 00009: val_loss did not improve from 1.26904\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.2206 - acc: 0.4509 - val_loss: 1.2787 - val_acc: 0.3901\n",
      "Epoch 10/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2088 - acc: 0.4489\n",
      "Epoch 00010: val_loss did not improve from 1.26904\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.2088 - acc: 0.4486 - val_loss: 1.2838 - val_acc: 0.3570\n",
      "Epoch 11/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1947 - acc: 0.4742\n",
      "Epoch 00011: val_loss did not improve from 1.26904\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.1965 - acc: 0.4734 - val_loss: 1.2961 - val_acc: 0.3759\n",
      "Epoch 12/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1879 - acc: 0.4579\n",
      "Epoch 00012: val_loss improved from 1.26904 to 1.25668, saving model to ./model_checkpoints/augmented_deep_model_600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 11s 6ms/sample - loss: 1.1881 - acc: 0.4569 - val_loss: 1.2567 - val_acc: 0.4161\n",
      "Epoch 13/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1823 - acc: 0.4718\n",
      "Epoch 00013: val_loss improved from 1.25668 to 1.24084, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 11s 6ms/sample - loss: 1.1824 - acc: 0.4710 - val_loss: 1.2408 - val_acc: 0.4184\n",
      "Epoch 14/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1660 - acc: 0.4802\n",
      "Epoch 00014: val_loss did not improve from 1.24084\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.1638 - acc: 0.4811 - val_loss: 1.3025 - val_acc: 0.3641\n",
      "Epoch 15/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1350 - acc: 0.4898\n",
      "Epoch 00015: val_loss did not improve from 1.24084\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.1354 - acc: 0.4894 - val_loss: 1.2640 - val_acc: 0.4043\n",
      "Epoch 16/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1213 - acc: 0.5048\n",
      "Epoch 00016: val_loss did not improve from 1.24084\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.1196 - acc: 0.5053 - val_loss: 1.2806 - val_acc: 0.3806\n",
      "Epoch 17/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1116 - acc: 0.5024\n",
      "Epoch 00017: val_loss improved from 1.24084 to 1.23983, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.1063 - acc: 0.5059 - val_loss: 1.2398 - val_acc: 0.4043\n",
      "Epoch 18/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0580 - acc: 0.5541\n",
      "Epoch 00018: val_loss improved from 1.23983 to 1.17623, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 11s 6ms/sample - loss: 1.0588 - acc: 0.5532 - val_loss: 1.1762 - val_acc: 0.4586\n",
      "Epoch 19/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0736 - acc: 0.5421\n",
      "Epoch 00019: val_loss did not improve from 1.17623\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.0709 - acc: 0.5431 - val_loss: 1.1773 - val_acc: 0.4752\n",
      "Epoch 20/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0410 - acc: 0.5583\n",
      "Epoch 00020: val_loss did not improve from 1.17623\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.0411 - acc: 0.5573 - val_loss: 1.1774 - val_acc: 0.4634\n",
      "Epoch 21/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0213 - acc: 0.5685\n",
      "Epoch 00021: val_loss improved from 1.17623 to 1.13224, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.0205 - acc: 0.5691 - val_loss: 1.1322 - val_acc: 0.5154\n",
      "Epoch 22/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9618 - acc: 0.6076\n",
      "Epoch 00022: val_loss did not improve from 1.13224\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.9633 - acc: 0.6058 - val_loss: 1.1682 - val_acc: 0.4752\n",
      "Epoch 23/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9515 - acc: 0.6124\n",
      "Epoch 00023: val_loss improved from 1.13224 to 1.10726, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 11s 6ms/sample - loss: 0.9521 - acc: 0.6117 - val_loss: 1.1073 - val_acc: 0.4799\n",
      "Epoch 24/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8872 - acc: 0.6406\n",
      "Epoch 00024: val_loss improved from 1.10726 to 1.07368, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.8844 - acc: 0.6418 - val_loss: 1.0737 - val_acc: 0.5296\n",
      "Epoch 25/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8793 - acc: 0.6316\n",
      "Epoch 00025: val_loss improved from 1.07368 to 1.04041, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 11s 6ms/sample - loss: 0.8755 - acc: 0.6330 - val_loss: 1.0404 - val_acc: 0.5437\n",
      "Epoch 26/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8803 - acc: 0.6520\n",
      "Epoch 00026: val_loss did not improve from 1.04041\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.8811 - acc: 0.6507 - val_loss: 1.0913 - val_acc: 0.4965\n",
      "Epoch 27/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8136 - acc: 0.6725\n",
      "Epoch 00027: val_loss did not improve from 1.04041\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.8140 - acc: 0.6738 - val_loss: 1.0422 - val_acc: 0.5626\n",
      "Epoch 28/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7645 - acc: 0.6881\n",
      "Epoch 00028: val_loss improved from 1.04041 to 1.01669, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.7617 - acc: 0.6891 - val_loss: 1.0167 - val_acc: 0.5414\n",
      "Epoch 29/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7498 - acc: 0.7025\n",
      "Epoch 00029: val_loss improved from 1.01669 to 1.00324, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 0.7556 - acc: 0.7021 - val_loss: 1.0032 - val_acc: 0.5508\n",
      "Epoch 30/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7558 - acc: 0.7007\n",
      "Epoch 00030: val_loss did not improve from 1.00324\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.7550 - acc: 0.7015 - val_loss: 1.1138 - val_acc: 0.5154\n",
      "Epoch 31/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7073 - acc: 0.7127\n",
      "Epoch 00031: val_loss did not improve from 1.00324\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.7024 - acc: 0.7134 - val_loss: 1.0739 - val_acc: 0.5603\n",
      "Epoch 32/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6201 - acc: 0.7632\n",
      "Epoch 00032: val_loss did not improve from 1.00324\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.6196 - acc: 0.7636 - val_loss: 1.0309 - val_acc: 0.5485\n",
      "Epoch 33/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6282 - acc: 0.7542\n",
      "Epoch 00033: val_loss did not improve from 1.00324\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.6274 - acc: 0.7541 - val_loss: 1.0496 - val_acc: 0.5603\n",
      "Epoch 34/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5916 - acc: 0.7716\n",
      "Epoch 00034: val_loss did not improve from 1.00324\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.5885 - acc: 0.7730 - val_loss: 1.0310 - val_acc: 0.6052\n",
      "Epoch 35/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6167 - acc: 0.7608\n",
      "Epoch 00035: val_loss improved from 1.00324 to 0.94194, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 11s 6ms/sample - loss: 0.6150 - acc: 0.7630 - val_loss: 0.9419 - val_acc: 0.6123\n",
      "Epoch 36/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5919 - acc: 0.7668\n",
      "Epoch 00036: val_loss did not improve from 0.94194\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.5923 - acc: 0.7665 - val_loss: 0.9524 - val_acc: 0.6099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5218 - acc: 0.7915\n",
      "Epoch 00037: val_loss did not improve from 0.94194\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.5247 - acc: 0.7908 - val_loss: 0.9720 - val_acc: 0.6028\n",
      "Epoch 38/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.7957\n",
      "Epoch 00038: val_loss did not improve from 0.94194\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.5381 - acc: 0.7955 - val_loss: 0.9791 - val_acc: 0.5863\n",
      "Epoch 39/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4645 - acc: 0.8299\n",
      "Epoch 00039: val_loss did not improve from 0.94194\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.4790 - acc: 0.8257 - val_loss: 1.0717 - val_acc: 0.5957\n",
      "Epoch 40/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4430 - acc: 0.8269\n",
      "Epoch 00040: val_loss did not improve from 0.94194\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.4409 - acc: 0.8268 - val_loss: 1.0434 - val_acc: 0.6170\n",
      "Epoch 41/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4308 - acc: 0.8311\n",
      "Epoch 00041: val_loss improved from 0.94194 to 0.93241, saving model to ./model_checkpoints/augmented_deep_model_600\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_600\\assets\n",
      "1692/1692 [==============================] - 11s 6ms/sample - loss: 0.4323 - acc: 0.8304 - val_loss: 0.9324 - val_acc: 0.6407\n",
      "Epoch 42/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4294 - acc: 0.8389\n",
      "Epoch 00042: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.4272 - acc: 0.8398 - val_loss: 1.0442 - val_acc: 0.6099\n",
      "Epoch 43/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3977 - acc: 0.8546\n",
      "Epoch 00043: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.3970 - acc: 0.8540 - val_loss: 1.2005 - val_acc: 0.5792\n",
      "Epoch 44/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8474\n",
      "Epoch 00044: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.4035 - acc: 0.8481 - val_loss: 1.0100 - val_acc: 0.6194\n",
      "Epoch 45/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3978 - acc: 0.8413\n",
      "Epoch 00045: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.4010 - acc: 0.8404 - val_loss: 0.9763 - val_acc: 0.6478\n",
      "Epoch 46/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3784 - acc: 0.8419\n",
      "Epoch 00046: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.3767 - acc: 0.8428 - val_loss: 0.9832 - val_acc: 0.6336\n",
      "Epoch 47/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.8912\n",
      "Epoch 00047: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.3046 - acc: 0.8918 - val_loss: 1.0073 - val_acc: 0.6407\n",
      "Epoch 48/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3831 - acc: 0.8450\n",
      "Epoch 00048: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.3811 - acc: 0.8457 - val_loss: 1.0850 - val_acc: 0.5981\n",
      "Epoch 49/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.8870\n",
      "Epoch 00049: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.3030 - acc: 0.8859 - val_loss: 1.0647 - val_acc: 0.6194\n",
      "Epoch 50/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8564\n",
      "Epoch 00050: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.3771 - acc: 0.8540 - val_loss: 1.0818 - val_acc: 0.5981\n",
      "Epoch 51/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.8858\n",
      "Epoch 00051: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.3033 - acc: 0.8848 - val_loss: 0.9902 - val_acc: 0.6430\n",
      "Epoch 52/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.8930\n",
      "Epoch 00052: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.3008 - acc: 0.8907 - val_loss: 1.1149 - val_acc: 0.5934\n",
      "Epoch 53/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.8798\n",
      "Epoch 00053: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.3183 - acc: 0.8806 - val_loss: 1.1187 - val_acc: 0.6383\n",
      "Epoch 54/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2826 - acc: 0.8996\n",
      "Epoch 00054: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.2808 - acc: 0.9001 - val_loss: 1.1411 - val_acc: 0.6005\n",
      "Epoch 55/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.8792\n",
      "Epoch 00055: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.3112 - acc: 0.8800 - val_loss: 1.1388 - val_acc: 0.6288\n",
      "Epoch 56/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9014\n",
      "Epoch 00056: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.2589 - acc: 0.9007 - val_loss: 1.1592 - val_acc: 0.6288\n",
      "Epoch 57/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9008\n",
      "Epoch 00057: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.2593 - acc: 0.9013 - val_loss: 1.1039 - val_acc: 0.6525\n",
      "Epoch 58/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9231\n",
      "Epoch 00058: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.2246 - acc: 0.9220 - val_loss: 1.0736 - val_acc: 0.6217\n",
      "Epoch 59/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2430 - acc: 0.9014\n",
      "Epoch 00059: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.2404 - acc: 0.9025 - val_loss: 1.0977 - val_acc: 0.6265\n",
      "Epoch 60/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9321\n",
      "Epoch 00060: val_loss did not improve from 0.93241\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.1843 - acc: 0.9326 - val_loss: 1.2460 - val_acc: 0.6241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x140e4d92080>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIME_WINDOW = 600\n",
    "TIME_STRIDE = 1000\n",
    "\n",
    "# cut the slices\n",
    "X_train_slices, y_train_slices = sliding_window(X_train, \n",
    "                                                y_train, \n",
    "                                                time_window=TIME_WINDOW,  \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "X_valid_slices, y_valid_slices = sliding_window(X_valid, \n",
    "                                                y_valid, \n",
    "                                                time_window=TIME_WINDOW, \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "print(\"Training data shape with slices: {}\".format(X_train_slices.shape))\n",
    "print(\"Training label shape with slice: {}\".format(y_train_slices.shape))\n",
    "print(\"Validation data shape with slices: {}\".format(X_valid_slices.shape))\n",
    "print(\"Validation label shape with slice: {}\".format(y_valid_slices.shape))\n",
    "\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/augmented_deep_model_' + str(TIME_WINDOW),\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "deep_aug_model_600 = construct_augmented_deep_model(TIME_WINDOW)\n",
    "deep_aug_model_600.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "deep_aug_model_600.summary()\n",
    "\n",
    "deep_aug_model_600.fit(X_train_slices, y_train_slices,\n",
    "                       validation_data = (X_valid_slices, y_valid_slices),\n",
    "                       epochs = 60,\n",
    "                       callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape with slices: (1692, 22, 500)\n",
      "Training label shape with slice: (1692,)\n",
      "Validation data shape with slices: (423, 22, 500)\n",
      "Validation label shape with slice: (423,)\n",
      "Model: \"deep_aug_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 22, 500)]         0         \n",
      "_________________________________________________________________\n",
      "reshape_60 (Reshape)         (None, 22, 500, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 22, 491, 25)       275       \n",
      "_________________________________________________________________\n",
      "permute_10 (Permute)         (None, 491, 25, 22)       0         \n",
      "_________________________________________________________________\n",
      "reshape_61 (Reshape)         (None, 491, 550, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 491, 1, 25)        13775     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 491, 1, 25)        1964      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 491, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "reshape_62 (Reshape)         (None, 491, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 163, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 154, 1, 50)        12550     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 154, 1, 50)        616       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 154, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 154, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "reshape_63 (Reshape)         (None, 154, 50, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 51, 50, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 42, 1, 100)        50100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 42, 1, 100)        168       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 42, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 42, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "reshape_64 (Reshape)         (None, 42, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 14, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 5, 1, 200)         200200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 5, 1, 200)         20        \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 5, 1, 200)         0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 5, 1, 200)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 4004      \n",
      "=================================================================\n",
      "Total params: 283,672\n",
      "Trainable params: 282,288\n",
      "Non-trainable params: 1,384\n",
      "_________________________________________________________________\n",
      "Train on 1692 samples, validate on 423 samples\n",
      "Epoch 1/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.9423 - acc: 0.2530\n",
      "Epoch 00001: val_loss improved from inf to 1.43796, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 10s 6ms/sample - loss: 1.9380 - acc: 0.2524 - val_loss: 1.4380 - val_acc: 0.2742\n",
      "Epoch 2/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.5387 - acc: 0.3107\n",
      "Epoch 00002: val_loss improved from 1.43796 to 1.35954, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.5385 - acc: 0.3115 - val_loss: 1.3595 - val_acc: 0.3333\n",
      "Epoch 3/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.4499 - acc: 0.3383\n",
      "Epoch 00003: val_loss improved from 1.35954 to 1.34670, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.4475 - acc: 0.3392 - val_loss: 1.3467 - val_acc: 0.3452\n",
      "Epoch 4/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.3666 - acc: 0.3720\n",
      "Epoch 00004: val_loss improved from 1.34670 to 1.29457, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.3658 - acc: 0.3735 - val_loss: 1.2946 - val_acc: 0.4113\n",
      "Epoch 5/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2900 - acc: 0.4243\n",
      "Epoch 00005: val_loss improved from 1.29457 to 1.25375, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.2939 - acc: 0.4208 - val_loss: 1.2538 - val_acc: 0.4326\n",
      "Epoch 6/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.2533 - acc: 0.4423\n",
      "Epoch 00006: val_loss improved from 1.25375 to 1.22228, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.2523 - acc: 0.4427 - val_loss: 1.2223 - val_acc: 0.4657\n",
      "Epoch 7/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1954 - acc: 0.4712\n",
      "Epoch 00007: val_loss improved from 1.22228 to 1.18683, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.1962 - acc: 0.4722 - val_loss: 1.1868 - val_acc: 0.5083\n",
      "Epoch 8/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1504 - acc: 0.4994\n",
      "Epoch 00008: val_loss did not improve from 1.18683\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 1.1513 - acc: 0.4976 - val_loss: 1.2041 - val_acc: 0.5012\n",
      "Epoch 9/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1256 - acc: 0.5264\n",
      "Epoch 00009: val_loss improved from 1.18683 to 1.17459, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 6ms/sample - loss: 1.1235 - acc: 0.5272 - val_loss: 1.1746 - val_acc: 0.5106\n",
      "Epoch 10/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.1066 - acc: 0.5373\n",
      "Epoch 00010: val_loss improved from 1.17459 to 1.14620, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 1.1048 - acc: 0.5390 - val_loss: 1.1462 - val_acc: 0.5366\n",
      "Epoch 11/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0258 - acc: 0.5577\n",
      "Epoch 00011: val_loss did not improve from 1.14620\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 1.0258 - acc: 0.5556 - val_loss: 1.1882 - val_acc: 0.4988\n",
      "Epoch 12/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 1.0254 - acc: 0.5703\n",
      "Epoch 00012: val_loss did not improve from 1.14620\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 1.0206 - acc: 0.5745 - val_loss: 1.1722 - val_acc: 0.4965\n",
      "Epoch 13/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9790 - acc: 0.6094\n",
      "Epoch 00013: val_loss improved from 1.14620 to 1.07920, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.9775 - acc: 0.6111 - val_loss: 1.0792 - val_acc: 0.5626\n",
      "Epoch 14/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9886 - acc: 0.5944\n",
      "Epoch 00014: val_loss did not improve from 1.07920\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.9867 - acc: 0.5957 - val_loss: 1.2496 - val_acc: 0.4184\n",
      "Epoch 15/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.9492 - acc: 0.6232\n",
      "Epoch 00015: val_loss improved from 1.07920 to 1.07766, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.9468 - acc: 0.6235 - val_loss: 1.0777 - val_acc: 0.5437\n",
      "Epoch 16/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8548 - acc: 0.6575\n",
      "Epoch 00016: val_loss did not improve from 1.07766\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.8615 - acc: 0.6548 - val_loss: 1.0794 - val_acc: 0.5603\n",
      "Epoch 17/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8423 - acc: 0.6701\n",
      "Epoch 00017: val_loss improved from 1.07766 to 1.01144, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.8434 - acc: 0.6684 - val_loss: 1.0114 - val_acc: 0.5887\n",
      "Epoch 18/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7975 - acc: 0.6959\n",
      "Epoch 00018: val_loss improved from 1.01144 to 1.00868, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.7987 - acc: 0.6956 - val_loss: 1.0087 - val_acc: 0.5863\n",
      "Epoch 19/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.8126 - acc: 0.6701\n",
      "Epoch 00019: val_loss did not improve from 1.00868\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.8137 - acc: 0.6702 - val_loss: 1.0822 - val_acc: 0.5674\n",
      "Epoch 20/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7608 - acc: 0.6983\n",
      "Epoch 00020: val_loss did not improve from 1.00868\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.7629 - acc: 0.6968 - val_loss: 1.0776 - val_acc: 0.5674\n",
      "Epoch 21/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.7269 - acc: 0.7151\n",
      "Epoch 00021: val_loss did not improve from 1.00868\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.7283 - acc: 0.7157 - val_loss: 1.0594 - val_acc: 0.5721\n",
      "Epoch 22/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6702 - acc: 0.7314\n",
      "Epoch 00022: val_loss improved from 1.00868 to 0.95685, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.6736 - acc: 0.7293 - val_loss: 0.9569 - val_acc: 0.6005\n",
      "Epoch 23/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6872 - acc: 0.7224\n",
      "Epoch 00023: val_loss did not improve from 0.95685\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.6868 - acc: 0.7228 - val_loss: 1.0418 - val_acc: 0.5532\n",
      "Epoch 24/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6632 - acc: 0.7344\n",
      "Epoch 00024: val_loss improved from 0.95685 to 0.95464, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.6626 - acc: 0.7346 - val_loss: 0.9546 - val_acc: 0.6383\n",
      "Epoch 25/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.6334 - acc: 0.7512\n",
      "Epoch 00025: val_loss did not improve from 0.95464\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.6327 - acc: 0.7524 - val_loss: 0.9898 - val_acc: 0.5934\n",
      "Epoch 26/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5947 - acc: 0.7602\n",
      "Epoch 00026: val_loss improved from 0.95464 to 0.93311, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.5947 - acc: 0.7600 - val_loss: 0.9331 - val_acc: 0.6241\n",
      "Epoch 27/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5641 - acc: 0.7710\n",
      "Epoch 00027: val_loss did not improve from 0.93311\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.5660 - acc: 0.7701 - val_loss: 0.9712 - val_acc: 0.6288\n",
      "Epoch 28/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5894 - acc: 0.7692\n",
      "Epoch 00028: val_loss did not improve from 0.93311\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.5900 - acc: 0.7683 - val_loss: 0.9780 - val_acc: 0.6194\n",
      "Epoch 29/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5066 - acc: 0.7969\n",
      "Epoch 00029: val_loss did not improve from 0.93311\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.5093 - acc: 0.7955 - val_loss: 0.9844 - val_acc: 0.6005\n",
      "Epoch 30/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.5462 - acc: 0.7831\n",
      "Epoch 00030: val_loss did not improve from 0.93311\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.5461 - acc: 0.7837 - val_loss: 0.9537 - val_acc: 0.6336\n",
      "Epoch 31/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4490 - acc: 0.8209\n",
      "Epoch 00031: val_loss improved from 0.93311 to 0.92171, saving model to ./model_checkpoints/augmented_deep_model_500\n",
      "INFO:tensorflow:Assets written to: ./model_checkpoints/augmented_deep_model_500\\assets\n",
      "1692/1692 [==============================] - 9s 5ms/sample - loss: 0.4470 - acc: 0.8215 - val_loss: 0.9217 - val_acc: 0.6265\n",
      "Epoch 32/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4994 - acc: 0.8095\n",
      "Epoch 00032: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.4982 - acc: 0.8097 - val_loss: 1.0073 - val_acc: 0.6123\n",
      "Epoch 33/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4574 - acc: 0.8167\n",
      "Epoch 00033: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.4591 - acc: 0.8150 - val_loss: 1.0974 - val_acc: 0.6099\n",
      "Epoch 34/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4349 - acc: 0.8317\n",
      "Epoch 00034: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.4323 - acc: 0.8333 - val_loss: 0.9792 - val_acc: 0.6478\n",
      "Epoch 35/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4378 - acc: 0.8233\n",
      "Epoch 00035: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.4382 - acc: 0.8233 - val_loss: 0.9789 - val_acc: 0.6170\n",
      "Epoch 36/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8510\n",
      "Epoch 00036: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.3877 - acc: 0.8487 - val_loss: 1.0371 - val_acc: 0.6123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3870 - acc: 0.8546\n",
      "Epoch 00037: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.3890 - acc: 0.8528 - val_loss: 0.9799 - val_acc: 0.6501\n",
      "Epoch 38/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3843 - acc: 0.8462\n",
      "Epoch 00038: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.3830 - acc: 0.8463 - val_loss: 1.2012 - val_acc: 0.5839\n",
      "Epoch 39/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.4513 - acc: 0.8359\n",
      "Epoch 00039: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.4479 - acc: 0.8369 - val_loss: 1.1083 - val_acc: 0.5887\n",
      "Epoch 40/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3450 - acc: 0.8660\n",
      "Epoch 00040: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.3495 - acc: 0.8635 - val_loss: 1.1979 - val_acc: 0.5839\n",
      "Epoch 41/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3717 - acc: 0.8492\n",
      "Epoch 00041: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.3716 - acc: 0.8487 - val_loss: 1.1583 - val_acc: 0.6028\n",
      "Epoch 42/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.8846\n",
      "Epoch 00042: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2992 - acc: 0.8848 - val_loss: 1.1933 - val_acc: 0.5816\n",
      "Epoch 43/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.8954\n",
      "Epoch 00043: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2759 - acc: 0.8948 - val_loss: 1.0643 - val_acc: 0.6099\n",
      "Epoch 44/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3097 - acc: 0.8966\n",
      "Epoch 00044: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.3106 - acc: 0.8960 - val_loss: 1.0747 - val_acc: 0.6312\n",
      "Epoch 45/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2723 - acc: 0.8930\n",
      "Epoch 00045: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2712 - acc: 0.8936 - val_loss: 1.1452 - val_acc: 0.6288\n",
      "Epoch 46/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.8816\n",
      "Epoch 00046: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2955 - acc: 0.8824 - val_loss: 1.1184 - val_acc: 0.6336\n",
      "Epoch 47/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.8906\n",
      "Epoch 00047: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2846 - acc: 0.8901 - val_loss: 1.0903 - val_acc: 0.6336\n",
      "Epoch 48/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.8882\n",
      "Epoch 00048: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2926 - acc: 0.8877 - val_loss: 1.3236 - val_acc: 0.5839\n",
      "Epoch 49/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.8900\n",
      "Epoch 00049: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2870 - acc: 0.8895 - val_loss: 1.0693 - val_acc: 0.6525\n",
      "Epoch 50/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.3084 - acc: 0.8894\n",
      "Epoch 00050: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.3113 - acc: 0.8883 - val_loss: 1.3980 - val_acc: 0.5556\n",
      "Epoch 51/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2625 - acc: 0.8942\n",
      "Epoch 00051: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2600 - acc: 0.8954 - val_loss: 1.0054 - val_acc: 0.6619\n",
      "Epoch 52/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9219\n",
      "Epoch 00052: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2023 - acc: 0.9220 - val_loss: 1.1649 - val_acc: 0.6265\n",
      "Epoch 53/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9087\n",
      "Epoch 00053: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2357 - acc: 0.9072 - val_loss: 1.3376 - val_acc: 0.6052\n",
      "Epoch 54/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9038\n",
      "Epoch 00054: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2525 - acc: 0.9048 - val_loss: 1.1482 - val_acc: 0.6548\n",
      "Epoch 55/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9050\n",
      "Epoch 00055: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2432 - acc: 0.9060 - val_loss: 1.3463 - val_acc: 0.6099\n",
      "Epoch 56/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.9044\n",
      "Epoch 00056: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2536 - acc: 0.9054 - val_loss: 1.2048 - val_acc: 0.6359\n",
      "Epoch 57/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9195\n",
      "Epoch 00057: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2123 - acc: 0.9196 - val_loss: 1.2290 - val_acc: 0.6028\n",
      "Epoch 58/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9123\n",
      "Epoch 00058: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2326 - acc: 0.9119 - val_loss: 1.3070 - val_acc: 0.6194\n",
      "Epoch 59/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9333\n",
      "Epoch 00059: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.1681 - acc: 0.9332 - val_loss: 1.0900 - val_acc: 0.6572\n",
      "Epoch 60/60\n",
      "1664/1692 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9147\n",
      "Epoch 00060: val_loss did not improve from 0.92171\n",
      "1692/1692 [==============================] - 7s 4ms/sample - loss: 0.2444 - acc: 0.9155 - val_loss: 1.4057 - val_acc: 0.5910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x140e8e9f0b8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIME_WINDOW = 500\n",
    "TIME_STRIDE = 1000\n",
    "\n",
    "# cut the slices\n",
    "X_train_slices, y_train_slices = sliding_window(X_train, \n",
    "                                                y_train, \n",
    "                                                time_window=TIME_WINDOW,  \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "X_valid_slices, y_valid_slices = sliding_window(X_valid, \n",
    "                                                y_valid, \n",
    "                                                time_window=TIME_WINDOW, \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "\n",
    "print(\"Training data shape with slices: {}\".format(X_train_slices.shape))\n",
    "print(\"Training label shape with slice: {}\".format(y_train_slices.shape))\n",
    "print(\"Validation data shape with slices: {}\".format(X_valid_slices.shape))\n",
    "print(\"Validation label shape with slice: {}\".format(y_valid_slices.shape))\n",
    "\n",
    "\n",
    "# save model with the best accuracy \n",
    "checkpoint_callback = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/augmented_deep_model_' + str(TIME_WINDOW),\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "deep_aug_model_500 = construct_augmented_deep_model(TIME_WINDOW)\n",
    "deep_aug_model_500.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "deep_aug_model_500.summary()\n",
    "\n",
    "deep_aug_model_500.fit(X_train_slices, y_train_slices,\n",
    "                       validation_data = (X_valid_slices, y_valid_slices),\n",
    "                       epochs = 60,\n",
    "                       callbacks=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423/423 [==============================] - 1s 2ms/sample - loss: 0.9217 - acc: 0.6265\n",
      "423/423 [==============================] - 1s 2ms/sample - loss: 0.9324 - acc: 0.6407\n",
      "423/423 [==============================] - 1s 2ms/sample - loss: 1.0291 - acc: 0.6147\n",
      "423/423 [==============================] - 1s 2ms/sample - loss: 1.0832 - acc: 0.6099\n",
      "423/423 [==============================] - 1s 2ms/sample - loss: 1.0418 - acc: 0.5981\n",
      "423/423 [==============================] - 1s 3ms/sample - loss: 1.0358 - acc: 0.5934\n"
     ]
    }
   ],
   "source": [
    "best_deep_aug_model_500 = keras.models.load_model('./model_checkpoints/augmented_deep_model_500')\n",
    "best_deep_aug_model_600 = keras.models.load_model('./model_checkpoints/augmented_deep_model_600')\n",
    "best_deep_aug_model_700 = keras.models.load_model('./model_checkpoints/augmented_deep_model_700')\n",
    "best_deep_aug_model_800 = keras.models.load_model('./model_checkpoints/augmented_deep_model_800')\n",
    "best_deep_aug_model_900 = keras.models.load_model('./model_checkpoints/augmented_deep_model_900')\n",
    "best_deep_aug_model_1000 = keras.models.load_model('./model_checkpoints/augmented_deep_model_1000')\n",
    "\n",
    "number_of_samples = [500, 600, 700, 800, 900, 1000]\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "\n",
    "# ==================================== 500 ==================================== #\n",
    "\n",
    "X_valid_slices, y_valid_slices = sliding_window(X_valid, \n",
    "                                                y_valid, \n",
    "                                                time_window=500, \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "accuracies.append(best_deep_aug_model_500.evaluate(X_valid_slices, y_valid_slices)[1])\n",
    "\n",
    "\n",
    "\n",
    "# ==================================== 600 ==================================== #\n",
    "\n",
    "X_valid_slices, y_valid_slices = sliding_window(X_valid, \n",
    "                                                y_valid, \n",
    "                                                time_window=600, \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "accuracies.append(best_deep_aug_model_600.evaluate(X_valid_slices, y_valid_slices)[1])\n",
    "\n",
    "\n",
    "\n",
    "# ==================================== 700 ==================================== #\n",
    "\n",
    "X_valid_slices, y_valid_slices = sliding_window(X_valid, \n",
    "                                                y_valid, \n",
    "                                                time_window=700, \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "accuracies.append(best_deep_aug_model_700.evaluate(X_valid_slices, y_valid_slices)[1])\n",
    "\n",
    "\n",
    "# ==================================== 800 ==================================== #\n",
    "\n",
    "X_valid_slices, y_valid_slices = sliding_window(X_valid, \n",
    "                                                y_valid, \n",
    "                                                time_window=800, \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "accuracies.append(best_deep_aug_model_800.evaluate(X_valid_slices, y_valid_slices)[1])\n",
    "\n",
    "\n",
    "# ==================================== 900 ==================================== #\n",
    "\n",
    "X_valid_slices, y_valid_slices = sliding_window(X_valid, \n",
    "                                                y_valid, \n",
    "                                                time_window=900, \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "accuracies.append(best_deep_aug_model_900.evaluate(X_valid_slices, y_valid_slices)[1])\n",
    "\n",
    "\n",
    "# ==================================== 1000 ==================================== #\n",
    "\n",
    "X_valid_slices, y_valid_slices = sliding_window(X_valid, \n",
    "                                                y_valid, \n",
    "                                                time_window=1000, \n",
    "                                                time_stride=TIME_STRIDE)\n",
    "\n",
    "accuracies.append(best_deep_aug_model_1000.evaluate(X_valid_slices, y_valid_slices)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "plt.xlabel('number of samples')\n",
    "plt.ylabel('accuracy')\n",
    "ax.plot(number_of_samples, accuracies);# Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443/443 [==============================] - 1s 1ms/sample - loss: 1.0110 - acc: 0.6140\n",
      "Deep model test loss: 1.0109507561268172\n",
      "Deep model test acc: 0.6139955\n"
     ]
    }
   ],
   "source": [
    "X_test_slices, y_test_slices = sliding_window(X_test, \n",
    "                                              y_test, \n",
    "                                              time_window=600, \n",
    "                                              time_stride=TIME_STRIDE)\n",
    "\n",
    "deep_model_results = best_deep_aug_model_600.evaluate(X_test_slices, y_test_slices)\n",
    "\n",
    "\n",
    "print('Deep model test loss:', deep_model_results[0])\n",
    "print('Deep model test acc:', deep_model_results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 654.545454,
   "position": {
    "height": "40px",
    "left": "266.375px",
    "right": "20px",
    "top": "2px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
